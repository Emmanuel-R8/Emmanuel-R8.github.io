
@article{ahujaEstimatingKullbackLeiblerDivergence2019,
  title = {Estimating {{Kullback}}-{{Leibler Divergence Using Kernel Machines}}},
  author = {Ahuja, Kartik},
  year = {2019},
  month = aug,
  abstract = {Recently, a method called the Mutual Information Neural Estimator (MINE) that uses neural networks has been proposed to estimate mutual information and more generally the Kullback-Leibler (KL) divergence between two distributions. The method uses the Donsker-Varadhan representation to arrive at the estimate of the KL divergence and is better than the existing estimators in terms of scalability and flexibility. The output of MINE algorithm is not guaranteed to be a consistent estimator. We propose a new estimator that instead of searching among functions characterized by neural networks searches the functions in a Reproducing Kernel Hilbert Space. We prove that the proposed estimator is consistent. We carry out simulations and show that when the datasets are small the proposed estimator is more reliable than the MINE estimator and when the datasets are large the performance of the two methods are close.},
  archivePrefix = {arXiv},
  eprint = {1905.00586},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/PV7A59JS/Ahuja - 2019 - Estimating Kullback-Leibler Divergence Using Kerne.pdf},
  journal = {arXiv:1905.00586 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{angINTERNATIONALASSETALLOCATION,
  title = {{{INTERNATIONAL ASSET ALLOCATION WITH TIME}}-{{VARYING CORRELATIONS}}},
  author = {Ang, Andrew and Bekaert, Geert},
  pages = {65},
  abstract = {It is widely believed that correlations between international equity markets tend to increase in highly volatile bear markets. This has led some to doubt the benefits of international diversification. This article solves the dynamic portfolio choice problem of a US investor faced with a time-varying investment opportunity set which may be characterized by correlations and volatilities that increase in bad times. We model the state dependance of US, UK, and German equity returns using a regime-switching model and find evidence for the existence of a high volatility regime, in which returns are more highly correlated and have lower means. Solving the dynamic asset allocation problem for a CCRA investor, we show international diversification is still valuable with regime changes. Currency hedging imparts further benefit. The costs of ignoring the regimes are small for moderate levels of risk aversion, and the intertemporal hedging demands induced by time-varying correlations are negligible.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/5KGJJ688/Ang and Bekaert - INTERNATIONAL ASSET ALLOCATION WITH TIME-VARYING C.pdf},
  language = {en}
}

@article{ardiaDifferentialEvolutionDEoptim,
  title = {Differential {{Evolution}} ({{DEoptim}}) for {{Non}}-{{Convex Portfolio Optimization}}},
  author = {Ardia, David and Boudt, Kris and Carl, Peter and Peterson, Brian G},
  pages = {8},
  abstract = {The R package DEoptim implements the Differential Evolution algorithm. This algorithm is an evolutionary technique similar to classic genetic algorithms that is useful for the solution of global optimization problems. In this note we provide an introduction to the package and demonstrate its utility for financial applications by solving a non-convex portfolio optimization problem.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/27BNMV22/Ardia et al. - Differential Evolution (DEoptim) for Non-Convex Po.pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@article{atanovSemiConditionalNormalizingFlows2020,
  title = {Semi-{{Conditional Normalizing Flows}} for {{Semi}}-{{Supervised Learning}}},
  author = {Atanov, Andrei and Volokhova, Alexandra and Ashukha, Arsenii and Sosnovik, Ivan and Vetrov, Dmitry},
  year = {2020},
  month = jun,
  abstract = {This paper proposes a semi-conditional normalizing flow model for semi-supervised learning. The model uses both labelled and unlabeled data to learn an explicit model of joint distribution over objects and labels. Semi-conditional architecture of the model allows us to efficiently compute a value and gradients of the marginal likelihood for unlabeled objects. The conditional part of the model is based on a proposed conditional coupling layer. We demonstrate a performance of the model for semi-supervised classification problem on different datasets. The model outperforms the baseline approach based on variational autoencoders on MNIST dataset.},
  archivePrefix = {arXiv},
  eprint = {1905.00505},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/SFNHWDAL/Atanov et al. - 2020 - Semi-Conditional Normalizing Flows for Semi-Superv.pdf},
  journal = {arXiv:1905.00505 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{audolyFragmentationRodsCascading,
  title = {Fragmentation of Rods by Cascading Cracks: Why Spaghetti Do Not Break in Half},
  author = {Audoly, Basile and Neukirch, Sebastien},
  pages = {5},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/6VJSQ9VS/Audoly and Neukirch - Fragmentation of rods by cascading cracks why spa.pdf},
  language = {en}
}

@article{badiaAgent57OutperformingAtari2020,
  title = {Agent57: {{Outperforming}} the {{Atari Human Benchmark}}},
  shorttitle = {Agent57},
  author = {Badia, Adri{\`a} Puigdom{\`e}nech and Piot, Bilal and Kapturowski, Steven and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Blundell, Charles},
  year = {2020},
  month = mar,
  abstract = {Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.},
  archivePrefix = {arXiv},
  eprint = {2003.13350},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/ZBPSRH5U/Badia et al. - 2020 - Agent57 Outperforming the Atari Human Benchmark.pdf},
  journal = {arXiv:2003.13350 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@incollection{baezPhysicsTopologyLogic2010,
  title = {Physics, {{Topology}}, {{Logic}} and {{Computation}}: {{A Rosetta Stone}}},
  shorttitle = {Physics, {{Topology}}, {{Logic}} and {{Computation}}},
  booktitle = {New {{Structures}} for {{Physics}}},
  author = {Baez, J. and Stay, M.},
  editor = {Coecke, Bob},
  year = {2010},
  volume = {813},
  pages = {95--172},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-12821-9_2},
  abstract = {In physics, Feynman diagrams are used to reason about quantum processes. In the 1980s, it became clear that underlying these diagrams is a powerful analogy between quantum physics and topology. Namely, a linear operator behaves very much like a `cobordism': a manifold representing spacetime, going between two manifolds representing space. This led to a burst of work on topological quantum field theory and `quantum topology'. But this was just the beginning: similar diagrams can be used to reason about logic, where they represent proofs, and computation, where they represent programs. With the rise of interest in quantum cryptography and quantum computation, it became clear that there is extensive network of analogies between physics, topology, logic and computation. In this expository paper, we make some of these analogies precise using the concept of `closed symmetric monoidal category'. We assume no prior knowledge of category theory, proof theory or computer science.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/AY9LSRK6/Baez and Stay - 2010 - Physics, Topology, Logic and Computation A Rosett.pdf},
  isbn = {978-3-642-12820-2 978-3-642-12821-9},
  language = {en}
}

@book{barberBayesianReasoningMachine2011,
  title = {Bayesian {{Reasoning}} and {{Machine Learning}}},
  author = {Barber, David},
  year = {2011},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511804779},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/YZBWK4AY/Barber - 2011 - Bayesian Reasoning and Machine Learning.pdf},
  isbn = {978-0-511-80477-9},
  language = {en}
}

@article{behrmannInvertibleResidualNetworks2019,
  title = {Invertible {{Residual Networks}}},
  author = {Behrmann, Jens and Grathwohl, Will and Chen, Ricky T. Q. and Duvenaud, David and Jacobsen, J{\"o}rn-Henrik},
  year = {2019},
  month = may,
  abstract = {We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both stateof-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.},
  archivePrefix = {arXiv},
  eprint = {1811.00995},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/NZEH22PN/Behrmann et al. - 2019 - Invertible Residual Networks.pdf},
  journal = {arXiv:1811.00995 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{behrmannInvertibleResidualNetworks2019a,
  title = {Invertible {{Residual Networks}}},
  author = {Behrmann, Jens and Grathwohl, Will and Chen, Ricky T. Q. and Duvenaud, David and Jacobsen, J{\"o}rn-Henrik},
  year = {2019},
  month = may,
  abstract = {We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both stateof-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.},
  archivePrefix = {arXiv},
  eprint = {1811.00995},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/IAYNU8DD/Behrmann et al. - 2019 - Invertible Residual Networks.pdf},
  journal = {arXiv:1811.00995 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{belghaziMINEMutualInformation2018,
  title = {{{MINE}}: {{Mutual Information Neural Estimation}}},
  shorttitle = {{{MINE}}},
  author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R. Devon},
  year = {2018},
  month = jun,
  abstract = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.},
  archivePrefix = {arXiv},
  eprint = {1801.04062},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/WAACYGNB/Belghazi et al. - 2018 - MINE Mutual Information Neural Estimation.pdf},
  journal = {arXiv:1801.04062 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@misc{BibBase,
  title = {{{BibBase}}},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/7BXRIZTR/saad-cusumanotowner-schaechtle-rinard-mansinghka-bayesiansynthesisofprobabilisticprogramsforaut.html},
  howpublished = {https://bibbase.org/network/publication/saad-cusumanotowner-schaechtle-rinard-mansinghka-bayesiansynthesisofprobabilisticprogramsforautomaticdatamodeling-2019}
}

@article{boosDynamicAssetAllocation,
  title = {Dynamic {{Asset Allocation}} with {{Regime Shifts}}},
  author = {Boos, Dominik and Schmid, Olivier and Koller, Jerome},
  pages = {18},
  abstract = {In a dynamic context, portfolio management is a sequential process in which an investor revises his allocation periodically with respect to changing endogenous conditions and to the ongoing evolution on the financial markets. In this paper, we present a stochastic multistage optimization model for the portfolio revision problem. We consider rebalancing activities, transaction costs, stochastic volatilities and many others. Furthermore, we introduce a general framework for the generation of the asset returns. The regime switching model proposed allows to capture fat tails, contagion, decoupling, volatility clustering and many other empirical properties of asset returns. Such a true dynamic approach to portfolio management leads to undisputable advantages in practical applications.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/WMJH9G5J/Boos et al. - Dynamic Asset Allocation with Regime Shifts.pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@article{borodinCanWeLearn2004,
  title = {Can {{We Learn}} to {{Beat}} the {{Best Stock}}},
  author = {Borodin, A. and {El-Yaniv}, R. and Gogan, V.},
  year = {2004},
  month = may,
  volume = {21},
  pages = {579--594},
  issn = {1076-9757},
  doi = {10.1613/jair.1336},
  abstract = {A novel algorithm for actively trading stocks is presented. While traditional expert advice and ``universal'' algorithms (as well as standard technical trading heuristics) attempt to predict winners or trends, our approach relies on predictable statistical relations between all pairs of stocks in the market. Our empirical results on historical markets provide strong evidence that this type of technical trading can ``beat the market'' and moreover, can beat the best stock in the market. In doing so we utilize a new idea for smoothing critical parameters in the context of expert learning.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/39GYZSR2/Borodin et al. - 2004 - Can We Learn to Beat the Best Stock.pdf},
  journal = {Journal of Artificial Intelligence Research},
  language = {en}
}

@inproceedings{boudtHedgeFundPortfolio2008,
  title = {Hedge Fund Portfolio Selection with Modified Expected Shortfall},
  booktitle = {Computational {{Finance}} and Its {{Applications III}}},
  author = {Boudt, K. and Peterson, B. G. and Carl, P.},
  year = {2008},
  month = may,
  volume = {I},
  pages = {99--107},
  publisher = {{WIT Press}},
  address = {{Cadiz, Spain}},
  doi = {10.2495/CF080101},
  abstract = {Modified Value-at-Risk (VaR) and Expected Shortfall (ES) are recently introduced downside risk estimators based on the Cornish-Fisher expansion for assets such as hedge funds whose returns are non-normally distributed. Modified VaR has been widely implemented as a portfolio selection criterion. We are the first to investigate hedge fund portfolio selection using modified ES as optimality criterion. We show that for the EDHEC hedge fund style indices, the optimal portfolios based on modified ES outperform out-of-sample the EDHEC Fund of Funds index and have better risk characteristics than the equal-weighted and Fund of Funds portfolios.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/5KY2YXJA/Boudt et al. - 2008 - Hedge fund portfolio selection with modified expec.pdf},
  isbn = {978-1-84564-111-5},
  language = {en}
}

@article{boudtVignettePortfolioOptimization,
  title = {Vignette: {{Portfolio Optimization}} with {{CVaR}} Budgets in {{PortfolioAnalytics}}},
  author = {Boudt, Kris and Carl, Peter and Peterson, Brian},
  pages = {13},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/D7IUUQQF/Boudt et al. - Vignette Portfolio Optimization with CVaR budgets.pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@article{brownDynamicPortfolioOptimization2011,
  title = {Dynamic {{Portfolio Optimization}} with {{Transaction Costs}}: {{Heuristics}} and {{Dual Bounds}}},
  shorttitle = {Dynamic {{Portfolio Optimization}} with {{Transaction Costs}}},
  author = {Brown, David B. and Smith, James E.},
  year = {2011},
  month = oct,
  volume = {57},
  pages = {1752--1770},
  issn = {0025-1909, 1526-5501},
  doi = {10/c64jz9},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/WYBMMSBQ/Brown and Smith - 2011 - Dynamic Portfolio Optimization with Transaction Co.pdf},
  journal = {Management Science},
  language = {en},
  number = {10}
}

@article{browneCANYOUBETTER,
  title = {{{CAN YOU DO BETTER THAN KELLY IN THE SHORT RUN}} ?},
  author = {Browne, Sid},
  pages = {15},
  abstract = {The Kelly criterion, also known as the optimal growth or logarithmic utility strategy, is optimal, or near optimal, in a wide variety of settings when wealth grows purely multiplicatively. However, most of these optimality properties are over an infinite horizon or are asymptotic in nature. In this paper, we analyze some of the short-run properties of the Kelly strategy, and compare its performance to the dynamic state and time dependent policy that is optimal for maximizing the probability of achieving a given value, or outperforming another strategy, by a given fixed deadline.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/EQFGUYKZ/Browne - CAN YOU DO BETTER THAN KELLY IN THE SHORT RUN .pdf},
  language = {en}
}

@misc{btrivellatoDeepDiveReformer,
  title = {A {{Deep Dive}} into the {{Reformer}}},
  author = {{B Trivellato}},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/J7H2ZU5C/reformer-deep-dive.html},
  howpublished = {https://www.pragmatic.ml/reformer-deep-dive/}
}

@article{bullaMarkovswitchingAssetAllocation2011,
  title = {Markov-Switching Asset Allocation: {{Do}} Profitable Strategies Exist?},
  shorttitle = {Markov-Switching Asset Allocation},
  author = {Bulla, Jan and Mergner, Sascha and Bulla, Ingo and Sesbo{\"u}{\'e}, Andr{\'e} and Chesneau, Christophe},
  year = {2011},
  month = nov,
  volume = {12},
  pages = {310--321},
  issn = {1470-8272, 1479-179X},
  doi = {10/bpdq7j},
  abstract = {This paper proposes a straightforward Markov-switching asset allocation model, which reduces the market exposure to periods of high volatility. The main purpose of the study is to examine the performance of a regime-based asset allocation strategy under realistic assumptions, compared to a buy and hold strategy. An empirical study, utilizing daily return series of major equity indices in the US, Japan, and Germany over the last 40 years, investigates the performance of the model. In an out-of-sample context, the strategy proves profitable after taking transaction costs into account. For the regional markets under consideration, the volatility reduces on average by 41\%. Additionally, annualized excess returns attain 18.5 to 201.6 basis points.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/S82V9CYU/Bulla et al. - 2011 - Markov-switching asset allocation Do profitable s.pdf},
  journal = {Journal of Asset Management},
  language = {en},
  number = {5}
}

@article{burtsevMemoryTransformer2020,
  title = {Memory {{Transformer}}},
  author = {Burtsev, Mikhail S. and Sapunov, Grigory V.},
  year = {2020},
  month = jun,
  abstract = {Transformer-based models have achieved state-of-the-art results in many natural language processing (NLP) tasks. The self-attention architecture allows us to combine information from all elements of a sequence into context-aware representations. However, all-to-all attention severely hurts the scaling of the model to large sequences. Another limitation is that information about the context is stored in the same element-wise representations. This makes the processing of properties related to the sequence as a whole more difficult. Adding trainable memory to selectively store local as well as global representations of a sequence is a promising direction to improve the Transformer model. Memory-augmented neural networks (MANNs) extend traditional neural architectures with general-purpose memory for representations. MANNs have demonstrated the capability to learn simple algorithms like Copy or Reverse and can be successfully trained via backpropagation on diverse tasks from question answering to language modeling outperforming RNNs and LSTMs of comparable complexity. In this work, we propose and study two extensions of the Transformer baseline (1) by adding memory tokens to store non-local representations, and (2) creating memory bottleneck for the global information. We evaluate these memory augmented Transformers on machine translation task and demonstrate that memory size positively correlates with the model performance. Attention patterns over the memory suggest that it improves the model's ability to process a global context. We expect that the application of Memory Transformer architectures to the tasks of language modeling, reading comprehension, and text summarization, as well as other NLP tasks that require the processing of long contexts will contribute to solving challenging problems of natural language understanding and generation.},
  archivePrefix = {arXiv},
  eprint = {2006.11527},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/W3A244MN/Burtsev and Sapunov - 2020 - Memory Transformer.pdf},
  journal = {arXiv:2006.11527 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{cerUniversalSentenceEncoder2018,
  title = {Universal {{Sentence Encoder}}},
  author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and {Guajardo-Cespedes}, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
  year = {2018},
  month = apr,
  abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
  archivePrefix = {arXiv},
  eprint = {1803.11175},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/NMLCDGYM/Cer et al. - 2018 - Universal Sentence Encoder.pdf},
  journal = {arXiv:1803.11175 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Computation and Language},
  language = {en},
  primaryClass = {cs}
}

@article{chelombievAdaptiveEstimatorsShow2019,
  title = {Adaptive {{Estimators Show Information Compression}} in {{Deep Neural Networks}}},
  author = {Chelombiev, Ivan and Houghton, Conor and O'Donnell, Cian},
  year = {2019},
  month = feb,
  abstract = {To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.},
  archivePrefix = {arXiv},
  eprint = {1902.09037},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/HGFMYCET/Chelombiev et al. - 2019 - Adaptive Estimators Show Information Compression i.pdf},
  journal = {arXiv:1902.09037 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{chenContinuousTimeFlowsEfficient2018,
  title = {Continuous-{{Time Flows}} for {{Efficient Inference}} and {{Density Estimation}}},
  author = {Chen, Changyou and Li, Chunyuan and Chen, Liqun and Wang, Wenlin and Pu, Yunchen and Carin, Lawrence},
  year = {2018},
  month = aug,
  abstract = {Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. Algorithms for the two tasks, such as normalizing flows and generative adversarial networks (GANs), are often developed independently. In this paper, we propose the concept of continuous-time flows (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit energy-based distribution with CTFs for density estimation. Both tasks rely on a new technique for distribution matching within amortized learning. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.},
  archivePrefix = {arXiv},
  eprint = {1709.01179},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/LKDW2YFG/Chen et al. - 2018 - Continuous-Time Flows for Efficient Inference and .pdf},
  journal = {arXiv:1709.01179 [stat]},
  keywords = {‚õî No DOI found,Statistics - Machine Learning},
  language = {en},
  primaryClass = {stat}
}

@article{chenNeuralOrdinaryDifferential2019,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2019},
  month = dec,
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archivePrefix = {arXiv},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/8M46XLWM/Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf},
  journal = {arXiv:1806.07366 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{chenNeuralOrdinaryDifferential2019a,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2019},
  month = dec,
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archivePrefix = {arXiv},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/ZYLJJZ6Z/Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf},
  journal = {arXiv:1806.07366 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{chenPossibilisticMeanVaR,
  title = {A {{Possibilistic Mean VaR Model}} for {{Portfolio Selection}}},
  author = {Chen, Guohua and Chen, Shou and Fang, Yong and Wang, Shouyang},
  pages = {9},
  abstract = {This paper deals with a portfolio selection problem with fuzzy return rates. A possibilistic mean VaR model was proposed for portfolio selection. Specially, we present a mathematical programming model with possibilistic constraint. The possibilistic programming problem can be solved by transforming it into a linear programming problem. A numerical example is given to illustrate the behavior of the proposed model.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/STXJWTLB/Chen et al. - A Possibilistic Mean VaR Model for Portfolio Selec.pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@article{chenResidualFlowsInvertible2020,
  title = {Residual {{Flows}} for {{Invertible Generative Modeling}}},
  author = {Chen, Ricky T. Q. and Behrmann, Jens and Duvenaud, David and Jacobsen, J{\"o}rn-Henrik},
  year = {2020},
  month = jul,
  abstract = {Flow-based generative models parameterize probability distributions through an invertible transformation and can be trained by maximum likelihood. Invertible residual networks provide a flexible family of transformations where only Lipschitz conditions rather than strict architectural constraints are needed for enforcing invertibility. However, prior work trained invertible residual networks for density estimation by relying on biased log-density estimates whose bias increased with the network's expressiveness. We give a tractable unbiased estimate of the log density using a ``Russian roulette'' estimator, and reduce the memory required during training by using an alternative infinite series for the gradient. Furthermore, we improve invertible residual blocks by proposing the use of activation functions that avoid derivative saturation and generalizing the Lipschitz condition to induced mixed norms. The resulting approach, called Residual Flows, achieves state-of-theart performance on density estimation amongst flow-based models, and outperforms networks that use coupling blocks at joint generative and discriminative modeling.},
  archivePrefix = {arXiv},
  eprint = {1906.02735},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/V896YHVG/Chen et al. - 2020 - Residual Flows for Invertible Generative Modeling.pdf},
  journal = {arXiv:1906.02735 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{chenVFlowMoreExpressive2020,
  title = {{{VFlow}}: {{More Expressive Generative Flows}} with {{Variational Data Augmentation}}},
  shorttitle = {{{VFlow}}},
  author = {Chen, Jianfei and Lu, Cheng and Chenli, Biqi and Zhu, Jun and Tian, Tian},
  year = {2020},
  month = feb,
  abstract = {Generative flows are promising tractable models for density modeling that define probabilistic distributions with invertible transformations. However, tractability imposes architectural constraints on generative flows, making them less expressive than other types of generative models. In this work, we study a previously overlooked constraint that all the intermediate representations must have the same dimensionality with the original data due to invertibility, limiting the width of the network. We tackle this constraint by augmenting the data with some extra dimensions and jointly learning a generative flow for augmented data as well as the distribution of augmented dimensions under a variational inference framework. Our approach, VFlow, is a generalization of generative flows and therefore always performs better. Combining with existing generative flows, VFlow achieves a new state-of-the-art 2.98 bits per dimension on the CIFAR-10 dataset and is more compact than previous models to reach similar modeling quality.},
  archivePrefix = {arXiv},
  eprint = {2002.09741},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/EYSWQ5L7/Chen et al. - 2020 - VFlow More Expressive Generative Flows with Varia.pdf},
  journal = {arXiv:2002.09741 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,G.3,I.2.6,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{childGeneratingLongSequences2019,
  title = {Generating {{Long Sequences}} with {{Sparse Transformers}}},
  author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  year = {2019},
  month = apr,
  abstract = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations o{$\surd$}f the attention matrix which reduce this to O(n n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
  archivePrefix = {arXiv},
  eprint = {1904.10509},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/BXAFGV57/Child et al. - 2019 - Generating Long Sequences with Sparse Transformers.pdf},
  journal = {arXiv:1904.10509 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{choRobustPortfolioOptimization,
  title = {Robust {{Portfolio Optimization Using Conditional Value At Risk Final Report}}},
  author = {Cho, Wei Ning},
  pages = {74},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/9ECNYTPN/Cho - Robust Portfolio Optimization Using Conditional Va.PDF},
  language = {en}
}

@article{clarkeMinimumVariancePortfolio,
  title = {Minimum {{Variance Portfolio Composition}}},
  author = {Clarke, Roger and {de Silva}, Harindra and Thorley, Steven},
  pages = {27},
  abstract = {Empirical studies document that equity portfolios constructed to have the lowest possible risk have surprisingly high average returns. We derive an analytic solution for the long-only minimum variance portfolio under the assumption of a single-factor covariance matrix. The equation for optimal security weights has a simple and intuitive form that provides several insights on minimum variance portfolio composition. While high idiosyncratic risk can lead to a low security weight, high systematic risk takes the large majority of investable securities out of long-only solutions. The relatively small set of securities that remain have market betas below an analytically specified threshold beta. The ratio of portfolio beta to the threshold beta dictates the portion of ex-ante portfolio variance that is market-factor related. We verify and illustrate the portfolio mathematics using historical data on the U.S. equity market and explore how the single-factor analytic results compare to numerical optimization under a generalized covariance matrix. The analytic and empirical results of this study suggest that minimum variance portfolio performance is largely a function of the long-standing empirical critique of the traditional CAPM that low beta stocks have relatively high average returns.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/QKXXSB6H/Clarke et al. - Minimum Variance Portfolio Composition.pdf},
  keywords = {‚ùìMultiple DOI},
  language = {en}
}

@article{clarkWhatDoesBERT2019,
  title = {What {{Does BERT Look At}}? {{An Analysis}} of {{BERT}}'s {{Attention}}},
  shorttitle = {What {{Does BERT Look At}}?},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  year = {2019},
  month = jun,
  abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
  archivePrefix = {arXiv},
  eprint = {1906.04341},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/KCPSB7SE/Clark et al. - 2019 - What Does BERT Look At An Analysis of BERT's Atte.pdf},
  journal = {arXiv:1906.04341 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Computation and Language},
  language = {en},
  primaryClass = {cs}
}

@article{cranmerDiscoveringSymbolicModels2020,
  title = {Discovering {{Symbolic Models}} from {{Deep Learning}} with {{Inductive Biases}}},
  author = {Cranmer, Miles and {Sanchez-Gonzalez}, Alvaro and Battaglia, Peter and Xu, Rui and Cranmer, Kyle and Spergel, David and Ho, Shirley},
  year = {2020},
  month = jun,
  abstract = {We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example\textemdash a detailed dark matter simulation\textemdash and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distributiondata better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.},
  archivePrefix = {arXiv},
  eprint = {2006.11287},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/FG6NHL5U/Cranmer et al. - 2020 - Discovering Symbolic Models from Deep Learning wit.pdf},
  journal = {arXiv:2006.11287 [astro-ph, physics:physics, stat]},
  keywords = {‚õî No DOI found,Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning,Physics - Computational Physics,Statistics - Machine Learning},
  language = {en},
  primaryClass = {astro-ph, physics:physics, stat}
}

@article{criminisiDecisionForestsUnified2011,
  title = {Decision {{Forests}}: {{A Unified Framework}} for {{Classification}}, {{Regression}}, {{Density Estimation}}, {{Manifold Learning}} and {{Semi}}-{{Supervised Learning}}},
  shorttitle = {Decision {{Forests}}},
  author = {Criminisi, Antonio},
  year = {2011},
  volume = {7},
  pages = {81--227},
  issn = {1572-2740, 1572-2759},
  doi = {10.1561/0600000035},
  abstract = {This review presents a unified, efficient model of random decision forests which can be applied to a number of machine learning, computer vision, and medical image analysis tasks.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/JDUIYJPN/Criminisi - 2011 - Decision Forests A Unified Framework for Classifi.pdf},
  journal = {Foundations and Trends\textregistered{} in Computer Graphics and Vision},
  language = {en},
  number = {2-3}
}

@article{cuiMultiScaleConvolutionalNeural2016,
  title = {Multi-{{Scale Convolutional Neural Networks}} for {{Time Series Classification}}},
  author = {Cui, Zhicheng and Chen, Wenlin and Chen, Yixin},
  year = {2016},
  month = may,
  abstract = {Time series classification (TSC), the problem of predicting class labels of time series, has been around for decades within the community of data mining and machine learning, and found many important applications such as biomedical engineering and clinical prediction. However, it still remains challenging and falls short of classification accuracy and efficiency. Traditional approaches typically involve extracting discriminative features from the original time series using dynamic time warping (DTW) or shapelet transformation, based on which an off-the-shelf classifier can be applied. These methods are ad-hoc and separate the feature extraction part with the classification part, which limits their accuracy performance. Plus, most existing methods fail to take into account the fact that time series often have features at different time scales. To address these problems, we propose a novel end-to-end neural network model, Multi-scale Convolutional Neural Network (MCNN), which incorporates feature extraction and classification in a single framework. Leveraging a novel multi-branch layer and learnable convolutional layers, MCNN automatically extracts features at different scales and frequencies, leading to superior feature representation. MCNN is also computationally efficient, as it naturally leverages GPU computing. We conduct comprehensive empirical evaluation with various existing methods on a large number of benchmark datasets, and show that MCNN advances the state-of-the-art by achieving superior accuracy performance than other leading methods.},
  archivePrefix = {arXiv},
  eprint = {1603.06995},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/6JST6RCE/Cui et al. - 2016 - Multi-Scale Convolutional Neural Networks for Time.pdf},
  journal = {arXiv:1603.06995 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryClass = {cs}
}

@article{cussensjamesStochasticLogicPrograms,
  title = {Stochastic {{Logic Programs}}: {{Sampling}}, {{Inference}} and {{Applications}}},
  author = {Cussens, James},
  abstract = {Algorithms for exact and approximate inference in stochastic logic programs (SLPs) are presented, based respectively, on variable elimination and importance sampling. We then show how SLPs can be used to represent prior distributions for machine learning, using (i) logic programs and (ii) Bayes net structures as examples. Drawing on existing work in statistics, we apply the Metropolis-Hasting algorithm to construct a Markov chain which samples from the posterior distribution. A Prolog implementation for this is described. We also discuss the possibility of constructing explicit representations of the posterior.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/45ANC58G/Stochastic Logic Programs Sampling, Inference and Applications.pdf}
}

@article{daiTransformerXLAttentiveLanguage2019,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed}}-{{Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, TransformerXL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-ofthe-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch1.},
  archivePrefix = {arXiv},
  eprint = {1901.02860},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/7Z69EMRK/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf},
  journal = {arXiv:1901.02860 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{dasMetaOptimizationIts2011,
  title = {Meta Optimization and Its Application to Portfolio Selection},
  booktitle = {Proceedings of the 17th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '11},
  author = {Das, Puja and Banerjee, Arindam},
  year = {2011},
  pages = {1163},
  publisher = {{ACM Press}},
  address = {{San Diego, California, USA}},
  doi = {10.1145/2020408.2020588},
  abstract = {Several data mining algorithms use iterative optimization methods for learning predictive models. It is not easy to determine upfront which optimization method will perform best or converge fast for such tasks. In this paper, we analyze Meta Algorithms (MAs) which work by adaptively combining iterates from a pool of base optimization algorithms. We show that the performance of MAs are competitive with the best convex combination of the iterates from the base algorithms for online as well as batch convex optimization problems. We illustrate the effectiveness of MAs on the problem of portfolio selection in the stock market and use several existing ideas for portfolio selection as base algorithms. Using daily S\&P500 data for the past 21 years and a benchmark NYSE dataset, we show that MAs outperform existing portfolio selection algorithms with provable guarantees by several orders of magnitude, and match the performance of the best heuristics in the pool.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/TPUP2M6F/Das and Banerjee - 2011 - Meta optimization and its application to portfolio.pdf},
  isbn = {978-1-4503-0813-7},
  language = {en}
}

@misc{davidmackayCourseInformationTheory,
  title = {Course on {{Information Theory}}, {{Pattern Recognition}}, and {{Neural Networks}} - {{VideoLectures}} - {{VideoLectures}}.{{NET}}},
  author = {{David MacKay}},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/6X8AMM5J/course_information_theory_pattern_recognition.html},
  howpublished = {http://videolectures.net/course\_information\_theory\_pattern\_recognition/}
}

@article{demiguelImprovingPortfolioSelection,
  title = {Improving {{Portfolio Selection Using Option}}-{{Implied Volatility}} and {{Skewness}}},
  author = {DeMiguel, Victor and Plyakha, Yuliya and Uppal, Raman and Vilkov, Grigory},
  pages = {59},
  abstract = {Our objective in this paper is to examine whether one can use option-implied information to improve the selection of mean-variance portfolios with a large number of stocks, and to document which aspects of option-implied information are most useful for improving their out-of-sample performance. Portfolio performance is measured in terms of four metrics: volatility, Sharpe ratio, certainty-equivalent return, and turnover. Our empirical evidence shows that using option-implied volatility helps to reduce portfolio volatility, but does not improve the Sharpe ratio or certaintyequivalent return; option-implied correlation does not improve any of the metrics; however, expected returns estimated using information in the volatility risk premium and option-implied skewness increase substantially both the Sharpe ratio and certainty-equivalent return, even after prohibiting shortsales and accounting for transactions costs.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/JQERKTWT/DeMiguel et al. - Improving Portfolio Selection Using Option-Implied.pdf},
  keywords = {‚ùìMultiple DOI},
  language = {en}
}

@article{demiguelOptimalNaiveDiversification2009,
  title = {Optimal {{Versus Naive Diversification}}: {{How Inefficient}} Is the 1/ {{{\emph{N}}}} {{Portfolio Strategy}}?},
  shorttitle = {Optimal {{Versus Naive Diversification}}},
  author = {DeMiguel, Victor and Garlappi, Lorenzo and Uppal, Raman},
  year = {2009},
  month = may,
  volume = {22},
  pages = {1915--1953},
  issn = {0893-9454, 1465-7368},
  doi = {10/dzkc89},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/D9ES589D/DeMiguel et al. - 2009 - Optimal Versus Naive Diversification How Ineffici.pdf},
  journal = {Review of Financial Studies},
  language = {en},
  number = {5}
}

@article{demiguelPortfolioSelectionRobust2009,
  title = {Portfolio {{Selection}} with {{Robust Estimation}}},
  author = {DeMiguel, Victor and Nogales, Francisco J.},
  year = {2009},
  month = jun,
  volume = {57},
  pages = {560--577},
  issn = {0030-364X, 1526-5463},
  doi = {10/c6gdh6},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/H9SQJVQA/DeMiguel and Nogales - 2009 - Portfolio Selection with Robust Estimation.pdf},
  journal = {Operations Research},
  language = {en},
  number = {3}
}

@article{dempsterROCKETExceptionallyFast2019,
  title = {{{ROCKET}}: {{Exceptionally}} Fast and Accurate Time Series Classification Using Random Convolutional Kernels},
  shorttitle = {{{ROCKET}}},
  author = {Dempster, Angus and Petitjean, Fran{\c c}ois and Webb, Geoffrey I.},
  year = {2019},
  month = oct,
  abstract = {Most methods for time series classification that attain state-of-the-art accuracy have high computational complexity, requiring significant training time even for smaller datasets, and are intractable for larger datasets. Additionally, many existing methods focus on a single type of feature such as shape or frequency. Building on the recent success of convolutional neural networks for time series classification, we show that simple linear classifiers using random convolutional kernels achieve state-of-the-art accuracy with a fraction of the computational expense of existing methods.},
  archivePrefix = {arXiv},
  eprint = {1910.13051},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/74SASV97/Dempster et al. - 2019 - ROCKET Exceptionally fast and accurate time serie.pdf},
  journal = {arXiv:1910.13051 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{dengModelingContinuousStochastic2020,
  title = {Modeling {{Continuous Stochastic Processes}} with {{Dynamic Normalizing Flows}}},
  author = {Deng, Ruizhi and Chang, Bo and Brubaker, Marcus A. and Mori, Greg and Lehrmann, Andreas},
  year = {2020},
  month = feb,
  abstract = {Normalizing flows transform a simple base distribution into a complex target distribution and have proved to be powerful models for data generation and density estimation. In this work, we propose a novel type of normalizing flow driven by a differential deformation of the continuous-time Wiener process. As a result, we obtain a rich time series model whose observable process inherits many of the appealing properties of its base process, such as efficient computation of likelihoods and marginals. Furthermore, our continuous treatment provides a natural framework for irregular time series with an independent arrival process, including straightforward interpolation. We illustrate the desirable properties of the proposed model on popular stochastic processes and demonstrate its superior flexibility to variational RNN and latent ODE baselines in a series of experiments on synthetic and real-world data.},
  archivePrefix = {arXiv},
  eprint = {2002.10516},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/73PUBFZD/Deng et al. - 2020 - Modeling Continuous Stochastic Processes with Dyna.pdf},
  journal = {arXiv:2002.10516 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{desaiOnlinePatternRecognition2003,
  title = {Online {{Pattern Recognition}} in {{Multivariate Data Streams}} Using {{Unsupervised Learning}}},
  author = {Desai, Devina and Oates, Tim and Shanbhag, Vishal},
  year = {2003},
  pages = {8},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/I799R8A2/Desai et al. - Online Pattern Recognition in Multivariate Data St.pdf},
  keywords = {‚õî No DOI found},
  language = {en}
}

@article{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.},
  archivePrefix = {arXiv},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/S5S8YJEB/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf},
  journal = {arXiv:1810.04805 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Computation and Language},
  language = {en},
  primaryClass = {cs}
}

@article{dewolfInductiveLogicProgramming,
  title = {Inductive {{Logic Programming}}},
  author = {{de Wolf}, R M},
  pages = {105},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/NVR57XGP/de Wolf - Inductive Logic Programming.pdf},
  language = {en}
}

@article{dharmalingamTechniquesProgramsUsed,
  title = {Techniques and {{Programs Used}} in {{Bidding}} and {{Playing}} Phases of {{Imperfect Information Game}} \textendash{} an {{Overview}}},
  author = {Dharmalingam, M},
  volume = {2},
  pages = {7},
  abstract = {Bridge is an international imperfect information game played with similar rules all over the world and it is played by millions of players. It is an intelligent game; it increases creativity and knowledge of human mind. Many of the researchers analyses the Bridge bidding and playing phases, and they developed many programs for getting better results. The programs were performing well and it is also matter of time before the computer beats most human bridge players. As such, the researchers mainly focused on the techniques and computer programs which were used in bridge bidding and playing phases.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/VQIQMJXX/Dharmalingam - Techniques and Programs Used in Bidding and Playin.pdf},
  language = {en}
}

@incollection{dimaioLearningApproximationInductive2004,
  title = {Learning an {{Approximation}} to {{Inductive Logic Programming Clause Evaluation}}},
  booktitle = {Inductive {{Logic Programming}}},
  author = {DiMaio, Frank and Shavlik, Jude},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Camacho, Rui and King, Ross and Srinivasan, Ashwin},
  year = {2004},
  volume = {3194},
  pages = {80--97},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-30109-7_10},
  abstract = {One challenge faced by many Inductive Logic Programming (ILP) systems is poor scalability to problems with large search spaces and many examples. Randomized search methods such as stochastic clause selection (SCS) and rapid random restarts (RRR) have proven somewhat successful at addressing this weakness. However, on datasets where hypothesis evaluation is computationally expensive, even these algorithms may take unreasonably long to discover a good solution. We attempt to improve the performance of these algorithms on datasets by learning an approximation to ILP hypothesis evaluation. We generate a small set of hypotheses, uniformly sampled from the space of candidate hypotheses, and evaluate this set on actual data. These hypotheses and their corresponding evaluation scores serve as training data for learning an approximate hypothesis evaluator. We outline three techniques that make use of the trained evaluation-function approximator in order to reduce the computation required during an ILP hypothesis search. We test our approximate clause evaluation algorithm using the popular ILP system Aleph. Empirical results are provided on several benchmark datasets. We show that the clause evaluation function can be accurately approximated.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/NCIY538K/DiMaio and Shavlik - 2004 - Learning an Approximation to Inductive Logic Progr.pdf},
  isbn = {978-3-540-22941-4 978-3-540-30109-7},
  language = {en}
}

@article{dinhNICENonlinearIndependent2015,
  title = {{{NICE}}: {{Non}}-Linear {{Independent Components Estimation}}},
  shorttitle = {{{NICE}}},
  author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  year = {2015},
  month = apr,
  abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
  archivePrefix = {arXiv},
  eprint = {1410.8516},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/5IKZDW5S/Dinh et al. - 2015 - NICE Non-linear Independent Components Estimation.pdf},
  journal = {arXiv:1410.8516 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@misc{DiscoveringLatentCovariance,
  title = {Discovering {{Latent Covariance Structures}} for {{Multiple Time Series}}},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/Discovering Latent Covariance Structures for Multiple Time Series2.pdf;/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/XNS6TCEQ/tong19a-supp.pdf;/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/2GCEP2MV/tong19a.html;/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/HMLPD7NK/lkm.html},
  howpublished = {http://proceedings.mlr.press/v97/tong19a.html}
}

@misc{DiscoveringLatentCovariance2019,
  title = {Discovering {{Latent Covariance Structures}} for {{Multiple Time Series}}},
  year = {2019},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/Discovering Latent Covariance Structures for Multiple Time Series.pdf;/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/IV95BBWW/tong19a-supp.pdf;/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/2B7EUXDI/tong19a.html},
  howpublished = {http://proceedings.mlr.press/v97/tong19a.html}
}

@article{drapPHOTOGRAMMETRYVIRTUALEXPLORATION,
  title = {{{PHOTOGRAMMETRY FOR VIRTUAL EXPLORATION OF UNDERWATER ARCHEOLOGICAL SITES}}},
  author = {Drap, P and Seinturier, J and Scaradozzi, D and Gambogi, P and Long, L and Gauch, F},
  pages = {6},
  abstract = {This article describes on-going developments of the VENUS European Project (Virtual ExploratioN of Underwater Sites, http://www.venus-project.eu) concerning the first mission to sea in Pianosa Island, Italy in October 2006. The VENUS project aims at providing scientific methodologies and technological tools for the virtual exploration of deep underwater archaeological sites. The VENUS project will improve the accessibility of underwater sites by generating thorough and exhaustive 3D records for virtual exploration.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/9NEMQXLJ/Drap et al. - PHOTOGRAMMETRY FOR VIRTUAL EXPLORATION OF UNDERWAT.pdf},
  language = {en}
}

@inbook{duBriefReviewMechanics2013,
  title = {A {{Brief Review}} of the {{Mechanics}} of {{Watch}} and {{Clock}}},
  booktitle = {The {{Mechanics}} of {{Mechanical Watches}} and {{Clocks}}},
  author = {Du, Ruxu and Xie, Longhan},
  year = {2013},
  volume = {21},
  pages = {5--45},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-29308-5_2},
  collaborator = {Du, Ruxu and Xie, Longhan},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/7FH67PTU/Du and Xie - 2013 - A Brief Review of the Mechanics of Watch and Clock.pdf},
  isbn = {978-3-642-29307-8 978-3-642-29308-5},
  language = {en}
}

@article{dupontAugmentedNeuralODEs2019,
  title = {Augmented {{Neural ODEs}}},
  author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
  year = {2019},
  month = oct,
  abstract = {We show that Neural Ordinary Differential Equations (ODEs) learn representations that preserve the topology of the input space and prove that this implies the existence of functions Neural ODEs cannot represent. To address these limitations, we introduce Augmented Neural ODEs which, in addition to being more expressive models, are empirically more stable, generalize better and have a lower computational cost than Neural ODEs.},
  archivePrefix = {arXiv},
  eprint = {1904.01681},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/78Y96747/Augmented_Neural_ODEs___NeurIPS_2019_Appendix.pdf;/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/T5HPRW2Y/Dupont et al. - 2019 - Augmented Neural ODEs.pdf},
  journal = {arXiv:1904.01681 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{dupontAugmentedNeuralODEs2019a,
  title = {Augmented {{Neural ODEs}}},
  author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
  year = {2019},
  month = oct,
  abstract = {We show that Neural Ordinary Differential Equations (ODEs) learn representations that preserve the topology of the input space and prove that this implies the existence of functions Neural ODEs cannot represent. To address these limitations, we introduce Augmented Neural ODEs which, in addition to being more expressive models, are empirically more stable, generalize better and have a lower computational cost than Neural ODEs.},
  archivePrefix = {arXiv},
  eprint = {1904.01681},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/VID8AV9G/Dupont et al. - 2019 - Augmented Neural ODEs.pdf},
  journal = {arXiv:1904.01681 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{effenbergerPrimerInformationTheory2013,
  title = {A Primer on Information Theory, with Applications to Neuroscience},
  author = {Effenberger, Felix},
  year = {2013},
  month = oct,
  abstract = {Given the constant rise in quantity and quality of data obtained from neural systems on many scales ranging from molecular to systems', information-theoretic analyses became increasingly necessary during the past few decades in the neurosciences. Such analyses can provide deep insights into the functionality of such systems, as well as a rigid mathematical theory and quantitative measures of information processing in both healthy and diseased states of neural systems. This chapter will present a short introduction to the fundamentals of information theory, especially suited for people having a less firm background in mathematics and probability theory. To begin, the fundamentals of probability theory such as the notion of probability, probability distributions, and random variables will be reviewed. Then, the concepts of information and entropy (in the sense of Shannon), mutual information, and transfer entropy (sometimes also referred to as conditional mutual information) will be outlined. As these quantities cannot be computed exactly from measured data in practice, estimation techniques for information-theoretic quantities will be presented. The chapter will conclude with the applications of information theory in the field of neuroscience, including questions of possible medical applications and a short review of software packages that can be used for information-theoretic analyses of neural data.},
  archivePrefix = {arXiv},
  eprint = {1304.2333},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/7RN673U6/Effenberger - 2013 - A primer on information theory, with applications .pdf},
  journal = {arXiv:1304.2333 [cs, math, q-bio]},
  keywords = {‚õî No DOI found,Computer Science - Information Theory,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryClass = {cs, math, q-bio}
}

@article{faberQuantitativeApproachTactical2007,
  title = {A {{Quantitative Approach}} to {{Tactical Asset Allocation}}},
  author = {Faber, Mebane T.},
  year = {2007},
  month = jan,
  volume = {9},
  pages = {69--79},
  issn = {1534-7524, 2374-1368},
  doi = {10.3905/jwm.2007.674809},
  abstract = {The purpose of this paper is to present a simple quantitative method that improves the risk-adjusted returns across various asset classes. A simple moving average timing model is tested since 1900 on the United States equity market before testing since 1973 on other diverse and publicly traded asset class indices, including the Morgan Stanley Capital International EAFE Index (MSCI EAFE), Goldman Sachs Commodity Index (GSCI), National Association of Real Estate Investment Trusts Index (NAREIT), and United States government 10-year Treasury bonds. The approach is then examined in a tactical asset allocation framework where the empirical results are equity-like returns with bond-like volatility and drawdown.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/T6KE4NJY/Faber - 2007 - A Quantitative Approach to Tactical Asset Allocati.pdf},
  journal = {The Journal of Wealth Management},
  language = {en},
  number = {4}
}

@article{fawazDeepLearningTime2019,
  title = {Deep Learning for Time Series Classification: A Review},
  shorttitle = {Deep Learning for Time Series Classification},
  author = {Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  year = {2019},
  month = jul,
  volume = {33},
  pages = {917--963},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-019-00619-1},
  abstract = {Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-ofthe-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By training 8,730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.},
  archivePrefix = {arXiv},
  eprint = {1809.04356},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/CASJLMTD/Fawaz et al. - 2019 - Deep learning for time series classification a re.pdf},
  journal = {Data Min Knowl Disc},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  number = {4}
}

@article{fawazDeepNeuralNetwork2019,
  title = {Deep {{Neural Network Ensembles}} for {{Time Series Classification}}},
  author = {Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  year = {2019},
  month = jul,
  pages = {1--6},
  doi = {10.1109/IJCNN.2019.8852316},
  abstract = {Deep neural networks have revolutionized many fields such as computer vision and natural language processing. Inspired by this recent success, deep learning started to show promising results for Time Series Classification (TSC). However, neural networks are still behind the state-of-the-art TSC algorithms, that are currently composed of ensembles of 37 non deep learning based classifiers. We attribute this gap in performance due to the lack of neural network ensembles for TSC. Therefore in this paper, we show how an ensemble of 60 deep learning models can significantly improve upon the current state-of-the-art performance of neural networks for TSC, when evaluated over the UCR/UEA archive: the largest publicly available benchmark for time series analysis. Finally, we show how our proposed Neural Network Ensemble (NNE) is the first time series classifier to outperform COTE while reaching similar performance to the current state-of-the-art ensemble HIVE-COTE.},
  archivePrefix = {arXiv},
  eprint = {1903.06602},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/AE3AU6QS/Fawaz et al. - 2019 - Deep Neural Network Ensembles for Time Series Clas.pdf},
  journal = {2019 International Joint Conference on Neural Networks (IJCNN)},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en}
}

@article{fawazTransferLearningTime2018,
  title = {Transfer Learning for Time Series Classification},
  author = {Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  year = {2018},
  month = dec,
  pages = {1367--1376},
  doi = {10.1109/BigData.2018.8621990},
  abstract = {Transfer learning for deep neural networks is the process of first training a base network on a source dataset, and then transferring the learned features (the network's weights) to a second network to be trained on a target dataset. This idea has been shown to improve deep neural network's generalization capabilities in many computer vision tasks such as image recognition and object localization. Apart from these applications, deep Convolutional Neural Networks (CNNs) have also recently gained popularity in the Time Series Classification (TSC) community. However, unlike for image recognition problems, transfer learning techniques have not yet been investigated thoroughly for the TSC task. This is surprising as the accuracy of deep learning models for TSC could potentially be improved if the model is fine-tuned from a pre-trained neural network instead of training it from scratch. In this paper, we fill this gap by investigating how to transfer deep CNNs for the TSC task. To evaluate the potential of transfer learning, we performed extensive experiments using the UCR archive which is the largest publicly available TSC benchmark containing 85 datasets. For each dataset in the archive, we pre-trained a model and then fine-tuned it on the other datasets resulting in 7140 different deep neural networks. These experiments revealed that transfer learning can improve or degrade the models predictions depending on the dataset used for transfer. Therefore, in an effort to predict the best source dataset for a given target dataset, we propose a new method relying on Dynamic Time Warping to measure inter-datasets similarities. We describe how our method can guide the transfer to choose the best source dataset leading to an improvement in accuracy on 71 out of 85 datasets.},
  archivePrefix = {arXiv},
  eprint = {1811.01533},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/PWXLVRHG/Fawaz et al. - 2018 - Transfer learning for time series classification.pdf},
  journal = {2018 IEEE International Conference on Big Data (Big Data)},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en}
}

@article{fedorovPredictionProgressionAlzheimer2019,
  title = {Prediction of {{Progression}} to {{Alzheimer}}'s Disease with {{Deep InfoMax}}},
  author = {Fedorov, Alex and Hjelm, R. Devon and Abrol, Anees and Fu, Zening and Du, Yuhui and Plis, Sergey and Calhoun, Vince D.},
  year = {2019},
  month = apr,
  abstract = {Arguably, unsupervised learning plays a crucial role in the majority of algorithms for processing brain imaging. A recently introduced unsupervised approach Deep InfoMax (DIM) is a promising tool for exploring brain structure in a flexible non-linear way. In this paper, we investigate the use of variants of DIM in a setting of progression to Alzheimer's disease in comparison with supervised AlexNet and ResNet inspired convolutional neural networks. As a benchmark, we use a classification task between four groups: patients with stable, and progressive mild cognitive impairment (MCI), with Alzheimer's disease, and healthy controls. Our dataset is comprised of 828 subjects from the Alzheimers Disease Neuroimaging Initiative (ADNI) database. Our experiments highlight encouraging evidence of the high potential utility of DIM in future neuroimaging studies.},
  archivePrefix = {arXiv},
  eprint = {1904.10931},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/H7IR2G7J/Fedorov et al. - 2019 - Prediction of Progression to Alzheimer's disease w.pdf},
  journal = {arXiv:1904.10931 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{finlayHowTrainYour2020,
  title = {How to {{Train Your Neural ODE}}: The {{World}} of {{Jacobian}} and {{Kinetic Regularization}}},
  author = {Finlay, Chris and Jacobsen, J{\"o}rn-Henrik and Nurbekyan, Levon and Oberman, Adam M},
  year = {2020},
  month = jun,
  pages = {14},
  abstract = {Training neural ODEs on large datasets has not been tractable due to the necessity of allowing the adaptive numerical ODE solver to refine its step size to very small values. In practice this leads to dynamics equivalent to many hundreds or even thousands of layers. In this paper, we overcome this apparent difficulty by introducing a theoretically-grounded combination of both optimal transport and stability regularizations which encourage neural ODEs to prefer simpler dynamics out of all the dynamics that solve a problem well. Simpler dynamics lead to faster convergence and to fewer discretizations of the solver, considerably decreasing wall-clock time without loss in performance. Our approach allows us to train neural ODE-based generative models to the same performance as the unregularized dynamics, with significant reductions in training time. This brings neural ODEs closer to practical relevance in large-scale applications.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/JBAQ8LIX/Finlay et al. - How to Train Your Neural ODE the World of Jacobia.pdf},
  keywords = {‚õî No DOI found},
  language = {en}
}

@incollection{fischerIntroductionRestrictedBoltzmann2012,
  title = {An {{Introduction}} to {{Restricted Boltzmann Machines}}},
  booktitle = {Progress in {{Pattern Recognition}}, {{Image Analysis}}, {{Computer Vision}}, and {{Applications}}},
  author = {Fischer, Asja and Igel, Christian},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Alvarez, Luis and Mejail, Marta and Gomez, Luis and Jacobo, Julio},
  year = {2012},
  volume = {7441},
  pages = {14--36},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-33275-3_2},
  abstract = {Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning problems. They attracted much attention recently after being proposed as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models. The basic concepts of graphical models are introduced first, however, basic knowledge in statistics is presumed. Different learning algorithms for RBMs are discussed. As most of them are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and the required MCMC techniques is provided.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/LQCBSVQZ/Fischer and Igel - 2012 - An Introduction to Restricted Boltzmann Machines.pdf},
  isbn = {978-3-642-33274-6 978-3-642-33275-3},
  language = {en}
}

@article{flachProbabilisticReasoningTerms,
  title = {Probabilistic Reasoning with Terms},
  author = {Flach, Peter A and Gyftodimos, Elias and Lachiche, Nicolas},
  pages = {31},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/KHAAWTX3/Flach et al. - Probabilistic reasoning with terms.pdf},
  language = {en}
}

@article{foxBootstrappingRegressionModels,
  title = {Bootstrapping {{Regression Models}}},
  author = {Fox, John},
  pages = {14},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/XW6BFMSY/Fox - Bootstrapping Regression Models.pdf},
  language = {en}
}

@article{freemanLearningPredictLooking2019,
  title = {Learning to {{Predict Without Looking Ahead}}: {{World Models Without Forward Prediction}}},
  shorttitle = {Learning to {{Predict Without Looking Ahead}}},
  author = {Freeman, C. Daniel and Metz, Luke and Ha, David},
  year = {2019},
  month = oct,
  abstract = {Much of model-based reinforcement learning involves learning a model of an agent's world, and training an agent to leverage this model to perform a task more efficiently. While these models are demonstrably useful for agents, every naturally occurring model of the world of which we are aware---e.g., a brain---arose as the byproduct of competing evolutionary pressures for survival, not minimization of a supervised forward-predictive loss via gradient descent. That useful models can arise out of the messy and slow optimization process of evolution suggests that forward-predictive modeling can arise as a side-effect of optimization under the right circumstances. Crucially, this optimization process need not explicitly be a forward-predictive loss. In this work, we introduce a modification to traditional reinforcement learning which we call observational dropout, whereby we limit the agents ability to observe the real environment at each timestep. In doing so, we can coerce an agent into learning a world model to fill in the observation gaps during reinforcement learning. We show that the emerged world model, while not explicitly trained to predict the future, can help the agent learn key skills required to perform well in its environment. Videos of our results available at https://learningtopredict.github.io/},
  archivePrefix = {arXiv},
  eprint = {1910.13038},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/3GT3G7WT/Freeman et al. - 2019 - Learning to Predict Without Looking Ahead World M.pdf},
  journal = {arXiv:1910.13038 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{gaierWeightAgnosticNeural2019,
  title = {Weight {{Agnostic Neural Networks}}},
  author = {Gaier, Adam and Ha, David},
  year = {2019},
  month = sep,
  abstract = {Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/},
  archivePrefix = {arXiv},
  eprint = {1906.04358},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/ZTW7SFVL/Gaier and Ha - 2019 - Weight Agnostic Neural Networks.pdf},
  journal = {arXiv:1906.04358 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{gambackPragmaticReasoningBridge,
  title = {Pragmatic {{Reasoning}} in {{Bridge}}},
  author = {Gamback, Bjorn and Rayner, Manny and Pell, Barney},
  pages = {22},
  abstract = {In this paper we argue that bidding in the game of Contract Bridge can pro tably be regarded as a micro-world suitable for experimenting with pragmatics. We sketch an analysis in which a \textbackslash bidding system" is treated as the semantics of an arti cial language, and show how this \textbackslash language", despite its apparent simplicity, is capable of supporting a wide variety of common speech acts parallel to those in natural languages; we also argue that the reason for the relatively unsuccessful nature of previous attempts to write strong Bridge playing programs has been their failure to address the need to reason explicitly about knowledge, pragmatics, probabilities and plans. We give an overview of Pragma, a system currently under development, which embodies these ideas in concrete form, using a combination of rule-based inference, stochastic simulation, and \textbackslash neural-net" learning. Examples are given illustrating the functionality of the system in its current form.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/C9UTRXKT/Gamback et al. - Pragmatic Reasoning in Bridge.pdf},
  language = {en}
}

@article{garneloDeepSymbolicReinforcement2016,
  title = {Towards {{Deep Symbolic Reinforcement Learning}}},
  author = {Garnelo, Marta and Arulkumaran, Kai and Shanahan, Murray},
  year = {2016},
  month = sep,
  abstract = {Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-ofconcept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system \textendash though just a prototype \textendash{} learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game.},
  archivePrefix = {arXiv},
  eprint = {1609.05518},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/JFR6VJ55/Garnelo et al. - 2016 - Towards Deep Symbolic Reinforcement Learning.pdf},
  journal = {arXiv:1609.05518 [cs]},
  keywords = {üîçNo DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{garneloDeepSymbolicReinforcement2016a,
  title = {Towards {{Deep Symbolic Reinforcement Learning}}},
  author = {Garnelo, Marta and Arulkumaran, Kai and Shanahan, Murray},
  year = {2016},
  month = oct,
  abstract = {Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-ofconcept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system \textendash though just a prototype \textendash{} learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game.},
  archivePrefix = {arXiv},
  eprint = {1609.05518},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/WULQRQ8F/Garnelo et al. - 2016 - Towards Deep Symbolic Reinforcement Learning.pdf},
  journal = {arXiv:1609.05518 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{gegeliaWhyGaugeSymmetry2012,
  title = {Why Gauge Symmetry?},
  author = {Gegelia, J.},
  year = {2012},
  month = jun,
  abstract = {It is argued that the Weinberg-Salam model is the way it is because the most general selfconsistent effective field theory of massive vector bosons interacting with fermions and photons at leading order coincides with the Weinberg-Salam model in unitary gauge where the scalar field is replaced by its vacuum expectation value. To support this argument the most general Lorentzinvariant effective Lagrangian of massive vector bosons coupled to massless fermions is considered. Restrictions imposed on the interaction terms following from the consistency with the constraints of the second class and the perturbative renormalizability in the sense of effective field theories is analyzed. It is shown that the leading order effective Lagrangian containing interaction terms with dimensionless coupling constants coincides with the leading order effective Lagrangian of the locally invariant Yang-Mills theory up to globally invariant mass term of the vector bosons. Including the fermion masses and mixings and the interaction with the electromagnetic field leads to an effective field theory which at leading order looks like as if it was an SU (2) \texttimes{} U (1) gauge invariant theory with spontaneous symmetry breaking in unitary gauge with the scalar field replaced by its vacuum expectation value.},
  archivePrefix = {arXiv},
  eprint = {1207.0156},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/M8DMXGP5/Gegelia - 2012 - Why gauge symmetry.pdf},
  journal = {arXiv:1207.0156 [hep-ph, physics:hep-th]},
  keywords = {üîçNo DOI found,High Energy Physics - Phenomenology,High Energy Physics - Theory},
  language = {en},
  primaryClass = {hep-ph, physics:hep-th}
}

@article{ginsbergStepsExpertLevelBridgePlaying,
  title = {G {{I B}} : {{Steps Toward}} an {{Expert}}-{{Level Bridge}}-{{Playing Program}}},
  author = {Ginsberg, Matthew L},
  pages = {6},
  abstract = {This paper describes GIB, the first bridgeplaying program to approach the level of a human expert. ( G I B finished twelfth in a handpicked field of thirty-four experts at an invitational event at the 1998 World Bridge Championships.) We give a basic overview of the algorithms used, describe their strengths and weaknesses, and present the results of experiments comparing GIB to both human opponents and other programs.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/KC42ASHA/Ginsberg - G I B  Steps Toward an Expert-Level Bridge-Playin.pdf},
  language = {en}
}

@article{goldfeldEstimatingInformationFlow2019,
  title = {Estimating {{Information Flow}} in {{Deep Neural Networks}}},
  author = {Goldfeld, Ziv and van den Berg, Ewout and Greenewald, Kristjan and Melnyk, Igor and Nguyen, Nam and Kingsbury, Brian and Polyanskiy, Yury},
  year = {2019},
  month = may,
  abstract = {We study the estimation of the mutual information I(X; T ) between the input X to a deep neural network (DNN) and the output vector T of its th hidden layer (an ``internal representation''). Focusing on feedforward networks with fixed weights and noisy internal representations, we develop a rigorous framework for accurate estimation of I(X; T ). By relating I(X; T ) to information transmission over additive white Gaussian noise channels, we reveal that compression, i.e. reduction in I(X; T ) over the course of training, is driven by progressive geometric clustering of the representations of samples from the same class. Experimental results verify this connection. Finally, we shift focus to purely deterministic DNNs, where I(X; T ) is provably vacuous, and show that nevertheless, these models also cluster inputs belonging to the same class. The binningbased approximation of I(X; T ) employed in past works to measure compression is identified as a measure of clustering, thus clarifying that these experiments were in fact tracking the same clustering phenomenon. Leveraging the clustering perspective, we provide new evidence that compression and generalization may not be causally related and discuss potential future research ideas.},
  archivePrefix = {arXiv},
  eprint = {1810.05728},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/PRTFUI7C/Goldfeld et al. - 2019 - Estimating Information Flow in Deep Neural Network.pdf},
  journal = {arXiv:1810.05728 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{grathwohlFFJORDFreeformContinuous2018,
  title = {{{FFJORD}}: {{Free}}-Form {{Continuous Dynamics}} for {{Scalable Reversible Generative Models}}},
  shorttitle = {{{FFJORD}}},
  author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
  year = {2018},
  month = oct,
  abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
  archivePrefix = {arXiv},
  eprint = {1810.01367},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/LWEN9DBV/Grathwohl et al. - 2018 - FFJORD Free-form Continuous Dynamics for Scalable.pdf},
  journal = {arXiv:1810.01367 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{guidolinOptimalPortfolioChoice2003,
  title = {Optimal {{Portfolio Choice}} under {{Regime Switching}}, {{Skew}} and {{Kurtosis Preferences}}},
  author = {Guidolin, Massimo and Timmermann, Allan G.},
  year = {2003},
  issn = {1556-5068},
  doi = {10/d8k328},
  abstract = {This paper proposes a new tractable approach to solving multi-period asset allocation problems. We assume that investor preferences are de\TH ned over moments of the terminal wealth distribution such as its skew and kurtosis. Time-variations in investment opportunities are driven by a regime switching process that can capture bull and bear states. We develop analytical methods that only require solving a small set of difference equations and thus are very convenient to use. These methods are applied to a simple portfolio selection problem involving choosing between a stock index and a risk-free asset in the presence of bull and bear states in the return distribution. If the market is in a bear state, investors increase allocations to stocks the longer their time horizon. Conversely, in bull markets it is optimal for investors to decrease allocations to stocks the longer their investment horizon.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/HCBEJT24/Guidolin and Timmermann - 2003 - Optimal Portfolio Choice under Regime Switching, S.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{guidolinStrategicAssetAllocation2004,
  title = {Strategic {{Asset Allocation}} and {{Consumption Decisions}} under {{Multivariate Regime Switching}}},
  author = {Guidolin, Massimo and Timmermann, Allan G.},
  year = {2004},
  issn = {1556-5068},
  doi = {10/fxz225},
  abstract = {This paper studies strategic asset allocation and consumption choice in the presence of regime switching in asset returns. We find evidence that four separate regimes - characterized as crash, slow growth, bull and recovery states - are required to capture the joint distribution of stock and bond returns. Optimal asset allocations vary considerably across these states - both among bonds and stocks and among large and small stocks - and change over time as investors revise their estimates of the underlying state probabilities. In the crash state investors always allocate more of their portfolio to stocks the longer their investment horizon, while the optimal allocation to stocks declines as a function of the investment horizon in bull markets. The joint effects of learning about the underlying state probabilities and predictability of asset returns from the dividend yield give rise to a non-monotonic relationship between the investment horizon and the demand for stocks. Consumption-to-wealth ratios are found to depend on the underlying state and welfare costs from ignoring regime switching are substantial even after accounting for parameter uncertainty. Out-of-sample forecasting experiments confirm the economic importance of accounting for the presence of regimes in asset returns.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/Y44NYYQL/Guidolin and Timmermann - 2004 - Strategic Asset Allocation and Consumption Decisio.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{harleyEmotionalNeedsQuestionnaire,
  title = {Emotional {{Needs Questionnaire}}},
  author = {Harley, Willard F},
  pages = {12},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/CH2YF9MG/Harley - Emotional Needs Questionnaire.pdf},
  language = {en}
}

@book{harleyFiveStepsRomantic2002,
  title = {Five Steps to Romantic Love: A Workbook for Readers of {{Love}} Busters and {{His}} Needs, Her Needs},
  shorttitle = {Five Steps to Romantic Love},
  author = {Harley, Willard F. and Harley, Willard F. and Harley, Willard F.},
  year = {2002},
  publisher = {{Fleming H. Revell}},
  address = {{Grand Rapids, Mich}},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/98WZSYB5/Harley et al. - 2002 - Five steps to romantic love a workbook for reader.pdf},
  isbn = {978-0-8007-5823-3},
  keywords = {Communication in marriage,Man-woman relationships,Marriage},
  language = {en},
  lccn = {HQ734 .H284 2002}
}

@article{hermannTeachingMachinesRead2015,
  title = {Teaching {{Machines}} to {{Read}} and {{Comprehend}}},
  author = {Hermann, Karl Moritz and Ko{\v c}isk{\'y}, Tom{\'a}{\v s} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  year = {2015},
  month = jun,
  abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
  archivePrefix = {arXiv},
  eprint = {1506.03340},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/RVVHX4JV/Hermann et al. - 2015 - Teaching Machines to Read and Comprehend.pdf},
  journal = {arXiv:1506.03340 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{hlynssonLearningGradientbasedICA2019,
  title = {Learning Gradient-Based {{ICA}} by Neurally Estimating Mutual Information},
  author = {Hlynsson, Hlynur Dav{\'i}{\dh} and Wiskott, Laurenz},
  year = {2019},
  month = apr,
  abstract = {Several methods of estimating the mutual information of random variables have been developed in recent years. They can prove valuable for novel approaches to learning statistically independent features. In this paper, we use one of these methods, a mutual information neural estimation (MINE) network, to present a proof-of-concept of how a neural network can perform linear ICA. We minimize the mutual information, as estimated by a MINE network, between the output units of a differentiable encoder network. This is done by simple alternate optimization of the two networks. The method is shown to get a qualitatively equal solution to FastICA on blind-source-separation of noisy sources.},
  archivePrefix = {arXiv},
  eprint = {1904.09858},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/EQ6HMZVT/Hlynsson and Wiskott - 2019 - Learning gradient-based ICA by neurally estimating.pdf},
  journal = {arXiv:1904.09858 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hoContractBridgeBidding,
  title = {Contract {{Bridge Bidding}} by {{Learning}}},
  author = {Ho, Chun-Yen and Lin, Hsuan-Tien},
  pages = {7},
  abstract = {Contract bridge is an example of an incomplete information game for which computers typically do not perform better than expert human bridge players. In particular, the typical bidding decisions of human bridge players are difficult to mimic with a computer program, and thus automatic bridge bidding remains to be a challenging research problem. Currently, the possibility of automatic bidding without mimicking human players has not been fully studied. In this work, we take an initiative to study such a possibility for the specific problem of bidding without competition. We propose a novel learning framework to let a computer program learn its own bidding decisions. The framework transforms the bidding problem into a learning problem, and then solves the problem with a carefully designed model that consists of cost-sensitive classifiers and upper-confidence-bound algorithms. We validate the proposed model and find that it performs competitively to the champion computer bridge program that mimics human bidding decisions.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/9D85Y5IF/Ho and Lin - Contract Bridge Bidding by Learning.pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@article{hodgkinsonStochasticNormalizingFlows2020,
  title = {Stochastic {{Normalizing Flows}}},
  author = {Hodgkinson, Liam and {van der Heide}, Chris and Roosta, Fred and Mahoney, Michael W.},
  year = {2020},
  month = feb,
  abstract = {We introduce stochastic normalizing flows, an extension of continuous normalizing flows for maximum likelihood estimation and variational inference (VI) using stochastic differential equations (SDEs). Using the theory of rough paths, the underlying Brownian motion is treated as a latent variable and approximated, enabling efficient training of neural SDEs as random neural ordinary differential equations. These SDEs can be used for constructing efficient Markov chains to sample from the underlying distribution of a given dataset. Furthermore, by considering families of targeted SDEs with prescribed stationary distribution, we can apply VI to the optimization of hyperparameters in stochastic MCMC.},
  archivePrefix = {arXiv},
  eprint = {2002.09547},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/J62YILSL/Hodgkinson et al. - 2020 - Stochastic Normalizing Flows.pdf},
  journal = {arXiv:2002.09547 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hoFlowImprovingFlowBased2019,
  title = {Flow++: {{Improving Flow}}-{{Based Generative Models}} with {{Variational Dequantization}} and {{Architecture Design}}},
  shorttitle = {Flow++},
  author = {Ho, Jonathan and Chen, Xi and Srinivas, Aravind and Duan, Yan and Abbeel, Pieter},
  year = {2019},
  month = may,
  abstract = {Flow-based generative models are powerful exact likelihood models with efficient sampling and inference. Despite their computational efficiency, flow-based models generally have much worse density modeling performance compared to stateof-the-art autoregressive models. In this paper, we investigate and improve upon three limiting design choices employed by flow-based models in prior work: the use of uniform noise for dequantization, the use of inexpressive affine flows, and the use of purely convolutional conditioning networks in coupling layers. Based on our findings, we propose Flow++, a new flow-based model that is now the state-of-the-art non-autoregressive model for unconditional density estimation on standard image benchmarks. Our work has begun to close the significant performance gap that has so far existed between autoregressive models and flow-based models. Our implementation is available at: https://github.com/ aravindsrinivas/flowpp.},
  archivePrefix = {arXiv},
  eprint = {1902.00275},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/4YSHAJ7T/Ho et al. - 2019 - Flow++ Improving Flow-Based Generative Models wit.pdf},
  journal = {arXiv:1902.00275 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hortuaConstrainingReionizationHistory2020,
  title = {Constraining the {{Reionization History}} Using {{Bayesian Normalizing Flows}}},
  author = {Hort{\'u}a, H{\'e}ctor J. and Malago, Luigi and Volpi, Riccardo},
  year = {2020},
  month = may,
  abstract = {The next generation 21 cm surveys open a new window onto the early stages of cosmic structure formation and provide new insights about the Epoch of Reionization (EoR). However, the non-Gaussian nature of the 21 cm signal along with the huge amount of data generated from these surveys will require more advanced techniques capable to efficiently extract the necessary information to constrain the Reionization History of the Universe. In this paper we present the use of Bayesian Neural Networks (BNNs) to predict the posterior distribution for four astrophysical and cosmological parameters. Besides achieving state-of-the-art prediction performances, the proposed methods provide accurate estimation of parameters uncertainties and infer correlations among them. Additionally, we demonstrate the advantages of Normalizing Flows (NF) combined with BNNs, being able to model more complex output distributions and thus capture key information as non-Gaussianities in the parameter conditional density distribution for astrophysical and cosmological datasets. Finally, we propose novel calibration methods employing Normalizing Flows after training, to produce reliable predictions, and we demonstrate the advantages of this approach both in terms of computational cost and prediction performances.},
  archivePrefix = {arXiv},
  eprint = {2005.07694},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/B5VJER77/Hort√∫a et al. - 2020 - Constraining the Reionization History using Bayesi.pdf},
  journal = {arXiv:2005.07694 [astro-ph]},
  keywords = {‚õî No DOI found,Astrophysics - Cosmology and Nongalactic Astrophysics,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {astro-ph}
}

@article{huangAugmentedNormalizingFlows2020,
  title = {Augmented {{Normalizing Flows}}: {{Bridging}} the {{Gap Between Generative Flows}} and {{Latent Variable Models}}},
  shorttitle = {Augmented {{Normalizing Flows}}},
  author = {Huang, Chin-Wei and Dinh, Laurent and Courville, Aaron},
  year = {2020},
  month = feb,
  abstract = {In this work, we propose a new family of generative flows on an augmented data space, with an aim to improve expressivity without drastically increasing the computational cost of sampling and evaluation of a lower bound on the likelihood. Theoretically, we prove the proposed flow can approximate a Hamiltonian ODE as a universal transport map. Empirically, we demonstrate stateof-the-art performance on standard benchmarks of flow-based generative modeling.},
  archivePrefix = {arXiv},
  eprint = {2002.07101},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/ICZWFXFP/Huang et al. - 2020 - Augmented Normalizing Flows Bridging the Gap Betw.pdf},
  journal = {arXiv:2002.07101 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{huangMusicTransformer2018,
  title = {Music {{Transformer}}},
  author = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Monica and Eck, Douglas},
  year = {2018},
  month = dec,
  abstract = {Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minutelong compositions (thousands of steps, four times the length modeled in Oore et al. (2018)) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies1. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.},
  archivePrefix = {arXiv},
  eprint = {1809.04281},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/ZDCPBDR6/Huang et al. - 2018 - Music Transformer.pdf},
  journal = {arXiv:1809.04281 [cs, eess, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, eess, stat}
}

@article{izmailovSemiSupervisedLearningNormalizing2019,
  title = {Semi-{{Supervised Learning}} with {{Normalizing Flows}}},
  author = {Izmailov, Pavel and Kirichenko, Polina and Finzi, Marc and Wilson, Andrew Gordon},
  year = {2019},
  month = dec,
  abstract = {Normalizing flows transform a latent distribution through an invertible neural network for a flexible and pleasingly simple approach to generative modelling, while preserving an exact likelihood. We propose FlowGMM, an end-to-end approach to generative semi-supervised learning with normalizing flows, using a latent Gaussian mixture model. FlowGMM is distinct in its simplicity, unified treatment of labelled and unlabelled data with an exact likelihood, interpretability, and broad applicability beyond image data. We show promising results on a wide range of applications, including AG-News and Yahoo Answers text data, tabular data, and semi-supervised image classification. We also show that FlowGMM can discover interpretable structure, provide real-time optimization-free feature visualizations, and specify well calibrated predictive distributions.},
  archivePrefix = {arXiv},
  eprint = {1912.13025},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/R9IG9J7A/Izmailov et al. - 2019 - Semi-Supervised Learning with Normalizing Flows.pdf},
  journal = {arXiv:1912.13025 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{jamrogaModellingArtiCial,
  title = {Modelling {{Arti}} Cial {{Intelligence}} on a {{Case}} of {{Bridge Card Play Bidding}}},
  author = {Jamroga, Wojciech},
  pages = {11},
  abstract = {In this paper a model is constructed for games with communication under incomplete information. The model, based on natural language processing techniques, is developed and tested on a case of Bridge card play bidding. The approach proposed here emphasizes the information processing nature of such games. Finally, some conclusions from implementation of a prototype bidding system and a new methodology for performance evaluation of such systems are presented.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/9K98WNCQ/Jamroga - Modelling Arti cial Intelligence on a Case of Brid.pdf},
  language = {en}
}

@article{jonesAcolMasterAcolMaster,
  title = {Acol {{Master}} or {{AcolMaster}}},
  author = {Jones, Mr Paul},
  pages = {19},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/8ESZMXAE/Jones - Acol Master or AcolMaster.pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@article{kalinchenkoCalibratingRiskPreferences2011,
  title = {Calibrating {{Risk Preferences}} with {{Generalized CAPM Based}} on {{Mixed CVaR Deviation}}},
  author = {Kalinchenko, Konstantin and Uryasev, Stanislav and Rockafellar, Tyrrell R.},
  year = {2011},
  issn = {1556-5068},
  doi = {10/fxpcwm},
  abstract = {The generalized Capital Asset Pricing Model based on mixed CVaR deviation is used for calibrating risk preferences of investors protecting investments in S\&P500 by means of options. The corresponding new generalized beta is designed to capture tail performance of S\&P500 returns. Calibration is done by extracting information about risk preferences from option prices on S\&P500. Actual market option prices are matched with the estimated prices from the pricing equation based on the generalized beta. In addition to the risk preferences, an optimal allocation to a portfolio of options for the considered group of investors is calculated.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/D89B6JRE/Kalinchenko et al. - 2011 - Calibrating Risk Preferences with Generalized CAPM.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{karamiInvertibleConvolutionalFlow2019,
  title = {Invertible {{Convolutional Flow}}},
  author = {Karami, Mahdi and Schuurmans, Dale and {Sohl-Dickstein}, Jascha and Dinh, Laurent and Duckworth, Daniel},
  year = {2019},
  month = dec,
  pages = {11},
  abstract = {Normalizing flows can be used to construct high quality generative probabilistic models, but training and sample generation require repeated evaluation of Jacobian determinants and function inverses. To make such computations feasible, current approaches employ highly constrained architectures that produce diagonal, triangular, or low rank Jacobian matrices. As an alternative, we investigate a set of novel normalizing flows based on the circular and symmetric convolutions. We show that these transforms admit efficient Jacobian determinant computation and inverse mapping (deconvolution) in O(N log N ) time. Additionally, element-wise multiplication, widely used in normalizing flow architectures, can be combined with these transforms to increase modeling flexibility. We further propose an analytic approach to designing nonlinear elementwise bijectors that induce special properties in the intermediate layers, by implicitly introducing specific regularizers in the loss. We show that these transforms allow more effective normalizing flow models to be developed for generative image models.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/5M2LCQ2Z/Karami et al. - Invertible Convolutional Flow.pdf},
  keywords = {‚õî No DOI found},
  language = {en}
}

@article{kazemiOmegaPerformanceMeasure,
  title = {Omega as a {{Performance Measure}}},
  author = {Kazemi, Hossein and Schneeweis, Thomas and Gupta, Raj},
  pages = {14},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/2S69HRHM/Kazemi et al. - Omega as a Performance Measure.pdf},
  language = {en}
}

@article{keatingUniversalPerformanceMeasure2002,
  title = {A {{Universal Performance Measure}}},
  author = {Keating, Con and Shadwick, William F},
  year = {2002},
  pages = {33},
  abstract = {We propose and examine a new performance measure in the spirit of the downside, lower partial moment and gain-loss literatures. We indicate how this may be applied across a broad range of problems in financial analysis. The performance measure captures all of the higher moments of the returns distribution. The performance measure is applied to a range of hedge fund style or strategy indices.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/RAJ8SUWS/Keating and Shadwick - 2002 - A Universal Performance Measure.pdf},
  language = {en}
}

@phdthesis{keithmartinbriggsFeigenbaumScalingDiscrete2011,
  title = {Feigenbaum {{Scaling}} in {{Discrete Dynamical Systems}}},
  author = {{Keith Martin Briggs}},
  year = {2011},
  month = jan,
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/6YES6KKW/Keith_Briggs_PhD.pdf}
}

@article{kempfEstimatingGlobalMinimum2006,
  title = {Estimating the {{Global Minimum Variance Portfolio}}},
  author = {Kempf, Alexander and Memmel, Christoph},
  year = {2006},
  month = oct,
  volume = {58},
  pages = {332--348},
  issn = {1439-2917, 2194-072X},
  doi = {10/gd535j},
  abstract = {According to standard portfolio theory, the tangency portfolio is the only efficient stock portfolio. However, empirical studies show that an investment in the global minimum variance portfolio often yields better out-of-sample results than does an investment in the tangency portfolio and suggest investing in the global minimum variance portfolio. But little is known about the distributions of the weights and return parameters of this portfolio. Our contribution is to determine these distributions. By doing so, we answer several important questions in asset management.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/VJZHLSMI/Kempf and Memmel - 2006 - Estimating the Global Minimum Variance Portfolio.pdf},
  journal = {Schmalenbach Business Review},
  language = {en},
  number = {4}
}

@article{kidgerNeuralControlledDifferential2020,
  title = {Neural {{Controlled Differential Equations}} for {{Irregular Time Series}}},
  author = {Kidger, Patrick and Morrill, James and Foster, James and Lyons, Terry},
  year = {2020},
  month = may,
  abstract = {Neural ordinary differential equations are an attractive option for modelling temporal dynamics. However, a fundamental issue is that the solution to an ordinary differential equation is determined by its initial condition, and there is no mechanism for adjusting the trajectory based on subsequent observations. Here, we demonstrate how this may be resolved through the well-understood mathematics of controlled differential equations. The resulting neural controlled differential equation model is directly applicable to the general setting of partiallyobserved irregularly-sampled multivariate time series, and (unlike previous work on this problem) it may utilise memory-efficient adjoint-based backpropagation even across observations. We demonstrate that our model achieves state-of-the-art performance against similar (ODE or RNN based) models in empirical studies on a range of datasets. Finally we provide theoretical results demonstrating universal approximation, and that our model subsumes alternative ODE models.},
  archivePrefix = {arXiv},
  eprint = {2005.08926},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/SH9BQRQJ/Kidger et al. - 2020 - Neural Controlled Differential Equations for Irreg.pdf},
  journal = {arXiv:2005.08926 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kimFinancialSeriesPrediction2019,
  title = {Financial Series Prediction Using {{Attention LSTM}}},
  author = {Kim, Sangyeon and Kang, Myungjoo},
  year = {2019},
  month = feb,
  abstract = {Financial time series prediction, especially with machine learning techniques, is an extensive field of study. In recent times, deep learning methods (especially time series analysis)have performed outstandingly for various industrial problems, with better prediction than machine learning methods. Moreover, many researchers have used deep learning methods to predict financial time series with various models in recent years. In this paper, we will compare various deep learning models, such as multilayer perceptron (MLP), one-dimensional convolutional neural networks (1D CNN), stacked long short-term memory (stacked LSTM), attention networks, and weighted attention networks for financial time series prediction. In particular, attention LSTM is not only used for prediction, but also for visualizing intermediate outputs to analyze the reason of prediction; therefore, we will show an example for understanding the model prediction intuitively with attention vectors. In addition, we focus on time and factors, which lead to an easy understanding of why certain trends are predicted when accessing a given time series table. We also modify the loss functions of the attention models with weighted categorical cross entropy; our proposed model produces a 0.76 hit ratio, which is superior to those of other methods for predicting the trends of the KOSPI 200.},
  archivePrefix = {arXiv},
  eprint = {1902.10877},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/4RTQCRWI/Kim and Kang - 2019 - Financial series prediction using Attention LSTM.pdf},
  journal = {arXiv:1902.10877 [cs, q-fin, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Quantitative Finance - Statistical Finance,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, q-fin, stat}
}

@article{kimSoftFlowProbabilisticFramework2020,
  title = {{{SoftFlow}}: {{Probabilistic Framework}} for {{Normalizing Flow}} on {{Manifolds}}},
  shorttitle = {{{SoftFlow}}},
  author = {Kim, Hyeongju and Lee, Hyeonseung and Kang, Woo Hyun and Lee, Joun Yeop and Kim, Nam Soo},
  year = {2020},
  month = jun,
  abstract = {Flow-based generative models are composed of invertible transformations between two random variables of the same dimension. Therefore, flow-based models cannot be adequately trained if the dimension of the data distribution does not match that of the underlying target distribution. In this paper, we propose SoftFlow, a probabilistic framework for training normalizing flows on manifolds. To sidestep the dimension mismatch problem, SoftFlow estimates a conditional distribution of the perturbed input data instead of learning the data distribution directly. We experimentally show that SoftFlow can capture the innate structure of the manifold data and generate high-quality samples unlike the conventional flow-based models. Furthermore, we apply the proposed framework to 3D point clouds to alleviate the difficulty of forming thin structures for flow-based models. The proposed model for 3D point clouds, namely SoftPointFlow, can estimate the distribution of various shapes more accurately and achieves state-of-the-art performance in point cloud generation.},
  archivePrefix = {arXiv},
  eprint = {2006.04604},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/XRWH5ZJF/Kim et al. - 2020 - SoftFlow Probabilistic Framework for Normalizing .pdf},
  journal = {arXiv:2006.04604 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{kingmaAutoEncodingVariationalBayes2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  month = may,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archivePrefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/YLDKGK76/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf},
  journal = {arXiv:1312.6114 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kingmaIntroductionVariationalAutoencoders2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2019},
  volume = {12},
  pages = {307--392},
  issn = {1935-8237, 1935-8245},
  doi = {10/ggfm34},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  archivePrefix = {arXiv},
  eprint = {1906.02691},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/P4U6RIQU/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf},
  journal = {FNT in Machine Learning},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  number = {4}
}

@article{kiskirasPortfolioRebalancingStable2013,
  title = {Portfolio {{Rebalancing}}: {{A Stable Source}} of {{Alpha}}?},
  shorttitle = {Portfolio {{Rebalancing}}},
  author = {Kiskiras, John and Nardon, Andrea},
  year = {2013},
  issn = {1556-5068},
  doi = {10/gd535d},
  abstract = {In this work we verify that portfolio rebalancing can generate an excess return under certain market conditions. In line with existing measures, developed specifically to capture that alpha (Rebalancing Bonus), we show that high volatility as well as low correlation maximize the magnitude of the excess return. However, in contrast to previous works, we demonstrate that the actual driver and therefore sufficient condition for a Rebalancing Bonus is the presence of relative mean-reversion. All results in the present note are supported via numerical simulations and a case study on global equities.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/39KE58T2/Kiskiras and Nardon - 2013 - Portfolio Rebalancing A Stable Source of Alpha.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{kitaevReformerEfficientTransformer2020,
  title = {Reformer: {{The Efficient Transformer}}},
  shorttitle = {Reformer},
  author = {Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  year = {2020},
  month = feb,
  abstract = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L2) to O(L log L), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
  archivePrefix = {arXiv},
  eprint = {2001.04451},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/6NKA3XYV/Kitaev et al. - 2020 - Reformer The Efficient Transformer.pdf},
  journal = {arXiv:2001.04451 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kobyzevNormalizingFlowsIntroduction2020,
  title = {Normalizing {{Flows}}: {{An Introduction}} and {{Review}} of {{Current Methods}}},
  shorttitle = {Normalizing {{Flows}}},
  author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
  year = {2020},
  month = jun,
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10/gg6723},
  abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
  archivePrefix = {arXiv},
  eprint = {1908.09257},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/RYZUCSDN/Kobyzev et al. - 2020 - Normalizing Flows An Introduction and Review of C.pdf},
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en}
}

@article{kobyzevNormalizingFlowsIntroduction2020a,
  title = {Normalizing {{Flows}}: {{An Introduction}} and {{Review}} of {{Current Methods}}},
  shorttitle = {Normalizing {{Flows}}},
  author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
  year = {2020},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.2992934},
  abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
  archivePrefix = {arXiv},
  eprint = {1908.09257},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/82ASJC66/Kobyzev et al. - 2020 - Normalizing Flows An Introduction and Review of C.pdf},
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en}
}

@article{kritzmanPortfolioRebalancingTest2007,
  title = {Portfolio {{Rebalancing}}: {{A Test}} of the {{Markowitz}}-{{Van Dijk Heuristic}}},
  shorttitle = {Portfolio {{Rebalancing}}},
  author = {Kritzman, Mark and Myrgren, Simon and Page, Sebastien},
  year = {2007},
  issn = {1556-5068},
  doi = {10/fx6bv6},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/24DETL9U/Kritzman et al. - 2007 - Portfolio Rebalancing A Test of the Markowitz-Van.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{krokhmalComparativeAnalysisLinear2002,
  title = {Comparative {{Analysis}} of {{Linear Portfolio Rebalancing Strategies}}: {{An Application}} to {{Hedge Funds}}},
  shorttitle = {Comparative {{Analysis}} of {{Linear Portfolio Rebalancing Strategies}}},
  author = {Krokhmal, Pavlo A. and Uryasev, Stanislav P. and Zrazhevsky, Grigory M.},
  year = {2002},
  issn = {1556-5068},
  doi = {10/b4xjp7},
  abstract = {This paper applies formal risk management methodologies to optimization of a portfolio of hedge funds (fund of funds). We compare recently developed risk management methodologies: Conditional Value-at-Risk and Conditional Drawdown-atRisk with more established Mean-Absolute Deviation, Maximum Loss, and Market Neutrality approaches. The common property of considered risk management techniques is that they admit the formulation of a portfolio optimization model as a linear programming (LP) problem. LP formulations allow for implementing efficient and robust portfo lio allocation algorithms, which can successfully handle optimization problems with thousands of instruments and scenarios. The performance of various risk constraints is investigated and discussed in detail for in-sample and out-of-sample testing of the algorithm. The numerical experiments show that imposing risk constraints may improve the ``real'' performance of a portfolio rebalancing strategy in out-of-sample runs. It is beneficial to combine several types of risk constraints that control different sources of risk.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/D2888LG6/Krokhmal et al. - 2002 - Comparative Analysis of Linear Portfolio Rebalanci.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{krokhmalNumericalComparisonCVaR,
  title = {Numerical {{Comparison}} of {{CVaR}} and {{CDaR Approaches}}: {{Application}} to {{Hedge Funds}}},
  author = {Krokhmal, Pavlo and Uryasev, Stanislav and Zrazhevsky, Grigory},
  pages = {25},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/KHVXKUES/Krokhmal et al. - Numerical Comparison of CVaR and CDaR Approaches .pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@article{krokhmalPortfolioOptimizationConditional2001,
  title = {Portfolio Optimization with Conditional Value-at-Risk Objective and Constraints},
  author = {Krokhmal, Pavlo and Uryasev, tanislav and Palmquist, Jonas},
  year = {2001},
  month = mar,
  volume = {4},
  pages = {43--68},
  issn = {14651211},
  doi = {10/gd535h},
  abstract = {Recently, a new approach for optimization of Conditional Value-at-Risk (CVaR) was suggested and tested with several applications. For continuous distributions, CVaR is defined as the expected loss exceeding Value-at Risk (VaR). However, generally, CVaR is the weighted average of VaR and losses exceeding VaR. Central to the approach is an optimization technique for calculating VaR and optimizing CVaR simultaneously. This paper extends this approach to the optimization problems with CVaR constraints. In particular, the approach can be used for maximizing expected returns under CVaR constraints. Multiple CVaR constraints with various confidence levels can be used to shape the profit/loss distribution. A case study for the portfolio of S\&P 100 stocks is performed to demonstrate how the new optimization techniques can be implemented.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/ZBWI3LZB/Krokhmal et al. - 2001 - Portfolio optimization with conditional value-at-r.pdf},
  journal = {The Journal of Risk},
  language = {en},
  number = {2}
}

@incollection{krokhmalUseConditionalValueatRisk2004,
  title = {Use of {{Conditional Value}}-at-{{Risk}} in {{Stochastic Programs}} with {{Poorly Defined Distributions}}},
  booktitle = {Recent {{Developments}} in {{Cooperative Control}} and {{Optimization}}},
  author = {Krokhmal, Pavlo and Murphey, Robert and Pardalos, Panos and Uryasev, Stanislav},
  editor = {Murphey, Robert and Pardalos, Panos M. and Butenko, Sergiy and Murphey, Robert and Pardalos, Panos M.},
  year = {2004},
  volume = {3},
  pages = {225--241},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4613-0219-3_13},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/EBBZE9T2/Krokhmal et al. - 2004 - Use of Conditional Value-at-Risk in Stochastic Pro.pdf},
  isbn = {978-1-4613-7947-8 978-1-4613-0219-3},
  language = {en}
}

@inproceedings{kulkarniPictureProbabilisticProgramming2015,
  title = {Picture: {{A}} Probabilistic Programming Language for Scene Perception},
  shorttitle = {Picture},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kulkarni, Tejas D and Kohli, Pushmeet and Tenenbaum, Joshua B and Mansinghka, Vikash},
  year = {2015},
  month = jun,
  pages = {4390--4399},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7299068},
  abstract = {Recent progress on probabilistic modeling and statistical learning, coupled with the availability of large training datasets, has led to remarkable progress in computer vision. Generative probabilistic models, or ``analysis-by-synthesis'' approaches, can capture rich scene structure but have been less widely applied than their discriminative counterparts, as they often require considerable problem-specific engineering in modeling and inference, and inference is typically seen as requiring slow, hypothesize-and-test Monte Carlo methods. Here we present Picture, a probabilistic programming language for scene understanding that allows researchers to express complex generative vision models, while automatically solving them using fast general-purpose inference machinery. Picture provides a stochastic scene language that can express generative models for arbitrary 2D/3D scenes, as well as a hierarchy of representation layers for comparing scene hypotheses with observed images by matching not simply pixels, but also more abstract features (e.g., contours, deep neural network activations). Inference can flexibly integrate advanced Monte Carlo strategies with fast bottomup data-driven methods. Thus both representations and inference strategies can build directly on progress in discriminatively trained systems to make generative vision more robust and efficient. We use Picture to write programs for 3D face analysis, 3D human pose estimation, and 3D object reconstruction \textendash{} each competitive with specially engineered baselines.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/BCYP4U9H/Kulkarni et al. - 2015 - Picture A probabilistic programming language for .pdf},
  isbn = {978-1-4673-6964-0},
  language = {en}
}

@article{lanALBERTLiteBERT2020,
  title = {{{ALBERT}}: {{A Lite BERT}} for {{Self}}-Supervised {{Learning}} of {{Language Representations}}},
  shorttitle = {{{ALBERT}}},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  year = {2020},
  month = feb,
  abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameterreduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
  archivePrefix = {arXiv},
  eprint = {1909.11942},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/J8WWSZYN/Lan et al. - 2020 - ALBERT A Lite BERT for Self-supervised Learning o.pdf},
  journal = {arXiv:1909.11942 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  language = {en},
  primaryClass = {cs}
}

@misc{LatentOrdinaryDifferential2019,
  title = {Latent {{Ordinary Differential Equations}} for {{Irregularly}}-{{Sampled Time Series}}},
  year = {2019},
  month = dec,
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/Latent Ordinary Differential Equations for Irregularly-Sampled Time Series.pdf;/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/9836VQT4/8773-latent-ordinary-differential-equations-for-irregularly-sampled-time-series-supplemental.zip;/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/V9VVF7L9/8773-latent-ordinary-differential-equations-for-irregularly-sampled-time-series.html},
  howpublished = {https://papers.nips.cc/paper/8773-latent-ordinary-differential-equations-for-irregularly-sampled-time-series}
}

@article{launayDirectFeedbackAlignment2020,
  title = {Direct {{Feedback Alignment Scales}} to {{Modern Deep Learning Tasks}} and {{Architectures}}},
  author = {Launay, Julien and Poli, Iacopo and Boniface, Fran{\c c}ois and Krzakala, Florent},
  year = {2020},
  month = jun,
  abstract = {Despite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.},
  archivePrefix = {arXiv},
  eprint = {2006.12878},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/PPSFXEP2/Launay et al. - 2020 - Direct Feedback Alignment Scales to Modern Deep Le.pdf},
  journal = {arXiv:2006.12878 [cs, stat]},
  keywords = {‚õî No DOI found,68T07 (Primary) 68T05 (Secondary),Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.6,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{LearningDifferentialEquations2020,
  title = {Learning {{Differential Equations}} That Are {{Easy}} to {{Solve}}},
  year = {2020},
  month = jul,
  pages = {17},
  abstract = {Differential equations parameterized by neural networks become expensive to solve numerically as training progresses. We propose a remedy that encourages learned dynamics to be easier to solve. Specifically, we introduce a differentiable surrogate for the time cost of standard numerical solvers, using higher-order derivatives of solution trajectories. These derivatives are efficient to compute with Taylormode automatic differentiation. Optimizing this additional objective trades model performance against the time cost of solving the learned dynamics. We demonstrate our approach by training substantially faster, while nearly as accurate, models in supervised classification, density estimation, and time-series modelling tasks.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/BL72ZCJC/Learning Differential Equations that are Easy to S.pdf},
  language = {en}
}

@article{leblancCombiningEstimatesRegression,
  title = {Combining Estimates in Regression and Classi Cation},
  author = {LeBlanc, Michael},
  pages = {21},
  abstract = {We consider the problem of how to combine a collection of general regression t vectors in order to obtain a better predictive model. The individual ts may be from subset linear regression, ridge regression, or something more complex like a neural network. We develop a general framework for this problem and examine a recent cross-validation-based proposal called \textbackslash stacking" in this context. Combination methods based on the bootstrap and analytic methods are also derived and compared in a number of examples, including best subsets regression and regression trees. Finally, we apply these ideas to classi cation problems where the estimated combination weights can yield insight into the structure of the problem.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/8HCE4NPV/LeBlanc - Combining estimates in regression and classi catio.pdf},
  language = {en}
}

@article{leeRandomCoefficientAutoregressive2006,
  title = {A Random Coefficient Autoregressive {{Markov}} Regime Switching Model for Dynamic Futures Hedging},
  author = {Lee, Hsiang-Tai and Yoder, Jonathan K. and Mittelhammer, Ron C. and McCluskey, Jill J.},
  year = {2006},
  month = feb,
  volume = {26},
  pages = {103--129},
  issn = {0270-7314, 1096-9934},
  doi = {10.1002/FUT.20193},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/A395R93C/Lee et al. - 2006 - A random coefficient autoregressive Markov regime .pdf},
  journal = {Journal of Futures Markets},
  language = {en},
  number = {2}
}

@article{lererImprovingPoliciesSearch2019,
  title = {Improving {{Policies}} via {{Search}} in {{Cooperative Partially Observable Games}}},
  author = {Lerer, Adam and Hu, Hengyuan and Foerster, Jakob and Brown, Noam},
  year = {2019},
  month = dec,
  abstract = {Recent superhuman results in games have largely been achieved in a variety of zero-sum settings, such as Go and Poker, in which agents need to compete against others. However, just like humans, real-world AI systems have to coordinate and communicate with other agents in cooperative partially observable environments as well. These settings commonly require participants to both interpret the actions of others and to act in a way that is informative when being interpreted. Those abilities are typically summarized as theory of mind and are seen as crucial for social interactions. In this paper we propose two different search techniques that can be applied to improve an arbitrary agreed-upon policy in a cooperative partially observable game. The first one, single-agent search, effectively converts the problem into a single agent setting by making all but one of the agents play according to the agreed-upon policy. In contrast, in multi-agent search all agents carry out the same common-knowledge search procedure whenever doing so is computationally feasible, and fall back to playing according to the agreed-upon policy otherwise. We prove that these search procedures are theoretically guaranteed to at least maintain the original performance of the agreed-upon policy (up to a bounded approximation error). In the benchmark challenge problem of Hanabi, our search technique greatly improves the performance of every agent we tested and when applied to a policy trained using RL achieves a new state-of-the-art score of 24.61 / 25 in the game, compared to a previous-best of 24.08 / 25.},
  archivePrefix = {arXiv},
  eprint = {1912.02318},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/MBCVKBW6/Lerer et al. - 2019 - Improving Policies via Search in Cooperative Parti.pdf},
  journal = {arXiv:1912.02318 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  language = {en},
  primaryClass = {cs}
}

@article{leuvenPortfolioOptimizationCVaR2010,
  title = {Portfolio Optimization with {{CVaR}} Budgets},
  author = {Leuven, K U},
  year = {2010},
  pages = {44},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/VYVL3ZZU/Leuven - 2010 - Portfolio optimization with CVaR budgets.pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@article{levy19thWorldComputerBridge2017,
  title = {19th {{World Computer}}-{{Bridge Championship}}},
  author = {Levy, Alvin},
  year = {2017},
  month = may,
  volume = {39},
  pages = {72--76},
  issn = {13896911, 24682438},
  doi = {10/gd6dxs},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/NG6N7BGK/Levy - 2017 - 19th World Computer-Bridge Championship.pdf},
  journal = {ICGA Journal},
  language = {en},
  number = {1}
}

@misc{LibrariesRecommenderSystems,
  title = {R Libraries for Recommender Systems},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/BL5TAAYU/77c90db326b4848368287e53b1a18e8d.html},
  howpublished = {https://gist.github.com/talegari/77c90db326b4848368287e53b1a18e8d}
}

@article{liEnhancingLocalityBreaking2020,
  title = {Enhancing the {{Locality}} and {{Breaking}} the {{Memory Bottleneck}} of {{Transformer}} on {{Time Series Forecasting}}},
  author = {Li, Shiyang and Jin, Xiaoyong and Xuan, Yao and Zhou, Xiyou and Chen, Wenhu and Wang, Yu-Xiang and Yan, Xifeng},
  year = {2020},
  month = jan,
  abstract = {Time series forecasting is an important problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. In this paper, we propose to tackle such forecasting problem with Transformer [1]. Although impressed by its performance in our preliminary study, we found its two major weaknesses: (1) locality-agnostics: the point-wise dotproduct self-attention in canonical Transformer architecture is insensitive to local context, which can make the model prone to anomalies in time series; (2) memory bottleneck: space complexity of canonical Transformer grows quadratically with sequence length L, making directly modeling long time series infeasible. In order to solve these two issues, we first propose convolutional self-attention by producing queries and keys with causal convolution so that local context can be better incorporated into attention mechanism. Then, we propose LogSparse Transformer with only O(L(log L)2) memory cost, improving forecasting accuracy for time series with fine granularity and strong long-term dependencies under constrained memory budget. Our experiments on both synthetic data and realworld datasets show that it compares favorably to the state-of-the-art.},
  archivePrefix = {arXiv},
  eprint = {1907.00235},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/WSCNF3RQ/Li et al. - 2020 - Enhancing the Locality and Breaking the Memory Bot.pdf},
  journal = {arXiv:1907.00235 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{limRecurrentNeuralFilters2020,
  title = {Recurrent {{Neural Filters}}: {{Learning Independent Bayesian Filtering Steps}} for {{Time Series Prediction}}},
  shorttitle = {Recurrent {{Neural Filters}}},
  author = {Lim, Bryan and Zohren, Stefan and Roberts, Stephen},
  year = {2020},
  month = mar,
  abstract = {Despite the recent popularity of deep generative state space models, few comparisons have been made between network architectures and the inference steps of the Bayesian filtering framework \textendash{} with most models simultaneously approximating both state transition and update steps with a single recurrent neural network (RNN). In this paper, we introduce the Recurrent Neural Filter (RNF), a novel recurrent autoencoder architecture that learns distinct representations for each Bayesian filtering step, captured by a series of encoders and decoders. Testing this on three real-world time series datasets, we demonstrate that the decoupled representations learnt improve the accuracy of one-step-ahead forecasts while providing realistic uncertainty estimates, and also facilitate multistep prediction through the separation of encoder stages.},
  archivePrefix = {arXiv},
  eprint = {1901.08096},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/788PLQ6M/Lim et al. - 2020 - Recurrent Neural Filters Learning Independent Bay.pdf},
  journal = {arXiv:1901.08096 [cs, eess, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, eess, stat}
}

@article{limTemporalFusionTransformers2020,
  title = {Temporal {{Fusion Transformers}} for {{Interpretable Multi}}-Horizon {{Time Series Forecasting}}},
  author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
  year = {2020},
  month = may,
  abstract = {Multi-horizon forecasting often contains a complex mix of inputs \textendash{} including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed in the past \textendash{} without any prior information on how they interact with the target. Several deep learning methods have been proposed, but they are typically `black-box' models which do not shed light on how they use the full range of inputs present in practical scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) \textendash{} a novel attentionbased architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, TFT uses recurrent layers for local processing and interpretable self-attention layers for long-term dependencies. TFT utilizes specialized components to select relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of scenarios. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use cases of TFT.},
  archivePrefix = {arXiv},
  eprint = {1912.09363},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/DVD3WITM/Lim et al. - 2020 - Temporal Fusion Transformers for Interpretable Mul.pdf},
  journal = {arXiv:1912.09363 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{limTemporalFusionTransformers2020a,
  title = {Temporal {{Fusion Transformers}} for {{Interpretable Multi}}-Horizon {{Time Series Forecasting}}},
  author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
  year = {2020},
  month = may,
  abstract = {Multi-horizon forecasting often contains a complex mix of inputs \textendash{} including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed in the past \textendash{} without any prior information on how they interact with the target. Several deep learning methods have been proposed, but they are typically `black-box' models which do not shed light on how they use the full range of inputs present in practical scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) \textendash{} a novel attentionbased architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, TFT uses recurrent layers for local processing and interpretable self-attention layers for long-term dependencies. TFT utilizes specialized components to select relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of scenarios. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use cases of TFT.},
  archivePrefix = {arXiv},
  eprint = {1912.09363},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/F3JJGPY3/Lim et al. - 2020 - Temporal Fusion Transformers for Interpretable Mul.pdf},
  journal = {arXiv:1912.09363 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{limTimeSeriesForecasting2020,
  title = {Time {{Series Forecasting With Deep Learning}}: {{A Survey}}},
  shorttitle = {Time {{Series Forecasting With Deep Learning}}},
  author = {Lim, Bryan and Zohren, Stefan},
  year = {2020},
  month = apr,
  abstract = {Numerous deep learning architectures have been developed to accommodate the diversity of time series datasets across different domains. In this article, we survey common encoder and decoder designs used in both one-step-ahead and multi-horizon time series forecasting -- describing how temporal information is incorporated into predictions by each model. Next, we highlight recent developments in hybrid deep learning models, which combine well-studied statistical models with neural network components to improve pure methods in either category. Lastly, we outline some ways in which deep learning can also facilitate decision support with time series data.},
  archivePrefix = {arXiv},
  eprint = {2004.13408},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/JANK7GIB/Lim and Zohren - 2020 - Time Series Forecasting With Deep Learning A Surv.pdf},
  journal = {arXiv:2004.13408 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{linDataEfficientMutualInformation2019,
  title = {Data-{{Efficient Mutual Information Neural Estimator}}},
  author = {Lin, Xiao and Sur, Indranil and Nastase, Samuel A. and Divakaran, Ajay and Hasson, Uri and Amer, Mohamed R.},
  year = {2019},
  month = may,
  abstract = {Measuring Mutual Information (MI) between high-dimensional, continuous, random variables from observed samples has wide theoretical and practical applications. Recent work, MINE [5], focused on estimating tight variational lower bounds of MI using neural networks, but assumed unlimited supply of samples to prevent overfitting. In real world applications, data is not always available at a surplus. In this work, we focus on improving data efficiency and propose a Data-Efficient MINE Estimator (DEMINE), by developing a relaxed predictive MI lower bound that can be estimated at higher data efficiency by orders of magnitudes. The predictive MI lower bound also enables us to develop a new meta-learning approach using task augmentation, Meta-DEMINE, to improve generalization of the network and further boost estimation accuracy empirically. With improved data-efficiency, our estimators enables statistical testing of dependency at practical dataset sizes. We demonstrate the effectiveness of our estimators on synthetic benchmarks and a real world fMRI data, with application of inter-subject correlation analysis.},
  archivePrefix = {arXiv},
  eprint = {1905.03319},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/4NFTZVAA/Lin et al. - 2019 - Data-Efficient Mutual Information Neural Estimator.pdf},
  journal = {arXiv:1905.03319 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{liScalableGradientsStochastic2020,
  title = {Scalable {{Gradients}} for {{Stochastic Differential Equations}}},
  author = {Li, Xuechen and Wong, Ting-Kam Leonard and Chen, Ricky T. Q. and Duvenaud, David},
  year = {2020},
  month = jul,
  abstract = {The adjoint sensitivity method scalably computes gradients of solutions to ordinary differential equations. We generalize this method to stochastic differential equations, allowing time-efficient and constant-memory computation of gradients with high-order adaptive solvers. Specifically, we derive a stochastic differential equation whose solution is the gradient, a memory-efficient algorithm for caching noise, and conditions under which numerical solutions converge. In addition, we combine our method with gradient-based stochastic variational inference for latent stochastic differential equations. We use our method to fit stochastic dynamics defined by neural networks, achieving competitive performance on a 50-dimensional motion capture dataset.},
  archivePrefix = {arXiv},
  eprint = {2001.01328},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/JW3VIKFY/Li et al. - 2020 - Scalable Gradients for Stochastic Differential Equ.pdf},
  journal = {arXiv:2001.01328 [cs, math, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{littermanForecastingBayesianVector1986,
  title = {Forecasting with {{Bayesian Vector Autoregressions}}: {{Five Years}} of {{Experience}}},
  shorttitle = {Forecasting with {{Bayesian Vector Autoregressions}}},
  author = {Litterman, Robert B.},
  year = {1986},
  month = jan,
  volume = {4},
  pages = {25},
  issn = {07350015},
  doi = {10.2307/1391384},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/M6B8RMIH/Litterman - 1986 - Forecasting with Bayesian Vector Autoregressions .PDF},
  journal = {Journal of Business \& Economic Statistics},
  language = {en},
  number = {1}
}

@article{louNeuralManifoldOrdinary2020,
  title = {Neural {{Manifold Ordinary Differential Equations}}},
  author = {Lou, Aaron and Lim, Derek and Katsman, Isay and Huang, Leo and Jiang, Qingxuan and Lim, Ser-Nam and De Sa, Christopher},
  year = {2020},
  month = jun,
  abstract = {To better conform to data geometry, recent deep generative modelling techniques adapt Euclidean constructions to non-Euclidean spaces. In this paper, we study normalizing flows on manifolds. Previous work has developed flow models for specific cases; however, these advancements hand craft layers on a manifold-by-manifold basis, restricting generality and inducing cumbersome design constraints. We overcome these issues by introducing Neural Manifold Ordinary Differential Equations, a manifold generalization of Neural ODEs, which enables the construction of Manifold Continuous Normalizing Flows (MCNFs). MCNFs require only local geometry (therefore generalizing to arbitrary manifolds) and compute probabilities with continuous change of variables (allowing for a simple and expressive flow construction). We find that leveraging continuous manifold dynamics produces a marked improvement for both density estimation and downstream tasks.},
  archivePrefix = {arXiv},
  eprint = {2006.10254},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/A3VJYU5C/Lou et al. - 2020 - Neural Manifold Ordinary Differential Equations.pdf},
  journal = {arXiv:2006.10254 [cs, math, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Mathematics - Differential Geometry,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{lucasESLProgrammingLanguage,
  title = {The {{ESL Programming Language}}},
  author = {Lucas, Brian G},
  pages = {38},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/2KG2IJ2B/Lucas - The ESL Programming Language.pdf},
  language = {en}
}

@article{maddisonMOVEEVALUATIONGO2015,
  title = {{{MOVE EVALUATION IN GO USING DEEP CONVOLUTIONAL NEURAL NETWORKS}}},
  author = {Maddison, Chris J and Huang, Aja and Sutskever, Ilya and Silver, David},
  year = {2015},
  pages = {8},
  abstract = {The game of Go is more challenging than other board games, due to the difficulty of constructing a position or move evaluation function. In this paper we investigate whether deep convolutional networks can be used to directly represent and learn this knowledge. We train a large 12-layer convolutional neural network by supervised learning from a database of human professional games. The network correctly predicts the expert move in 55\% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional-search program GnuGo in 97\% of games, and matched the performance of a state-of-the-art Monte-Carlo tree search that simulates two million positions per move.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/GKUH43AP/Maddison et al. - 2015 - MOVE EVALUATION IN GO USING DEEP CONVOLUTIONAL NEU.pdf},
  language = {en}
}

@article{mahajanNormalizingFlowsMultiScale2020,
  title = {Normalizing {{Flows}} with {{Multi}}-{{Scale Autoregressive Priors}}},
  author = {Mahajan, Shweta and Bhattacharyya, Apratim and Fritz, Mario and Schiele, Bernt and Roth, Stefan},
  year = {2020},
  month = apr,
  abstract = {Flow-based generative models are an important class of exact inference models that admit efficient inference and sampling for image synthesis. Owing to the efficiency constraints on the design of the flow layers, e.g. split coupling flow layers in which approximately half the pixels do not undergo further transformations, they have limited expressiveness for modeling long-range data dependencies compared to autoregressive models that rely on conditional pixel-wise generation. In this work, we improve the representational power of flow-based models by introducing channel-wise dependencies in their latent space through multi-scale autoregressive priors (mAR). Our mAR prior for models with split coupling flow layers (mAR-SCF) can better capture dependencies in complex multimodal data. The resulting model achieves state-of-the-art density estimation results on MNIST, CIFAR-10, and ImageNet. Furthermore, we show that mAR-SCF allows for improved image generation quality, with gains in FID and Inception scores compared to state-of-the-art flow-based models.},
  archivePrefix = {arXiv},
  eprint = {2004.03891},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/SYSFM8I6/Mahajan et al. - 2020 - Normalizing Flows with Multi-Scale Autoregressive .pdf},
  journal = {arXiv:2004.03891 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{maPortfolioOptimizationModel,
  title = {A {{Portfolio Optimization Model}} with {{Regime}}-{{Switching Risk Factors}} for {{Sector Exchange Traded Funds}}},
  author = {Ma, Ying and MacLean, Leonard and Xu, Kuan and Zhao, Yonggan},
  pages = {23},
  abstract = {This paper develops a portfolio optimization model with a market neutral strategy under a Markov regime-switching framework. The selected investment instruments consist of the nine sector exchange traded funds (ETFs) that represent the U.S. stock market. The Bayesian information criterion is used to determine the optimal number of regimes. The investment objective is to dynamically maximize the portfolio alpha (excess return over the T-Bill) subject to neutralization of the portfolio sensitivities to the selected risk factors. The portfolio risk exposures are shown to change with various style and macro factors over time. The maximization problem in this context can be established as a regime-dependent linear programming problem. The optimal portfolio constructed as such is expected to outperform a naive benchmark strategy, which equally weights the ETFs. We evaluate the in-sample and out-of-sample performance of the regime-dependent market neutral strategy against the equally weighted strategy. We find that the former generally outperforms the latter.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/LBEYLJFX/Ma et al. - A Portfolio Optimization Model with Regime-Switchi.pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@misc{MarketRegimesSectorial,
  title = {Market {{Regimes}}, {{Sectorial Investments}}, and {{Time}}-{{Varying Risk Premiums}} by {{Kuan Xu}}, {{Payton LIu}}, {{Yonggan Zhao}} :: {{SSRN}}},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/Market Regimes, Sectorial Investments, and Time-Varying Risk Premiums by Kuan.pdf},
  howpublished = {https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=1571873}
}

@article{marquetteGeneralPurposeProbabilistic2018,
  title = {General {{Purpose Probabilistic Programming Platform}} with {{Effective Stochastic Inference}}},
  author = {Marquette, Kathryn W Ctr Usaf Afmc Afrl/Ritf},
  year = {2018},
  month = apr,
  pages = {87},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/VHF7U56H/Marquette - General Purpose Probabilistic Programming Platform.pdf},
  keywords = {‚õî No DOI found},
  language = {en}
}

@article{martinMonteCarloTransformer2020,
  title = {The {{Monte Carlo Transformer}}: A Stochastic Self-Attention Model for Sequence Prediction},
  shorttitle = {The {{Monte Carlo Transformer}}},
  author = {Martin, Alice and Ollion, Charles and Strub, Florian and Corff, Sylvain Le and Pietquin, Olivier},
  year = {2020},
  month = jul,
  abstract = {This paper introduces the Sequential Monte Carlo Transformer, an original approach that naturally captures the observations distribution in a recurrent architecture. The keys, queries, values and attention vectors of the network are considered as the unobserved stochastic states of its hidden structure. This generative model is such that at each time step the received observation is a random function of these past states in a given attention window. In this general state-space setting, we use Sequential Monte Carlo methods to approximate the posterior distributions of the states given the observations, and then to estimate the gradient of the log-likelihood. We thus propose a generative model providing a predictive distribution, instead of a single-point estimate.},
  archivePrefix = {arXiv},
  eprint = {2007.08620},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/U4QSN4X9/Martin et al. - 2020 - The Monte Carlo Transformer a stochastic self-att.pdf},
  journal = {arXiv:2007.08620 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{mathieuRiemannianContinuousNormalizing2020,
  title = {Riemannian {{Continuous Normalizing Flows}}},
  author = {Mathieu, Emile and Nickel, Maximilian},
  year = {2020},
  month = jun,
  abstract = {Normalizing flows have shown great promise for modelling flexible probability distributions in a computationally tractable way. However, whilst data is often naturally described on Riemannian manifolds such as spheres, torii, and hyperbolic spaces, most normalizing flows implicitly assume a flat geometry, making them either misspecified or ill-suited in these situations. To overcome this problem, we introduce Riemannian continuous normalizing flows, a model which admits the parametrization of flexible probability measures on smooth manifolds by defining flows as the solution to ordinary differential equations. We show that this approach can lead to substantial improvements on both synthetic and real-world data when compared to standard flows or previously introduced projected flows.},
  archivePrefix = {arXiv},
  eprint = {2006.10605},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/3GCQMR9C/Mathieu and Nickel - 2020 - Riemannian Continuous Normalizing Flows.pdf},
  journal = {arXiv:2006.10605 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@book{mcdonnellOptimalPortfolioModeling2012,
  title = {Optimal {{Portfolio Modeling}}: {{Models}} to {{Maximize Return}} and {{Control Risk}} in {{Excel}} and {{R}} + {{CD}}-{{ROM}}},
  shorttitle = {Optimal {{Portfolio Modeling}}},
  editor = {McDonnell, Philip J.},
  year = {2012},
  month = jan,
  publisher = {{John Wiley \& Sons, Inc.}},
  address = {{Hoboken, NJ, USA}},
  doi = {10.1002/9781119197515},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/AUYIWY4M/McDonnell - 2012 - Optimal Portfolio Modeling Models to Maximize Ret.pdf},
  isbn = {978-1-119-19751-5 978-0-470-11766-8},
  language = {en}
}

@article{medoHowQuantifyInfluence2009,
  title = {How to Quantify the Influence of Correlations on Investment Diversification},
  author = {Medo, Matus and Yeung, Chi Ho and Zhang, Yi-Cheng},
  year = {2009},
  month = mar,
  volume = {18},
  pages = {34--39},
  issn = {10575219},
  doi = {10.1016/j.irfa.2009.01.001},
  abstract = {When assets are correlated, benefits of investment diversification are reduced. To measure the influence of correlations on investment performance, a new quantity\textemdash the effective portfolio size\textemdash is proposed and investigated in both artificial and real situations. We show that in most cases, the effective portfolio size is much smaller than the actual number of assets in the portfolio and that it lowers even further during financial crises.},
  archivePrefix = {arXiv},
  eprint = {0805.3397},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/RMHCJ7DJ/Medo et al. - 2009 - How to quantify the influence of correlations on i.3397},
  journal = {International Review of Financial Analysis},
  keywords = {Physics - Physics and Society,Quantitative Finance - Portfolio Management},
  language = {en},
  number = {1-2}
}

@article{meucciBlackLittermanApproachOriginal2008,
  title = {The {{Black}}-{{Litterman Approach}}: {{Original Model}} and {{Extensions}}},
  shorttitle = {The {{Black}}-{{Litterman Approach}}},
  author = {Meucci, Attilio},
  year = {2008},
  issn = {1556-5068},
  doi = {10.2139/ssrn.1117574},
  abstract = {We walk the reader through the Black-Litterman approach, providing all the proofs. We show how minor modifications of the original model greatly improve its range of applications. We discuss full generalizations of this and related models. Code is available at MATLAB Central File Exchange.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/42BJVQSP/Meucci - 2008 - The Black-Litterman Approach Original Model and E.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{meucciFullyFlexibleBayesian,
  title = {Fully {{Flexible Bayesian Networks}}},
  author = {Meucci, Attilio},
  pages = {11},
  abstract = {We propose a methodology to stress-test a set of risk drivers under minimal information. This methodology applies the entropy-based "fully flexible views" approach in Meucci (2008) and a novel consistency algorithm to extend the Bayesian network approach in Rebonato (2010). Starting from a plausible market distribution of the drivers, we are able to stress any conditional probabilities. These conditional probabilities comprise, but are not restricted to, causal Bayesian networks. Furthermore, in our approach we can also stress-test expectations, volatilities, correlations, quantiles, medians, etc.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/37DD5ZZ6/Meucci - Fully Flexible Bayesian Networks.pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@article{meucciFullyFlexibleViews,
  title = {Fully {{Flexible Views}}: {{Theory}} and {{Practice}}},
  author = {Meucci, Attilio},
  pages = {27},
  abstract = {We propose a unified methodology to input non-linear views from any number of users in fully general non-normal markets, and perform, among others, stresstesting, scenario analysis, and ranking allocation. We walk the reader through the theory and we detail an extremely efficient algorithm to easily implement this methodology under fully general assumptions. As it turns out, no repricing is ever necessary, hence the methodology can be readily applied to books with complex derivatives. We also present an analytical solution, useful for benchmarking, which per se generalizes notable previous results. Code illustrating this methodology in practice is available at www.symmys.com {$\Rightarrow$} Teaching {$\Rightarrow$} MATLAB.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/S9UE7CRQ/Meucci - Fully Flexible Views Theory and Practice.pdf},
  language = {en}
}

@article{meucciPrayerTenStepChecklist2011,
  title = {`{{The Prayer}}' {{Ten}}-{{Step Checklist}} for {{Advanced Risk}} and {{Portfolio Management}}},
  author = {Meucci, Attilio},
  year = {2011},
  issn = {1556-5068},
  doi = {10/fzj39q},
  abstract = {We present "the Prayer", a foolproof recipe of ten sequential steps for all portfolio managers, risk managers, algorithmic traders across all asset classes and all investment horizons, to model and manage the P\&L distribution of their positions.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/35LZAXBX/Meucci - 2011 - ‚ÄòThe Prayer‚Äô Ten-Step Checklist for Advanced Risk .pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{meucciReviewDynamicAllocation2010,
  title = {Review of {{Dynamic Allocation Strategies}}: {{Utility Maximization}}, {{Option Replication}}, {{Insurance}}, {{Drawdown Control}}, {{Convex}}/{{Concave Management}}},
  shorttitle = {Review of {{Dynamic Allocation Strategies}}},
  author = {Meucci, Attilio},
  year = {2010},
  issn = {1556-5068},
  doi = {10/fzbq3d},
  abstract = {We review the main approaches to dynamically reallocate capital between a risky portfolio and a risk-free account: expected utility maximization; option-based portfolio insurance (OBPI); and drawdown control, closely related to constant proportion portfolio insurance (CPPI). We present a refresher of the theory under general assumptions. We discuss the connections among the different approaches, as well as their relationship with convex and concave strategies. We provide explicit, practicable solutions with all the computations as well as numerical examples. Fully documented code for all the strategies is also provided.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/SETQQKT8/Meucci - 2010 - Review of Dynamic Allocation Strategies Utility M.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{mitchellj.feigenbaumUniversalBehaviourNonLinear1980,
  title = {Universal {{Behaviour}} in {{Non}}-{{Linear Systems}}},
  author = {{Mitchell J. Feigenbaum}},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/3JXH9BN9/tr.pdf},
  journal = {Los Alamos Science}
}

@article{mlnarvikMultistrategyTradingUtilizing,
  title = {Multi-Strategy Trading Utilizing Market Regimes},
  author = {Mlnar{\textasciicaron}{\i}k, Hynek and Ramamoorthy, Subramanian and Savani, Rahul},
  pages = {4},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/IS6HJLNH/MlnarÀáƒ±k et al. - Multi-strategy trading utilizing market regimes.pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@article{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archivePrefix = {arXiv},
  eprint = {1312.5602},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/6U4CS6B5/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf},
  journal = {arXiv:1312.5602 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{molteniEfficientMethodComputation2016,
  title = {An Efficient Method for the Computation of the {{Feigenbaum}} Constants to High Precision},
  author = {Molteni, Andrea},
  year = {2016},
  month = feb,
  abstract = {We propose a new practical algorithm for computing the Feigenbaum constants {$\alpha$} and {$\delta$}, having significantly lower time and space complexity than previously used methods. The algorithm builds upon well-known linear algebra techniques, and is easily parallelizable. An implementation of it has been developed and used to determine both constants to 10 000 decimal places.},
  archivePrefix = {arXiv},
  eprint = {1602.02357},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/X77FYXKU/Molteni - 2016 - An efficient method for the computation of the Fei.pdf},
  journal = {arXiv:1602.02357 [math]},
  keywords = {üîçNo DOI found,11Y60; 26A18,Mathematics - Dynamical Systems,Mathematics - Numerical Analysis},
  language = {en},
  primaryClass = {math}
}

@inproceedings{mueenFastApproximateCorrelation2010,
  title = {Fast Approximate Correlation for Massive Time-Series Data},
  booktitle = {Proceedings of the 2010 International Conference on {{Management}} of Data - {{SIGMOD}} '10},
  author = {Mueen, Abdullah and Nath, Suman and Liu, Jie},
  year = {2010},
  pages = {171},
  publisher = {{ACM Press}},
  address = {{Indianapolis, Indiana, USA}},
  doi = {10.1145/1807167.1807188},
  abstract = {We consider the problem of computing all-pair correlations in a warehouse containing a large number (e.g., tens of thousands) of time-series (or, signals). The problem arises in automatic discovery of patterns and anomalies in data intensive applications such as data center management, environmental monitoring, and scientific experiments. However, with existing techniques, solving the problem for a large stream warehouse is extremely expensive, due to the problem's inherent quadratic I/O and CPU complexities.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/BS8JLFH3/Mueen et al. - 2010 - Fast approximate correlation for massive time-seri.pdf},
  isbn = {978-1-4503-0032-2},
  language = {en}
}

@article{muggletonInductiveLogicProgramming1994,
  title = {Inductive {{Logic Programming}}: {{Theory}} and Methods},
  shorttitle = {Inductive {{Logic Programming}}},
  author = {Muggleton, Stephen and {de Raedt}, Luc},
  year = {1994},
  month = may,
  volume = {19-20},
  pages = {629--679},
  issn = {07431066},
  doi = {10.1016/0743-1066(94)90035-3},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/JLZCESY8/Muggleton and de Raedt - 1994 - Inductive Logic Programming Theory and methods.pdf},
  journal = {The Journal of Logic Programming},
  language = {en}
}

@incollection{muggletonMetaBayesBayesianMetaInterpretative2014,
  title = {{{MetaBayes}}: {{Bayesian Meta}}-{{Interpretative Learning Using Higher}}-{{Order Stochastic Refinement}}},
  shorttitle = {{{MetaBayes}}},
  booktitle = {Inductive {{Logic Programming}}},
  author = {Muggleton, Stephen H. and Lin, Dianhuan and Chen, Jianzhong and {Tamaddoni-Nezhad}, Alireza},
  editor = {Zaverucha, Gerson and Santos Costa, V{\'i}tor and Paes, Aline},
  year = {2014},
  volume = {8812},
  pages = {1--17},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-44923-3_1},
  abstract = {Recent papers have demonstrated that both predicate invention and the learning of recursion can be efficiently implemented by way of abduction with respect to a meta-interpreter. This paper shows how Meta-Interpretive Learning (MIL) can be extended to implement a Bayesian posterior distribution over the hypothesis space by treating the meta-interpreter as a Stochastic Logic Program. The resulting M etaBayes system uses stochastic refinement to randomly sample consistent hypotheses which are used to approximate Bayes' Prediction. Most approaches to Statistical Relational Learning involve separate phases of model estimation and parameter estimation. We show how a variant of the MetaBayes approach can be used to carry out simultaneous model and parameter estimation for a new representation we refer to as a Super-imposed Logic Program (SiLPs). The implementation of this approach is referred to as M etaBayesSiLP . SiLPs are a particular form of ProbLog program, and so the parameters can also be estimated using the more traditional EM approach employed by ProbLog. This second approach is implemented in a new system called M ilP robLog. Experiments are conducted on learning grammars, family relations and a natural language domain. These demonstrate that M etaBayes outperforms M etaBayesMAP in terms of predictive accuracy and also outperforms both M ilP robLog and M etaBayesSiLP on log likelihood measures. However, M etaBayes incurs substantially higher running times than M etaBayesMAP . On the other hand, M etaBayes and M etaBayesSiLP have similar running times while both have much shorter running times than M ilP robLog.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/HSSAAA63/Muggleton et al. - 2014 - MetaBayes Bayesian Meta-Interpretative Learning U.pdf},
  isbn = {978-3-662-44922-6 978-3-662-44923-3},
  language = {en}
}

@article{nairMassivelyParallelMethods,
  title = {Massively {{Parallel Methods}} for {{Deep Reinforcement Learning}}},
  author = {Nair, Arun and Srinivasan, Praveen and Blackwell, Sam and Alcicek, Cagdas and Fearon, Rory and Maria, Alessandro De and Panneershelvam, Vedavyas and Suleyman, Mustafa and Beattie, Charles and Petersen, Stig and Legg, Shane and Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David},
  pages = {9},
  abstract = {We present the first massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm (DQN) (Mnih et al., 2013). Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters. Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/UIEBHHF3/Nair et al. - Massively Parallel Methods for Deep Reinforcement .pdf},
  language = {en}
}

@incollection{nassifScoreYouLift2013,
  title = {Score {{As You Lift}} ({{SAYL}}): {{A Statistical Relational Learning Approach}} to {{Uplift Modeling}}},
  shorttitle = {Score {{As You Lift}} ({{SAYL}})},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Nassif, Houssam and Kuusisto, Finn and Burnside, Elizabeth S. and Page, David and Shavlik, Jude and Santos Costa, V{\'i}tor},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Blockeel, Hendrik and Kersting, Kristian and Nijssen, Siegfried and {\v Z}elezn{\'y}, Filip},
  year = {2013},
  volume = {8190},
  pages = {595--611},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-40994-3_38},
  abstract = {We introduce Score As You Lift (SAYL), a novel Statistical Relational Learning (SRL) algorithm, and apply it to an important task in the diagnosis of breast cancer. SAYL combines SRL with the marketing concept of uplift modeling, uses the area under the uplift curve to direct clause construction and final theory evaluation, integrates rule learning and probability assignment, and conditions the addition of each new theory rule to existing ones.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/Q6NBTFV3/Nassif et al. - 2013 - Score As You Lift (SAYL) A Statistical Relational.pdf},
  isbn = {978-3-642-40993-6 978-3-642-40994-3},
  language = {en}
}

@misc{NeurIPS2019,
  title = {{{NeurIPS}} 2019},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/9WFPZKLR/ScheduleMultitrack.html},
  howpublished = {https://nips.cc/Conferences/2019/ScheduleMultitrack?event=15862}
}

@article{nystrupIdentifyingRegimeShifts,
  title = {Identifying {{Regime Shifts}} in {{Financial Time Series}} to {{Build Robust Portfolios}}},
  author = {Nystrup, Peter},
  pages = {323},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/RKNS2JGF/Nystrup - Identifying Regime Shifts in Financial Time Series.pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@article{onkenDiscretizeOptimizeVsOptimizeDiscretize2020,
  title = {Discretize-{{Optimize}} vs. {{Optimize}}-{{Discretize}} for {{Time}}-{{Series Regression}} and {{Continuous Normalizing Flows}}},
  author = {Onken, Derek and Ruthotto, Lars},
  year = {2020},
  month = may,
  abstract = {We compare the discretize-optimize (Disc-Opt) and optimize-discretize (Opt-Disc) approaches for time-series regression and continuous normalizing flows using neural ODEs. Neural ODEs, first described in Chen et al. (2018), are ordinary differential equations (ODEs) with neural network components; these models have competitively solved a variety of machine learning applications. Training a neural ODE can be phrased as an optimal control problem where the neural network weights are the controls and the hidden features are the states. Every iteration of gradient-based training involves solving an ODE forward in time and another backward in time, which can require large amounts of computation, time, and memory. Gholami et al. (2019) compared the Opt-Disc and Disc-Opt approaches for neural ODEs arising as continuous limits of residual neural networks used in image classification tasks. Their findings suggest that Disc-Opt achieves preferable performance due to the guaranteed accuracy of gradients. In this paper, we extend this comparison to neural ODEs applied to time-series regression and continuous normalizing flows (CNFs). Timeseries regression and CNFs differ from classification in that the actual ODE model is needed in the prediction and inference phase, respectively. Meaningful models must also satisfy additional requirements, e.g., the invertibility of the CNF. As the continuous model satisfies these requirements by design, Opt-Disc approaches may appear advantageous. Through our numerical experiments, we demonstrate that with careful numerical treatment, Disc-Opt methods can achieve similar performance as Opt-Disc at inference with drastically reduced training costs. Disc-Opt reduced costs in six out of seven separate problems with training time reduction ranging from 39\% to 97\%, and in one case, Disc-Opt reduced training from nine days to less than one day.},
  archivePrefix = {arXiv},
  eprint = {2005.13420},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/RVEFE4HY/Onken and Ruthotto - 2020 - Discretize-Optimize vs. Optimize-Discretize for Ti.pdf},
  journal = {arXiv:2005.13420 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{oordRepresentationLearningContrastive2019,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
  year = {2019},
  month = jan,
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  archivePrefix = {arXiv},
  eprint = {1807.03748},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/RRA5G7KM/Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf},
  journal = {arXiv:1807.03748 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{papailiasImprovedMovingAverage2011,
  title = {An {{Improved Moving Average Technical Trading Rule II}} - {{Can We Obtain Performance Improvements}} with {{Short Sales}}?},
  author = {Papailias, Fotis and Thomakos, Dimitrios D.},
  year = {2011},
  issn = {1556-5068},
  doi = {10.2139/ssrn.1958906},
  abstract = {In this paper we extend the methodology of our earlier work (Papailias and Thomakos, 2011) on a modified moving average technical trading rule by allowing short sales. We show how short sales change the trading rule which now acts as a dynamic trailing `stop-and-reverse', instead of a dynamic trailing stop as in the context of `long-only' trades. Then we empirically examine the performance of our modification in the context of a `long/short' trading approach and discuss the differentiation and implications in strategy performance. We compare both the modified version of our trading rule with the standard moving average approach and also compare the long/short approach with the long-only approach of the earlier paper.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/CLA9QGSQ/Papailias and Thomakos - 2011 - An Improved Moving Average Technical Trading Rule .pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{papamakariosNormalizingFlowsProbabilistic2019,
  title = {Normalizing {{Flows}} for {{Probabilistic Modeling}} and {{Inference}}},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  year = {2019},
  month = dec,
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1912.02762},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/QADLJLMR/Papamakarios et al. - 2019 - Normalizing Flows for Probabilistic Modeling and I.pdf},
  journal = {arXiv:1912.02762 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{papamakariosNormalizingFlowsProbabilistic2019a,
  title = {Normalizing {{Flows}} for {{Probabilistic Modeling}} and {{Inference}}},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  year = {2019},
  month = dec,
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1912.02762},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/G4G5QK4G/Papamakarios et al. - 2019 - Normalizing Flows for Probabilistic Modeling and I.pdf},
  journal = {arXiv:1912.02762 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@misc{PDFOnlinePattern,
  title = {[{{PDF}}] {{Online Pattern Recognition}} in {{Multivariate Data Streams}} Using {{Unsupervised Learning}} | {{Semantic Scholar}}},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/CHRA72B9/fbe34973016e8e41b12abb318539ba6589d071dc.html},
  howpublished = {https://www.semanticscholar.org/paper/Online-Pattern-Recognition-in-Multivariate-Data-ddevina/fbe34973016e8e41b12abb318539ba6589d071dc}
}

@article{peluchettiInfinitelyDeepNeural2020,
  title = {Infinitely Deep Neural Networks as Diffusion Processes},
  author = {Peluchetti, Stefano and Favaro, Stefano},
  year = {2020},
  month = feb,
  abstract = {When the parameters are independently and identically distributed (initialized) neural networks exhibit undesirable properties that emerge as the number of layers increases, e.g. a vanishing dependency on the input and a concentration on restrictive families of functions including constant functions. We consider parameter distributions that shrink as the number of layers increases in order to recover well-behaved stochastic processes in the limit of infinite depth. This leads to set forth a link between infinitely deep residual networks and solutions to stochastic differential equations, i.e. diffusion processes. We show that these limiting processes do not suffer from the aforementioned issues and investigate their properties.},
  archivePrefix = {arXiv},
  eprint = {1905.11065},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/UF9J2MKC/Peluchetti and Favaro - 2020 - Infinitely deep neural networks as diffusion proce.pdf},
  journal = {arXiv:1905.11065 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{poliHypersolversFastContinuousDepth2020,
  title = {Hypersolvers: {{Toward Fast Continuous}}-{{Depth Models}}},
  shorttitle = {Hypersolvers},
  author = {Poli, Michael and Massaroli, Stefano and Yamashita, Atsushi and Asama, Hajime and Park, Jinkyoo},
  year = {2020},
  month = jul,
  abstract = {The infinite\textendash depth paradigm pioneered by Neural ODEs has launched a renaissance in the search for novel dynamical system\textendash inspired deep learning primitives; however, their utilization in problems of non\textendash trivial size has often proved impossible due to poor computational scalability. This work paves the way for scalable Neural ODEs with time\textendash to\textendash prediction comparable to traditional discrete networks. We introduce hypersolvers, neural networks designed to solve ODEs with low overhead and theoretical guarantees on accuracy. The synergistic combination of hypersolvers and Neural ODEs allows for cheap inference and unlocks a new frontier for practical application of continuous\textendash depth models. Experimental evaluations on standard benchmarks, such as sampling for continuous normalizing flows, reveal consistent pareto efficiency over classical numerical methods.},
  archivePrefix = {arXiv},
  eprint = {2007.09601},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/GX57BLHC/Poli et al. - 2020 - Hypersolvers Toward Fast Continuous-Depth Models.pdf},
  journal = {arXiv:2007.09601 [cs, math, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{prasadDeepRecurrentNeural,
  title = {Deep {{Recurrent Neural Networks}} for {{Time}}- {{Series Prediction}}},
  author = {Prasad, Sharat C and Prasad, Piyush},
  pages = {19},
  abstract = {Ability of deep networks to extract high level features and of recurrent networks to perform time-series inference have been studied. In view of universality of one hidden layer network at approximating functions under weak constraints, the benefit of multiple layers is to enlarge the space of dynamical systems approximated or, given the space, reduce the number of units required for a certain error. Traditionally shallow networks with manually engineered features are used, back-propagation extent is limited to one and attempt to choose a large number of hidden units to satisfy the Markov condition is made. In case of Markov models, it has been shown that many systems need to be modeled as higher order. In the present work, we present deep recurrent networks with longer backpropagation through time extent as a solution to modeling systems that are high order and to predicting ahead. We study epileptic seizure suppression electro-stimulator. Extraction of manually engineered complex features and prediction employing them has not allowed small low-power implementations as, to avoid possibility of surgery, extraction of any features that may be required has to be included. In this solution, a recurrent neural network performs both feature extraction and prediction. We prove analytically that adding hidden layers or increasing backpropagation extent increases the rate of decrease of approximation error. A Dynamic Programming (DP) training procedure employing matrix operations is derived. DP and use of matrix operations makes the procedure efficient particularly when using data-parallel computing. The simulation studies show the geometry of the parameter space, that the network learns the temporal structure, that parameters converge while model output displays same dynamic behavior as the system and greater than .99 Average Detection Rate on all real seizure data tried.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/3XJCIGAM/Prasad and Prasad - Deep Recurrent Neural Networks for Time- Series Pr.pdf;/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/P2SQABIY/Prasad and Prasad - Deep Recurrent Neural Networks for Time- Series Pr.pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@misc{ProbabilisticProgrammingProgrammable2018,
  title = {Probabilistic Programming with Programmable Inference | {{Proceedings}} of the 39th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  year = {2018},
  month = jun,
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/Probabilistic programming with programmable inference Proceedings of the 39th.pdf;/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/8ZH3LY9H/3192366.html},
  howpublished = {https://dl.acm.org/doi/10.1145/3192366.3192409}
}

@misc{ProbableNetworksPlausible,
  title = {Probable Networks and Plausible Predictions \textemdash{} a Review of Practical {{Bayesian}} Methods for Supervised Neural Networks: {{Network}}: {{Computation}} in {{Neural Systems}}: {{Vol}} 6, {{No}} 3},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/Probable networks and plausible predictions ‚Äî a review of practical Bayesian.pdf;/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/KEZVLDKJ/0954-898X_6_3_011.html},
  howpublished = {https://www.tandfonline.com/doi/abs/10.1088/0954-898X\_6\_3\_011}
}

@article{qinDualStageAttentionBasedRecurrent2017,
  title = {A {{Dual}}-{{Stage Attention}}-{{Based Recurrent Neural Network}} for {{Time Series Prediction}}},
  author = {Qin, Yao and Song, Dongjin and Chen, Haifeng and Cheng, Wei and Jiang, Guofei and Cottrell, Garrison},
  year = {2017},
  month = aug,
  abstract = {The Nonlinear autoregressive exogenous (NARX) model, which predicts the current value of a time series based upon its previous values as well as the current and past values of multiple driving (exogenous) series, has been studied for decades. Despite the fact that various NARX models have been developed, few of them can capture the long-term temporal dependencies appropriately and select the relevant driving series to make predictions. In this paper, we propose a dual-stage attention-based recurrent neural network (DA-RNN) to address these two issues. In the first stage, we introduce an input attention mechanism to adaptively extract relevant driving series (a.k.a., input features) at each time step by referring to the previous encoder hidden state. In the second stage, we use a temporal attention mechanism to select relevant encoder hidden states across all time steps. With this dual-stage attention scheme, our model can not only make predictions effectively, but can also be easily interpreted. Thorough empirical studies based upon the SML 2010 dataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can outperform state-of-the-art methods for time series prediction.},
  archivePrefix = {arXiv},
  eprint = {1704.02971},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/ZCNPLUPM/Qin et al. - 2017 - A Dual-Stage Attention-Based Recurrent Neural Netw.pdf},
  journal = {arXiv:1704.02971 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{rackauckasUniversalDifferentialEquations2020,
  title = {Universal {{Differential Equations}} for {{Scientific Machine Learning}}},
  author = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
  year = {2020},
  month = jun,
  abstract = {In the context of science, the well-known adage ``a picture is worth a thousand words'' might well be ``a model is worth a thousand datasets.'' Scientific models, such as Newtonian physics or biological gene regulatory networks, are human-driven simplifications of complex phenomena that serve as surrogates for the countless experiments that validated the models. Recently, machine learning has been able to overcome the inaccuracies of approximate modeling by directly learning the entire set of nonlinear interactions from data. However, without any predetermined structure from the scientific basis behind the problem, machine learning approaches are flexible but data-expensive, requiring large databases of homogeneous labeled training data. A central challenge is reconciling data that is at odds with simplified models without requiring ``big data''.},
  archivePrefix = {arXiv},
  eprint = {2001.04385},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/2V8LDQ94/Rackauckas et al. - 2020 - Universal Differential Equations for Scientific Ma.pdf},
  journal = {arXiv:2001.04385 [cs, math, q-bio, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Mathematics - Dynamical Systems,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, q-bio, stat}
}

@article{raeCompressiveTransformersLongRange2019,
  title = {Compressive {{Transformers}} for {{Long}}-{{Range Sequence Modelling}}},
  author = {Rae, Jack W. and Potapenko, Anna and Jayakumar, Siddhant M. and Lillicrap, Timothy P.},
  year = {2019},
  month = nov,
  abstract = {We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new openvocabulary language modelling benchmark derived from books, PG-19.},
  archivePrefix = {arXiv},
  eprint = {1911.05507},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/IJ7DW2MH/Rae et al. - 2019 - Compressive Transformers for Long-Range Sequence M.pdf},
  journal = {arXiv:1911.05507 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{rasulMultivariateProbabilisticTime2020,
  title = {Multi-Variate {{Probabilistic Time Series Forecasting}} via {{Conditioned Normalizing Flows}}},
  author = {Rasul, Kashif and Sheikh, Abdul-Saboor and Schuster, Ingmar and Bergmann, Urs and Vollgraf, Roland},
  year = {2020},
  month = may,
  abstract = {Time series forecasting is often fundamental to scientific and engineering problems and enables decision making. With ever increasing data set sizes, a trivial solution to scale up predictions is to assume independence between interacting time series. However, modeling statistical dependencies can improve accuracy and enable analysis of interaction effects. Deep learning methods are well suited for this problem, but multi-variate models often assume a simple parametric distribution and do not scale to high dimensions. In this work we model the multi-variate temporal dynamics of time series via an autoregressive deep learning model, where the data distribution is represented by a conditioned normalizing flow. This combination retains the power of autoregressive models, such as good performance in extrapolation into the future, with the flexibility of flows as a general purpose high-dimensional distribution model, while remaining computationally tractable. We show that it improves over the state-of-the-art for standard metrics on many real-world data sets with several thousand interacting time-series.},
  archivePrefix = {arXiv},
  eprint = {2002.06103},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/LI5RS8ZX/Rasul et al. - 2020 - Multi-variate Probabilistic Time Series Forecastin.pdf},
  journal = {arXiv:2002.06103 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@misc{RecommenderSystems101,
  title = {Recommender {{Systems}} 101 - a Step by Step Practical Example in {{R}}},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/TK5YF9KE/recommender-systems-101-practical-example-in-r.html},
  howpublished = {http://bigdata-doctor.com/recommender-systems-101-practical-example-in-r/}
}

@misc{RegTechCompaniesSolve,
  title = {{{RegTech}} Companies to Solve Compliance and Regulatory Issues | {{Deloitte Luxembourg}}},
  howpublished = {https://www2.deloitte.com/lu/en/pages/technology/articles/regtech-companies-compliance.html}
}

@article{rezendeVariationalInferenceNormalizing2016,
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  year = {2016},
  month = jun,
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  archivePrefix = {arXiv},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/9PTUZEX4/Rezende and Mohamed - 2016 - Variational Inference with Normalizing Flows.pdf},
  journal = {arXiv:1505.05770 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  language = {en},
  primaryClass = {cs, stat}
}

@inbook{riguzziPITASystemLogicalProbabilistic2014,
  title = {The {{PITA System}} for {{Logical}}-{{Probabilistic Inference}}},
  booktitle = {Latest {{Advances}} in {{Inductive Logic Programming}}},
  author = {Riguzzi, Fabrizio and Swift, Terrance},
  year = {2014},
  month = dec,
  pages = {79--86},
  publisher = {{IMPERIAL COLLEGE PRESS}},
  doi = {10.1142/9781783265091_0010},
  collaborator = {Muggleton, Stephen H and Watanabe, Hiroaki},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/74XBXX8G/Riguzzi and Swift - 2014 - The PITA System for Logical-Probabilistic Inferenc.pdf},
  isbn = {978-1-78326-508-4 978-1-78326-509-1},
  language = {en}
}

@article{riguzziPITASystemTabling2011,
  title = {The {{PITA System}}: {{Tabling}} and {{Answer Subsumption}} for {{Reasoning}} under {{Uncertainty}}},
  shorttitle = {The {{PITA System}}},
  author = {Riguzzi, Fabrizio and Swift, Terrance},
  year = {2011},
  month = jul,
  volume = {11},
  pages = {433--449},
  issn = {1471-0684, 1475-3081},
  doi = {10.1017/S147106841100010X},
  abstract = {Many real world domains require the representation of a measure of uncertainty. The most common such representation is probability, and the combination of probability with logic programs has given rise to the field of Probabilistic Logic Programming (PLP), leading to languages such as the Independent Choice Logic, Logic Programs with Annotated Disjunctions (LPADs), Problog, PRISM and others. These languages share a similar distribution semantics, and methods have been devised to translate programs between these languages. The complexity of computing the probability of queries to these general PLP programs is very high due to the need to combine the probabilities of explanations that may not be exclusive. As one alternative, the PRISM system reduces the complexity of query answering by restricting the form of programs it can evaluate. As an entirely different alternative, Possibilistic Logic Programs adopt a simpler metric of uncertainty than probability.},
  archivePrefix = {arXiv},
  eprint = {1107.4747},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/394US4S3/Riguzzi and Swift - 2011 - The PITA System Tabling and Answer Subsumption fo.pdf},
  journal = {Theory and Practice of Logic Programming},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  language = {en},
  number = {4-5}
}

@article{royEfficientContentBasedSparse2020,
  title = {Efficient {{Content}}-{{Based Sparse Attention}} with {{Routing Transformers}}},
  author = {Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  year = {2020},
  month = mar,
  abstract = {Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers.},
  archivePrefix = {arXiv},
  eprint = {2003.05997},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/UPJCJDYI/Roy et al. - 2020 - Efficient Content-Based Sparse Attention with Rout.pdf},
  journal = {arXiv:2003.05997 [cs, eess, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, eess, stat}
}

@article{saadBayesianSynthesisProbabilistic2019,
  title = {Bayesian {{Synthesis}} of {{Probabilistic Programs}} for {{Automatic Data Modeling}}},
  author = {Saad, Feras A. and {Cusumano-Towner}, Marco F. and Schaechtle, Ulrich and Rinard, Martin C. and Mansinghka, Vikash K.},
  year = {2019},
  month = jan,
  volume = {3},
  pages = {1--32},
  issn = {2475-1421, 2475-1421},
  doi = {10/gfttv2},
  abstract = {We present new techniques for automatically constructing probabilistic programs for data analysis, interpretation, and prediction. These techniques work with probabilistic domain-specific data modeling languages that capture key properties of a broad class of data generating processes, using Bayesian inference to synthesize probabilistic programs in these modeling languages given observed data. We provide a precise formulation of Bayesian synthesis for automatic data modeling that identifies sufficient conditions for the resulting synthesis procedure to be sound. We also derive a general class of synthesis algorithms for domain-specific languages specified by probabilistic context-free grammars and establish the soundness of our approach for these languages. We apply the techniques to automatically synthesize probabilistic programs for time series data and multivariate tabular data. We show how to analyze the structure of the synthesized programs to compute, for key qualitative properties of interest, the probability that the underlying data generating process exhibits each of these properties. Second, we translate probabilistic programs in the domain-specific language into probabilistic programs in Venture, a general-purpose probabilistic programming system. The translated Venture programs are then executed to obtain predictions of new time series data and new multivariate data records. Experimental results show that our techniques can accurately infer qualitative structure in multiple real-world data sets and outperform standard data analysis methods in forecasting and predicting new data.},
  archivePrefix = {arXiv},
  eprint = {1907.06249},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/9GM6D3CA/Saad et al. - 2019 - Bayesian Synthesis of Probabilistic Programs for A.pdf},
  journal = {Proc. ACM Program. Lang.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages,Statistics - Computation},
  language = {en},
  number = {POPL}
}

@article{saadTemporallyReweightedChineseRestaurant2018,
  title = {Temporally-{{Reweighted Chinese Restaurant Process Mixtures}} for {{Clustering}}, {{Imputing}}, and {{Forecasting Multivariate Time Series}}},
  author = {Saad, Feras A. and Mansinghka, Vikash K.},
  year = {2018},
  month = apr,
  abstract = {This article proposes a Bayesian nonparametric method for forecasting, imputation, and clustering in sparsely observed, multivariate time series data. The method is appropriate for jointly modeling hundreds of time series with widely varying, non-stationary dynamics. Given a collection of N time series, the Bayesian model first partitions them into independent clusters using a Chinese restaurant process prior. Within a cluster, all time series are modeled jointly using a novel ``temporally-reweighted'' extension of the Chinese restaurant process mixture. Markov chain Monte Carlo techniques are used to obtain samples from the posterior distribution, which are then used to form predictive inferences. We apply the technique to challenging forecasting and imputation tasks using seasonal flu data from the US Center for Disease Control and Prevention, demonstrating superior forecasting accuracy and competitive imputation accuracy as compared to multiple widely used baselines. We further show that the model discovers interpretable clusters in datasets with hundreds of time series, using macroeconomic data from the Gapminder Foundation.},
  archivePrefix = {arXiv},
  eprint = {1710.06900},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/46UMS6JX/Saad and Mansinghka - 2018 - Temporally-Reweighted Chinese Restaurant Process M.pdf},
  journal = {arXiv:1710.06900 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  language = {en},
  primaryClass = {cs, stat}
}

@article{salinasHighDimensionalMultivariateForecasting2019,
  title = {High-{{Dimensional Multivariate Forecasting}} with {{Low}}-{{Rank Gaussian Copula Processes}}},
  author = {Salinas, David and {Bohlke-Schneider}, Michael and Callot, Laurent and Medico, Roberto and Gasthaus, Jan},
  year = {2019},
  month = oct,
  abstract = {Predicting the dependencies between observations from multiple time series is critical for applications such as anomaly detection, financial risk management, causal analysis, or demand forecasting. However, the computational and numerical difficulties of estimating time-varying and high-dimensional covariance matrices often limits existing methods to handling at most a few hundred dimensions or requires making strong assumptions on the dependence between series. We propose to combine an RNN-based time series model with a Gaussian copula process output model with a low-rank covariance structure to reduce the computational complexity and handle non-Gaussian marginal distributions. This permits to drastically reduce the number of parameters and consequently allows the modeling of time-varying correlations of thousands of time series. We show on several realworld datasets that our method provides significant accuracy improvements over state-of-the-art baselines and perform an ablation study analyzing the contributions of the different components of our model.},
  archivePrefix = {arXiv},
  eprint = {1910.03002},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/BTCCSQWP/Salinas et al. - 2019 - High-Dimensional Multivariate Forecasting with Low.pdf},
  journal = {arXiv:1910.03002 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{santiagoIterativeDesensitisationImage2007,
  title = {Iterative {{Desensitisation}} of {{Image Restoration Filters}} under {{Wrong PSF}} and {{Noise Estimates}}},
  author = {Santiago, Miguel A. and Cisneros, Guillermo and Bernu{\'e}s, Emiliano},
  year = {2007},
  month = dec,
  volume = {2007},
  issn = {1687-6180},
  doi = {10.1155/2007/72658},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/AIIXG6N8/Santiago et al. - 2007 - Iterative Desensitisation of Image Restoration Fil.pdf},
  journal = {EURASIP Journal on Advances in Signal Processing},
  language = {en},
  number = {1}
}

@article{santosOptimalPortfoliosMinimum,
  title = {Optimal Portfolios with Minimum Capital Requirements},
  author = {Santos, Andre A P and Nogales, Francisco J and Ruiz, Esther},
  pages = {37},
  abstract = {We propose a novel approach to active risk management based on the Basel II regulations to obtain optimal portfolios with minimum capital requirements. In order to avoid regulatory penalties due to an excessive number of value-at-risk (VaR) violations, capital requirements are minimized subject to a given number of violations over the previous trading year. An empirical application for three portfolios involving different types of assets demonstrates that the proposed approach delivers an improved balance between capital requirement levels and the number of VaR exceedances. Furthermore, the risk-adjusted performance of the proposed approach is superior to that of minimum-VaR portfolios.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/73LQ58JQ/Santos et al. - Optimal portfolios with minimum capital requiremen.pdf},
  keywords = {‚ùìMultiple DOI},
  language = {en}
}

@article{sarraRenormalizedMutualInformation2020,
  title = {Renormalized {{Mutual Information}} for {{Artificial Scientific Discovery}}},
  author = {Sarra, Leopoldo and Aiello, Andrea and Marquardt, Florian},
  year = {2020},
  month = jun,
  abstract = {We derive a well-defined renormalized version of mutual information that allows to estimate the dependence between continuous random variables in the important case when one is deterministically dependent on the other. This is the situation relevant for feature extraction, where the goal is to produce a low-dimensional effective description of a high-dimensional system. Our approach enables the discovery of collective variables in physical systems, thus adding to the toolbox of artificial scientific discovery, while also aiding the analysis of information flow in artificial neural networks.},
  archivePrefix = {arXiv},
  eprint = {2005.01912},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/PZNHT8ZT/Sarra et al. - 2020 - Renormalized Mutual Information for Artificial Sci.pdf},
  journal = {arXiv:2005.01912 [physics]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Physics - Data Analysis; Statistics and Probability},
  language = {en},
  primaryClass = {physics}
}

@article{sarraRenormalizedMutualInformation2020a,
  title = {Renormalized {{Mutual Information}} for {{Artificial Scientific Discovery}}},
  author = {Sarra, Leopoldo and Aiello, Andrea and Marquardt, Florian},
  year = {2020},
  month = jun,
  abstract = {We derive a well-defined renormalized version of mutual information that allows to estimate the dependence between continuous random variables in the important case when one is deterministically dependent on the other. This is the situation relevant for feature extraction, where the goal is to produce a low-dimensional effective description of a high-dimensional system. Our approach enables the discovery of collective variables in physical systems, thus adding to the toolbox of artificial scientific discovery, while also aiding the analysis of information flow in artificial neural networks.},
  archivePrefix = {arXiv},
  eprint = {2005.01912},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/ARG3HNJ3/Sarra et al. - 2020 - Renormalized Mutual Information for Artificial Sci.pdf},
  journal = {arXiv:2005.01912 [physics]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Physics - Data Analysis; Statistics and Probability},
  language = {en},
  primaryClass = {physics}
}

@article{sarraRenormalizedMutualInformation2020b,
  title = {Renormalized {{Mutual Information}} for {{Artificial Scientific Discovery}}},
  author = {Sarra, Leopoldo and Aiello, Andrea and Marquardt, Florian},
  year = {2020},
  month = jun,
  abstract = {We derive a well-defined renormalized version of mutual information that allows to estimate the dependence between continuous random variables in the important case when one is deterministically dependent on the other. This is the situation relevant for feature extraction, where the goal is to produce a low-dimensional effective description of a high-dimensional system. Our approach enables the discovery of collective variables in physical systems, thus adding to the toolbox of artificial scientific discovery, while also aiding the analysis of information flow in artificial neural networks.},
  archivePrefix = {arXiv},
  eprint = {2005.01912},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/BQL38IGK/Sarra et al. - 2020 - Renormalized Mutual Information for Artificial Sci.pdf},
  journal = {arXiv:2005.01912 [physics]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Physics - Data Analysis; Statistics and Probability},
  language = {en},
  primaryClass = {physics}
}

@article{schaechtleProbabilisticProgrammingGaussian2016,
  title = {Probabilistic {{Programming}} with {{Gaussian Process Memoization}}},
  author = {Schaechtle, Ulrich and Zinberg, Ben and Radul, Alexey and Stathis, Kostas and Mansinghka, Vikash K},
  year = {2016},
  month = jul,
  pages = {39},
  abstract = {Gaussian Processes (GPs) are widely used tools in statistics, machine learning, robotics, computer vision, and scientific computation. However, despite their popularity, they can be difficult to apply; all but the simplest classification or regression applications require specification and inference over complex covariance functions that do not admit simple analytical posteriors. Probabilistic programming shows potential for reducing the latter barrier, if the computational surface of a GP can be suitably packaged for cooperating with available generic inference tactics. This paper shows how to embed Gaussian processes in any higher-order probabilistic programming language, using an idiom based on memoization, and demonstrates its utility by implementing and extending classic and state-of-the-art GP applications. The interface to Gaussian processes, called gpmem, takes an arbitrary realvalued computational process as input and returns a statistical emulator that automatically improves as the original process is invoked and its input-output behavior is recorded. The flexibility of gpmem is illustrated via three applications: (i) robust GP regression with hierarchical hyper-parameter learning, (ii) discovering symbolic expressions from time-series data by fully Bayesian structure learning over kernels generated by a stochastic grammar, and (iii) a bandit formulation of Bayesian optimization with automatic inference and action selection. All applications share a single 100-line Python library and require fewer than 20 lines of probabilistic code each.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/EXHRDAUS/Schaechtle et al. - Probabilistic Programming with Gaussian Process Me.pdf},
  keywords = {‚õî No DOI found},
  language = {en}
}

@article{schaechtleTimeSeriesStructure2017,
  title = {Time {{Series Structure Discovery}} via {{Probabilistic Program Synthesis}}},
  author = {Schaechtle, Ulrich and Saad, Feras and Radul, Alexey and Mansinghka, Vikash},
  year = {2017},
  month = may,
  abstract = {There is a widespread need for techniques that can discover structure from time series data. Recently introduced techniques such as Automatic Bayesian Covariance Discovery (ABCD) provide a way to find structure within a single time series by searching through a space of covariance kernels that is generated using a simple grammar. While ABCD can identify a broad class of temporal patterns, it is difficult to extend and can be brittle in practice. This paper shows how to extend ABCD by formulating it in terms of probabilistic program synthesis. The key technical ideas are to (i) represent models using abstract syntax trees for a domain-specific probabilistic language, and (ii) represent the time series model prior, likelihood, and search strategy using probabilistic programs in a sufficiently expressive language. The final probabilistic program is written in under 70 lines of probabilistic code in Venture. The paper demonstrates an application to time series clustering that involves a non-parametric extension to ABCD, experiments for interpolation and extrapolation on real-world econometric data, and improvements in accuracy over both non-parametric and standard regression baselines.},
  archivePrefix = {arXiv},
  eprint = {1611.07051},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/KUNSUN32/Schaechtle et al. - 2017 - Time Series Structure Discovery via Probabilistic .pdf},
  journal = {arXiv:1611.07051 [stat]},
  keywords = {‚õî No DOI found,Statistics - Machine Learning},
  language = {en},
  primaryClass = {stat}
}

@article{schererNewLookMinimum2010,
  title = {A {{New Look}} at {{Minimum Variance Investing}}},
  author = {Scherer, Bernd},
  year = {2010},
  issn = {1556-5068},
  doi = {10/fzzv6s},
  abstract = {Disappointed with the performance of market weighted benchmark portfolios yet skeptical about the merits of active portfolio management, investors in recent years turned to alternative index definitions. Minimum variance investing is one of these popular rule driven, i.e. new passive concepts. I show in this paper theoretically and empirically that the portfolio construction process behind minimum variance investing implicitly picks up risk-based pricing anomalies. In other words the minimum variance tends to hold low beta and low residual risk stocks. Long/short portfolios based on these characteristics have been associated in the empirical literature with risk-adjusted outperformance (alpha). This paper shows that 83\% of the variation of the minimum variance portfolio excess returns (relative to a capitalization weighted alternative) can be attributed to the FAMA/FRENCH factors as well as to the returns on two characteristic anomaly portfolios. All regression coefficients (factor exposures) are highly significant, stable over the estimation period and correspond remarkably well with our economic intuition.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/4LTEFI87/Scherer - 2010 - A New Look at Minimum Variance Investing.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{schmidt-thiemeArtificialIntelligenceInductive2007,
  title = {Artificial {{Intelligence}} 8. {{Inductive Logic Programming}}},
  author = {{Schmidt-Thieme}, Lars},
  year = {2007},
  pages = {8},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/WVHS6VHE/Schmidt-Thieme - 2007 - ArtiÔ¨Åcial Intelligence 8. Inductive Logic Programm.pdf},
  journal = {. Inductive Logic Programming},
  language = {en}
}

@article{schmidtNormalizingFlowsNovelty2019,
  title = {Normalizing Flows for Novelty Detection in Industrial Time Series Data},
  author = {Schmidt, Maximilian and Simic, Marko},
  year = {2019},
  month = jun,
  abstract = {Flow-based deep generative models learn data distributions by transforming a simple base distribution into a complex distribution via a set of invertible transformations. Due to the invertibility, such models can score unseen data samples by computing their exact likelihood under the learned distribution. This makes flowbased models a perfect tool for novelty detection, an anomaly detection technique where unseen data samples are classified as normal or abnormal by scoring them against a learned model of normal data. We show that normalizing flows can be used as novelty detectors in time series. Two flow-based models, Masked Autoregressive Flows and Free-form Jacobian of Reversible Dynamics restricted by autoregressive MADE networks, are tested on synthetic data and motor current data from an industrial machine and achieve good results, outperforming a conventional novelty detection method, the Local Outlier Factor.},
  archivePrefix = {arXiv},
  eprint = {1906.06904},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/D3YW2TNK/Schmidt and Simic - 2019 - Normalizing flows for novelty detection in industr.pdf},
  journal = {arXiv:1906.06904 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{schrittwieserMasteringAtariGo2020,
  title = {Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  year = {2020},
  month = feb,
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  archivePrefix = {arXiv},
  eprint = {1911.08265},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/73UGSFWR/Schrittwieser et al. - 2020 - Mastering Atari, Go, Chess and Shogi by Planning w.pdf},
  journal = {arXiv:1911.08265 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{seedatCalibratedScalableUncertainty2019,
  title = {Towards Calibrated and Scalable Uncertainty Representations for Neural Networks},
  author = {Seedat, Nabeel and Kanan, Christopher},
  year = {2019},
  month = dec,
  abstract = {For many applications it is critical to know the uncertainty of a neural network's predictions. While a variety of neural network parameter estimation methods have been proposed for uncertainty estimation, they have not been rigorously compared across uncertainty measures. We assess four of these parameter estimation methods to calibrate uncertainty estimation using four different uncertainty measures: entropy, mutual information, aleatoric uncertainty and epistemic uncertainty. We evaluate the calibration of these parameter estimation methods using expected calibration error. Additionally, we propose a novel method of neural network parameter estimation called RECAST, which combines cosine annealing with warm restarts with Stochastic Gradient Langevin Dynamics, capturing more diverse parameter distributions. When benchmarked against mutilated image data, we show that RECAST is well-calibrated and when combined with predictive entropy and epistemic uncertainty it offers the best calibrated measure of uncertainty when compared to recent methods.},
  archivePrefix = {arXiv},
  eprint = {1911.00104},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/DUZ8KRFS/Seedat and Kanan - 2019 - Towards calibrated and scalable uncertainty repres.pdf},
  journal = {arXiv:1911.00104 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{sezerFinancialTimeSeries2019,
  title = {Financial {{Time Series Forecasting}} with {{Deep Learning}} : {{A Systematic Literature Review}}: 2005-2019},
  shorttitle = {Financial {{Time Series Forecasting}} with {{Deep Learning}}},
  author = {Sezer, Omer Berat and Gudelek, Mehmet Ugur and Ozbayoglu, Ahmet Murat},
  year = {2019},
  month = nov,
  abstract = {Financial time series forecasting is, without a doubt, the top choice of computational intelligence for finance researchers from both academia and financial industry due to its broad implementation areas and substantial impact. Machine Learning (ML) researchers came up with various models and a vast number of studies have been published accordingly. As such, a significant amount of surveys exist covering ML for financial time series forecasting studies. Lately, Deep Learning (DL) models started appearing within the field, with results that significantly outperform traditional ML counterparts. Even though there is a growing interest in developing models for financial time series forecasting research, there is a lack of review papers that were solely focused on DL for finance. Hence, our motivation in this paper is to provide a comprehensive literature review on DL studies for financial time series forecasting implementations. We not only categorized the studies according to their intended forecasting implementation areas, such as index, forex, commodity forecasting, but also grouped them based on their DL model choices, such as Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), Long-Short Term Memory (LSTM). We also tried to envision the future for the field by highlighting the possible setbacks and opportunities, so the interested researchers can benefit.},
  archivePrefix = {arXiv},
  eprint = {1911.13288},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/52XSW76B/Sezer et al. - 2019 - Financial Time Series Forecasting with Deep Learni.pdf},
  journal = {arXiv:1911.13288 [cs, q-fin, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,I.1.2,Quantitative Finance - Computational Finance,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, q-fin, stat}
}

@article{shahEfficientLanguageModeling,
  title = {Efficient {{Language Modeling}} of {{Long}}-{{Term Dependencies}}},
  author = {Shah, Manan},
  pages = {8},
  abstract = {The transformer architecture [1] has received significant research attention due to its ability to effectively model word-level and global contextual dependencies exclusively using self-attention, thereby enabling language models such as BERT [2] to achieve state-of-the-art performance. However, the quadratic cost of attention and linear memory cost per layer have severely limited the transformer to operate solely on short sequences, a problem partially addressed by the reversible layers and LSH attention proposed in the reformer [3]. In this work, we study and improve properties of the reformer as a language model, introducing k-means clustering for attention and connection tying in reversible layers to improve reformer complexity and representational power. We evaluate our model on masked language modeling as well as selected GLUE benchmarks, and we find that our modifications significantly improve training times and model capacity. The methods presented in our work are generalizable to other attention-based models and have the potential to drastically improve the efficacy of long-term language dependency modeling.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/4VZLASZA/Shah - EfÔ¨ÅÔ¨Åcent Language Modeling of Long-Term Dependenci.pdf},
  keywords = {‚õî No DOI found},
  language = {en}
}

@article{shawSelfAttentionRelativePosition2018,
  title = {Self-{{Attention}} with {{Relative Position Representations}}},
  author = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  year = {2018},
  month = apr,
  abstract = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graphlabeled inputs.},
  archivePrefix = {arXiv},
  eprint = {1803.02155},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/GILY9T6P/Shaw et al. - 2018 - Self-Attention with Relative Position Representati.pdf},
  journal = {arXiv:1803.02155 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Computation and Language},
  language = {en},
  primaryClass = {cs}
}

@article{shibataFunctionsThatEmerge2017,
  title = {Functions That {{Emerge}} through {{End}}-to-{{End Reinforcement Learning}} - {{The Direction}} for {{Artificial General Intelligence}} -},
  author = {Shibata, Katsunari},
  year = {2017},
  month = may,
  abstract = {Recently, triggered by the impressive results in TV-games or game of Go by Google DeepMind, end-to-end reinforcement learning (RL) is collecting attentions. Although little is known, the author's group has propounded this framework for around 20 years and already has shown a variety of functions that emerge in a neural network (NN) through RL. In this paper, they are introduced again at this timing.},
  archivePrefix = {arXiv},
  eprint = {1703.02239},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/74YUJ3I2/Shibata - 2017 - Functions that Emerge through End-to-End Reinforce.pdf;/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/MUMUT549/Appendix.pdf},
  journal = {arXiv:1703.02239 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@article{shihTemporalPatternAttention2019,
  title = {Temporal {{Pattern Attention}} for {{Multivariate Time Series Forecasting}}},
  author = {Shih, Shun-Yao and Sun, Fan-Keng and Lee, Hung-yi},
  year = {2019},
  month = sep,
  abstract = {Forecasting of multivariate time series data, for instance the prediction of electricity consumption, solar power production, and polyphonic piano pieces, has numerous valuable applications. However, complex and non-linear interdependencies between time steps and series complicate this task. To obtain accurate prediction, it is crucial to model long-term dependency in time series data, which can be achieved by recurrent neural networks (RNNs) with an attention mechanism. The typical attention mechanism reviews the information at each previous time step and selects relevant information to help generate the outputs; however, it fails to capture temporal patterns across multiple time steps. In this paper, we propose using a set of filters to extract time-invariant temporal patterns, similar to transforming time series data into its ``frequency domain''. Then we propose a novel attention mechanism to select relevant time series, and use its frequency domain information for multivariate forecasting. We apply the proposed model on several real-world tasks and achieve state-of-the-art performance in almost all of cases. Our source code is available at https://github.com/gantheory/TPA-LSTM.},
  archivePrefix = {arXiv},
  eprint = {1809.04206},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/VAVGQ2MI/Shih et al. - 2019 - Temporal Pattern Attention for Multivariate Time S.pdf},
  journal = {arXiv:1809.04206 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{smithComputerBridgeBig,
  title = {Computer {{Bridge}}\textemdash{{A Big Win}} for {{AI Planning}}},
  author = {Smith, Stephen J J and Nau, Dana and Throop, Tom},
  pages = {14},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/L757VZX3/Smith et al. - Computer Bridge‚ÄîA Big Win for AI Planning.pdf},
  language = {en}
}

@article{songUnderstandingLimitationsVariational2020,
  title = {Understanding the {{Limitations}} of {{Variational Mutual Information Estimators}}},
  author = {Song, Jiaming and Ermon, Stefano},
  year = {2020},
  month = mar,
  abstract = {Variational approaches based on neural networks are showing promise for estimating mutual information (MI) between high dimensional variables. However, they can be difficult to use in practice due to poorly understood bias/variance tradeoffs. We theoretically show that, under some conditions, estimators such as MINE exhibit variance that could grow exponentially with the true amount of underlying MI. We also empirically demonstrate that existing estimators fail to satisfy basic self-consistency properties of MI, such as data processing and additivity under independence. Based on a unified perspective of variational approaches, we develop a new estimator that focuses on variance reduction. Empirical results on standard benchmark tasks demonstrate that our proposed estimator exhibits improved biasvariance trade-offs on standard benchmark tasks.},
  archivePrefix = {arXiv},
  eprint = {1910.06222},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/AZTMNR3F/Song and Ermon - 2020 - Understanding the Limitations of Variational Mutua.pdf},
  journal = {arXiv:1910.06222 [cs, math, stat]},
  keywords = {‚õî No DOI found,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, stat}
}

@incollection{steinrueckenAutomaticStatistician2019,
  title = {The {{Automatic Statistician}}},
  booktitle = {Automated {{Machine Learning}}},
  author = {Steinruecken, Christian and Smith, Emma and Janz, David and Lloyd, James and Ghahramani, Zoubin},
  editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year = {2019},
  pages = {161--173},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-05318-5_9},
  abstract = {The Automatic Statistician project aims to automate data science, producing predictions and human-readable reports from raw datasets with minimal human intervention. Alongside basic graphs and statistics, the generated reports contain a curation of high-level insights about the dataset, that are obtained from (1) an automated construction of models for the dataset, (2) a comparison of these models, and (3) a software component that turns these results into natural language descriptions. This chapter describes the common architecture of such Automatic Statistician systems, and discusses some of the design decisions and technical challenges.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/32LC5Q8W/Steinruecken et al. - 2019 - The Automatic Statistician.pdf},
  isbn = {978-3-030-05317-8 978-3-030-05318-5},
  language = {en}
}

@article{suDeconvolutionDefocusedImage2010,
  title = {Deconvolution of {{Defocused Image}} with {{Multivariate Local Polynomial Regression}} and {{Iterative Wiener Filtering}} in {{DWT Domain}}},
  author = {Su, Liyun and Li, Fenglan},
  year = {2010},
  volume = {2010},
  pages = {1--14},
  issn = {1024-123X, 1563-5147},
  doi = {10.1155/2010/605241},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/QC7FRVKF/Su and Li - 2010 - Deconvolution of Defocused Image with Multivariate.pdf},
  journal = {Mathematical Problems in Engineering},
  language = {en}
}

@article{sukhbaatarAdaptiveAttentionSpan2019,
  title = {Adaptive {{Attention Span}} in {{Transformers}}},
  author = {Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
  year = {2019},
  month = aug,
  abstract = {We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.},
  archivePrefix = {arXiv},
  eprint = {1905.07799},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/XGWHLB33/Sukhbaatar et al. - 2019 - Adaptive Attention Span in Transformers.pdf},
  journal = {arXiv:1905.07799 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{sunOptimalRebalancingStrategy,
  title = {Optimal {{Rebalancing Strategy}} for {{Institutional Portfolios}}},
  author = {Sun, Walter and Fan, Ayres and Chen, Li-Wei and Schouwenaars, Tom and Albota, Marius A and Freyfogle, Ed and Grover, Josh},
  pages = {24},
  abstract = {Institutional fund managers generally rebalance using ad hoc methods such as calendar basis or tolerance band triggers. We propose a different framework that quantifies the cost of a rebalancing strategy in terms of risk-adjusted returns net of transaction costs. We then develop an optimal rebalancing strategy that actively seeks to minimize that cost. We use certainty equivalents and the transaction costs associated with a policy to define a cost-to-go function, and we minimize this expected cost-to-go using dynamic programming. We apply Monte Carlo simulations to demonstrate that our method outperforms traditional rebalancing strategies like monthly, quarterly, annual, and 5\% tolerance rebalancing. We also show the robustness of our method to model error by performing sensitivity analyses.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/E8HGAUP5/Sun et al. - Optimal Rebalancing Strategy for Institutional Por.pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@article{sunOptimalRebalancingStrategy2004,
  title = {Optimal {{Rebalancing Strategy Using Dynamic Programming}} for {{Institutional Portfolios}}},
  author = {Sun, Walter and Fan, Ayres C. and Chen, Li-Wei and Schouwenaars, Tom and Albota, Marius A.},
  year = {2004},
  issn = {1556-5068},
  doi = {10/fznc7d},
  abstract = {Institutional fund managers generally rebalance using ad hoc methods such as calendar basis or tolerance band triggers. We propose a different framework that quantifies the cost of a rebalancing strategy in terms of risk-adjusted returns net of transaction costs. We then develop an optimal rebalancing strategy that actively seeks to minimize that cost. We use certainty equivalents and the transaction costs associated with a policy to define a cost-to-go function, and we minimize this expected cost-to-go using dynamic programming. We apply Monte Carlo simulations to demonstrate that our method outperforms traditional rebalancing strategies like monthly, quarterly, annual, and 5\% tolerance rebalancing. We also show the robustness of our method to model error by performing sensitivity analyses.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/3IV57T9Q/Sun et al. - 2004 - Optimal Rebalancing Strategy Using Dynamic Program.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@misc{SurveyLongTermContext2020,
  title = {A {{Survey}} of {{Long}}-{{Term Context}} in {{Transformers}}},
  year = {2020},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/A8BUZMHT/a-survey-of-methods-for-incorporating-long-term-context.html},
  howpublished = {https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/}
}

@article{tanEfficientNetRethinkingModel2019,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  year = {2019},
  month = nov,
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.},
  archivePrefix = {arXiv},
  eprint = {1905.11946},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/ZH42VHB7/Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf},
  journal = {arXiv:1905.11946 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@techreport{taylorForecastingScale2017,
  title = {Forecasting at Scale},
  author = {Taylor, Sean J and Letham, Benjamin},
  year = {2017},
  month = sep,
  institution = {{PeerJ Preprints}},
  doi = {10.7287/peerj.preprints.3190v2},
  abstract = {Forecasting is a common data science task that helps organizations with capacity planning, goal setting, and anomaly detection. Despite its importance, there are serious challenges associated with producing reliable and high quality forecasts \textendash especially when there are a variety of time series and analysts with expertise in time series modeling are relatively rare. To address these challenges, we describe a practical approach to forecasting ``at scale'' that combines configurable models with analyst-in-the-loop performance analysis. We propose a modular regression model with interpretable parameters that can be intuitively adjusted by analysts with domain knowledge about the time series. We describe performance analyses to compare and evaluate forecasting procedures, and automatically flag forecasts for manual review and adjustment. Tools that help analysts to use their expertise most effectively enable reliable, practical forecasting of business time series.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/HG2R7JVK/Taylor and Letham - 2017 - Forecasting at scale.pdf},
  language = {en},
  type = {Preprint}
}

@inproceedings{tingBayesianRegressionInput2006,
  title = {Bayesian Regression with Input Noise for High Dimensional Data},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning  - {{ICML}} '06},
  author = {Ting, Jo-Anne and D'Souza, Aaron and Schaal, Stefan},
  year = {2006},
  pages = {937--944},
  publisher = {{ACM Press}},
  address = {{Pittsburgh, Pennsylvania}},
  doi = {10.1145/1143844.1143962},
  abstract = {This paper examines high dimensional regression with noise-contaminated input and output data. Goals of such learning problems include optimal prediction with noiseless query points and optimal system identification. As a first step, we focus on linear regression methods, since these can be easily cast into nonlinear learning problems with locally weighted learning approaches. Standard linear regression algorithms generate biased regression estimates if input noise is present and suffer numerically when the data contains redundancy and irrelevancy. Inspired by Factor Analysis Regression, we develop a variational Bayesian algorithm that is robust to ill-conditioned data, automatically detects relevant features, and identifies input and output noise \textendash{} all in a computationally efficient way. We demonstrate the effectiveness of our techniques on synthetic data and on a system identification task for a rigid body dynamics model of a robotic vision head. Our algorithm performs 10 to 70\% better than previously suggested methods.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/WTD997AS/Ting et al. - 2006 - Bayesian regression with input noise for high dime.pdf},
  isbn = {978-1-59593-383-6},
  language = {en}
}

@article{tranRecurrentMemoryNetworks2016,
  title = {Recurrent {{Memory Networks}} for {{Language Modeling}}},
  author = {Tran, Ke and Bisazza, Arianna and Monz, Christof},
  year = {2016},
  month = apr,
  abstract = {Recurrent Neural Networks (RNN) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge. In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. We demonstrate the power of RMN on language modeling and sentence completion tasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset. Additionally we perform in-depth analysis of various linguistic dimensions that RMN captures. On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2\% accuracy, surpassing the previous state-of-the-art by a large margin.},
  archivePrefix = {arXiv},
  eprint = {1601.01272},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/8GNCKT98/Tran et al. - 2016 - Recurrent Memory Networks for Language Modeling.pdf},
  journal = {arXiv:1601.01272 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Computation and Language},
  language = {en},
  primaryClass = {cs}
}

@article{tranSimpleDistributedAccelerated2018,
  title = {Simple, {{Distributed}}, and {{Accelerated Probabilistic Programming}}},
  author = {Tran, Dustin and Hoffman, Matthew and Moore, Dave and Suter, Christopher and Vasudevan, Srinivas and Radul, Alexey and Johnson, Matthew and Saurous, Rif A.},
  year = {2018},
  month = nov,
  abstract = {We describe a simple, low-level approach for embedding probabilistic programming in a deep learning ecosystem. In particular, we distill probabilistic programming down to a single abstraction---the random variable. Our lightweight implementation in TensorFlow enables numerous applications: a model-parallel variational auto-encoder (VAE) with 2nd-generation tensor processing units (TPUv2s); a data-parallel autoregressive model (Image Transformer) with TPUv2s; and multi-GPU No-U-Turn Sampler (NUTS). For both a state-of-the-art VAE on 64x64 ImageNet and Image Transformer on 256x256 CelebA-HQ, our approach achieves an optimal linear speedup from 1 to 256 TPUv2 chips. With NUTS, we see a 100x speedup on GPUs over Stan and 37x over PyMC3.},
  archivePrefix = {arXiv},
  eprint = {1811.02091},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/C68ES2R4/Tran et al. - 2018 - Simple, Distributed, and Accelerated Probabilistic.pdf},
  journal = {arXiv:1811.02091 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Computer Science - Programming Languages,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{tzenNeuralStochasticDifferential2019,
  title = {Neural {{Stochastic Differential Equations}}: {{Deep Latent Gaussian Models}} in the {{Diffusion Limit}}},
  shorttitle = {Neural {{Stochastic Differential Equations}}},
  author = {Tzen, Belinda and Raginsky, Maxim},
  year = {2019},
  month = oct,
  abstract = {In deep latent Gaussian models, the latent variable is generated by a time-inhomogeneous Markov chain, where at each time step we pass the current state through a parametric nonlinear map, such as a feedforward neural net, and add a small independent Gaussian perturbation. This work considers the diffusion limit of such models, where the number of layers tends to infinity, while the step size and the noise variance tend to zero. The limiting latent object is an It\^o diffusion process that solves a stochastic differential equation (SDE) whose drift and diffusion coefficient are implemented by neural nets. We develop a variational inference framework for these neural SDEs via stochastic automatic differentiation in Wiener space, where the variational approximations to the posterior are obtained by Girsanov (mean-shift) transformation of the standard Wiener process and the computation of gradients is based on the theory of stochastic flows. This permits the use of black-box SDE solvers and automatic differentiation for end-to-end inference. Experimental results with synthetic data are provided.},
  archivePrefix = {arXiv},
  eprint = {1905.09883},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/4FXJ2JBH/Tzen and Raginsky - 2019 - Neural Stochastic Differential Equations Deep Lat.pdf},
  journal = {arXiv:1905.09883 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@incollection{uryasevCombiningModelTest2006,
  title = {Combining {{Model}} and {{Test Data}} for {{Optimal Determination}} of {{Percentiles}} and {{Allowables}}: {{CVaR Regression Approach}}, {{Part II}}},
  shorttitle = {Combining {{Model}} and {{Test Data}} for {{Optimal Determination}} of {{Percentiles}} and {{Allowables}}},
  booktitle = {Robust {{Optimization}}-{{Directed Design}}},
  author = {Uryasev, Stan and Trindade, A. Alexandre},
  editor = {Kurdila, Andrew J. and Pardalos, Panos M. and Zabarankin, Michael},
  year = {2006},
  volume = {81},
  pages = {209--246},
  publisher = {{Kluwer Academic Publishers}},
  address = {{Boston}},
  doi = {10.1007/0-387-28654-3_10},
  abstract = {This report makes a more detailed assessment of the CVaR regression method proposed in [3] for determining A-Basis and B-Basis allowables, and quantifying the impact of test data and different analytical models on failure load predictions. Although the method can in principle be applied to any desired quantile, we will focus only on the 10th (B-Basis) in this study. We consider failure data arising from two sources: (1) a controlled environment where data is simulated from different Weibull distributions; (2) a supplied dataset similar to that of [3] augmented with failure load predictions from two additional analytical models (Model S2 and Model S3). Using absolute deviation between true and estimated 10th percentiles and the CVaR regression goodness-of-fit measure introduced in [3] as accuracy-assessment criteria, the key findings are as follows. The accuracy of CVaR regression is relatively insensitive to the number of batches present, but fairly sensitive to the number of test points per batch3. There are diminishing benefits in using more than 10 batches, or more than 10 test points per batch, in any one application of CVaR regression. The estimates of A-basis and B-basis are fairly robust, in the sense that they are not severely affected by miscalibrations (biases or errors) in the analytical models. Among the analytical models used as the sole input with no (input) test data, the best performer is Model S, followed by Model S2. Model S3 is the worst performer. The models contribute substantially to percentile prediction when up to 3 test points are used as input. When 4 test points are used as input, the 3 models can be roughly equated to the input information provided by one additional test point.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/YUEKCW5I/Uryasev and Trindade - 2006 - Combining Model and Test Data for Optimal Determin.pdf},
  isbn = {978-0-387-28263-3},
  language = {en}
}

@incollection{uryasevCombiningModelTest2006a,
  title = {Combining {{Model}} and {{Test Data}} for {{Optimal Determination}} of {{Percentiles}} and {{Allowables}}: {{CVaR Regression Approach}}, {{Part I}}},
  shorttitle = {Combining {{Model}} and {{Test Data}} for {{Optimal Determination}} of {{Percentiles}} and {{Allowables}}},
  booktitle = {Robust {{Optimization}}-{{Directed Design}}},
  author = {Uryasev, Stan and Trindade, A. Alexandre},
  editor = {Kurdila, Andrew J. and Pardalos, Panos M. and Zabarankin, Michael},
  year = {2006},
  volume = {81},
  pages = {179--207},
  publisher = {{Kluwer Academic Publishers}},
  address = {{Boston}},
  doi = {10.1007/0-387-28654-3_9},
  abstract = {We propose a coherent methodology for integrating various sources of variability on properties of materials in order to accurately predict percentiles of their failure load distribution. The approach involves the linear combination of factors that are associated with failure load, into a statistical factor model. This model directly estimates percentiles of the failure load distribution (rather than mean values as in ordinary least squares regression). A regression framework with CVaR deviation as the measure of optimality, is used in constructing the estimates. We consider estimates of confidence intervals for the estimates of percentiles, and adopt the most promising of these to compute A-Basis and B-Basis values. Numerical experiments with the available dataset show that the approach is quite robust, and can lead to a significant savings in number of actual testings. The approach pools together information from earlier experiments and model runs, with new experiments and model predictions, resulting in accurate inferences even in the presence of relatively small datasets.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/L5PJEMGN/Uryasev and Trindade - 2006 - Combining Model and Test Data for Optimal Determin.pdf},
  isbn = {978-0-387-28263-3},
  language = {en}
}

@article{vahdatNVAEDeepHierarchical2020,
  title = {{{NVAE}}: {{A Deep Hierarchical Variational Autoencoder}}},
  shorttitle = {{{NVAE}}},
  author = {Vahdat, Arash and Kautz, Jan},
  year = {2020},
  month = jul,
  abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ as shown in Fig. 1. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\texttimes 256 pixels.},
  archivePrefix = {arXiv},
  eprint = {2007.03898},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/UHRU3L4C/Vahdat and Kautz - 2020 - NVAE A Deep Hierarchical Variational Autoencoder.pdf},
  journal = {arXiv:2007.03898 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{varadiMinimumCorrelationAlgorithm,
  title = {Minimum {{Correlation Algorithm}}},
  author = {Varadi, David and Kapler, Michael and Bee, Henry and Rittenhouse, Corey},
  pages = {17},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/6F5EAXFA/Varadi et al. - Minimum Correlation Algorithm.pdf},
  language = {en}
}

@misc{VariationalAutoencodersJointly2019,
  title = {Variational {{Autoencoders}} with {{Jointly Optimized Latent Dependency Structure}} | {{OpenReview}}},
  year = {2019},
  month = feb,
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/Variational Autoencoders with Jointly Optimized Latent Dependency Structure.pdf;/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/IDRINTGK/forum.html},
  howpublished = {https://openreview.net/forum?id=SJgsCjCqt7}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archivePrefix = {arXiv},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/YQBHCITI/Vaswani et al. - 2017 - Attention Is All You Need.pdf},
  journal = {arXiv:1706.03762 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{velickovicDeepGraphInfomax2018,
  title = {Deep {{Graph Infomax}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Fedus, William and Hamilton, William L. and Li{\`o}, Pietro and Bengio, Yoshua and Hjelm, R. Devon},
  year = {2018},
  month = dec,
  abstract = {We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs\textemdash both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1809.10341},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/CSXKVIS3/Veliƒçkoviƒá et al. - 2018 - Deep Graph Infomax.pdf},
  journal = {arXiv:1809.10341 [cs, math, stat]},
  keywords = {‚õî No DOI found,Computer Science - Information Theory,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{velickovicDeepGraphInfomax2018a,
  title = {Deep {{Graph Infomax}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Fedus, William and Hamilton, William L. and Li{\`o}, Pietro and Bengio, Yoshua and Hjelm, R. Devon},
  year = {2018},
  month = dec,
  abstract = {We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs\textemdash both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1809.10341},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/ZBKJSYUP/Veliƒçkoviƒá et al. - 2018 - Deep Graph Infomax.pdf},
  journal = {arXiv:1809.10341 [cs, math, stat]},
  keywords = {‚õî No DOI found,Computer Science - Information Theory,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{voDynamicAssetAllocation2013,
  title = {Dynamic {{Asset Allocation}} with {{Regime Shifts}} and {{Long Horizon CVaR}}-{{Constraints}}},
  author = {Vo, Huy Thanh and Maurer, Raimond},
  year = {2013},
  issn = {1556-5068},
  doi = {10/gd6dxv},
  abstract = {We analyse portfolio policies for investors who invest optimally for given investment horizons with respect to Conditional Value-at-Risk constraints. We account for nonnormally distributed, skewed, and leptokurtic asset return distributions due to regime shifts. The focus is on standard CRRA utility with a money back guarantee at maturity, which is often augmented to individual retirement plans. Optimal solutions for the unconstrained as well as the constrained policy are provided and examined for risk management costs calculated as welfare losses. Our results confirm previous findings that money back guarantees yield mild downside protection at low economic costs for most long term investors.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/TUPNN6AE/Vo and Maurer - 2013 - Dynamic Asset Allocation with Regime Shifts and Lo.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{wangLinformerSelfAttentionLinear2020,
  title = {Linformer: {{Self}}-{{Attention}} with {{Linear Complexity}}},
  shorttitle = {Linformer},
  author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  year = {2020},
  month = jun,
  abstract = {Large transformer models have shown extraordinary success in achieving state-ofthe-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses O(n2) time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from O(n2) to O(n) in both time and space. The resulting linear transformer, the Linformer, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
  archivePrefix = {arXiv},
  eprint = {2006.04768},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/TTBNFD63/Wang et al. - 2020 - Linformer Self-Attention with Linear Complexity.pdf},
  journal = {arXiv:2006.04768 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{wangNewPortfolioRebalancing2014,
  title = {A {{New Portfolio Rebalancing Model}} with {{Transaction Costs}}},
  author = {Wang, Meihua and Li, Cheng and Xue, Honggang and Xu, Fengmin},
  year = {2014},
  volume = {2014},
  pages = {1--7},
  issn = {1110-757X, 1687-0042},
  doi = {10/f52zz2},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/W5QHDIVA/Wang et al. - 2014 - A New Portfolio Rebalancing Model with Transaction.pdf},
  journal = {Journal of Applied Mathematics},
  language = {en}
}

@article{wangTimeSeriesClassification2016,
  title = {Time {{Series Classification}} from {{Scratch}} with {{Deep Neural Networks}}: {{A Strong Baseline}}},
  shorttitle = {Time {{Series Classification}} from {{Scratch}} with {{Deep Neural Networks}}},
  author = {Wang, Zhiguang and Yan, Weizhong and Oates, Tim},
  year = {2016},
  month = dec,
  abstract = {We propose a simple but strong baseline for time series classification from scratch with deep neural networks. Our proposed baseline models are pure end-to-end without any heavy preprocessing on the raw data or feature crafting. The proposed Fully Convolutional Network (FCN) achieves premium performance to other state-of-the-art approaches and our exploration of the very deep neural networks with the ResNet structure is also competitive. The global average pooling in our convolutional model enables the exploitation of the Class Activation Map (CAM) to find out the contributing region in the raw data for the specific labels. Our models provides a simple choice for the real world application and a good starting point for the future research. An overall analysis is provided to discuss the generalization capability of our models, learned features, network structures and the classification semantics.},
  archivePrefix = {arXiv},
  eprint = {1611.06455},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/C595XYGI/Wang et al. - 2016 - Time Series Classification from Scratch with Deep .pdf},
  journal = {arXiv:1611.06455 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{weinbergWhatQuantumField1997,
  title = {What Is {{Quantum Field Theory}}, and {{What Did We Think It Is}}?},
  author = {Weinberg, Steven},
  year = {1997},
  month = feb,
  abstract = {This is a talk presented at the conference ``Historical and Philosophical Reflections on the Foundations of Quantum Field Theory,'' at Boston University, March 1996. It will be published in the proceedings of this conference.},
  archivePrefix = {arXiv},
  eprint = {hep-th/9702027},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/RETW6T82/Weinberg - 1997 - What is Quantum Field Theory, and What Did We Thin.pdf},
  journal = {arXiv:hep-th/9702027},
  keywords = {üîçNo DOI found,Condensed Matter,General Relativity and Quantum Cosmology,High Energy Physics - Phenomenology,High Energy Physics - Theory},
  language = {en}
}

@article{winklerLearningLikelihoodsConditional2019,
  title = {Learning {{Likelihoods}} with {{Conditional Normalizing Flows}}},
  author = {Winkler, Christina and Worrall, Daniel and Hoogeboom, Emiel and Welling, Max},
  year = {2019},
  month = nov,
  abstract = {Normalizing Flows (NFs) are able to model complicated distributions pY (y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density pZ(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities pY |X (y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.},
  archivePrefix = {arXiv},
  eprint = {1912.00042},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/2IRNLKIM/Winkler et al. - 2019 - Learning Likelihoods with Conditional Normalizing .pdf},
  journal = {arXiv:1912.00042 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{wittyBayesianCausalInference2019,
  title = {Bayesian Causal Inference via Probabilistic Program Synthesis},
  author = {Witty, Sam and Lew, Alexander and Jensen, David and Mansinghka, Vikash},
  year = {2019},
  month = oct,
  abstract = {Causal inference can be formalized as Bayesian inference that combines a prior distribution over causal models and likelihoods that account for both observations and interventions. We show that it is possible to implement this approach using a sufficiently expressive probabilistic programming language. Priors are represented using probabilistic programs that generate source code in a domain specific language. Interventions are represented using probabilistic programs that edit this source code to modify the original generative process. This approach makes it straightforward to incorporate data from atomic interventions, as well as shift interventions, variance-scaling interventions, and other interventions that modify causal structure. This approach also enables the use of general-purpose inference machinery for probabilistic programs to infer probable causal structures and parameters from data. This abstract describes a prototype of this approach in the Gen probabilistic programming language.},
  archivePrefix = {arXiv},
  eprint = {1910.14124},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/IXNJ4KSF/Witty et al. - 2019 - Bayesian causal inference via probabilistic progra.pdf},
  journal = {arXiv:1910.14124 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{wuDeepTransformerModels2020,
  title = {Deep {{Transformer Models}} for {{Time Series Forecasting}}: {{The Influenza Prevalence Case}}},
  shorttitle = {Deep {{Transformer Models}} for {{Time Series Forecasting}}},
  author = {Wu, Neo and Green, Bradley and Ben, Xue and O'Banion, Shawn},
  year = {2020},
  month = jan,
  abstract = {In this paper, we present a new approach to time series forecasting. Time series data are prevalent in many scientific and engineering disciplines. Time series forecasting is a crucial task in modeling time series data, and is an important area of machine learning. In this work we developed a novel method that employs Transformer-based machine learning models to forecast time series data. This approach works by leveraging selfattention mechanisms to learn complex patterns and dynamics from time series data. Moreover, it is a generic framework and can be applied to univariate and multivariate time series data, as well as time series embeddings. Using influenzalike illness (ILI) forecasting as a case study, we show that the forecasting results produced by our approach are favorably comparable to the stateof-the-art.},
  archivePrefix = {arXiv},
  eprint = {2001.08317},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/NIIHFJRQ/Wu et al. - 2020 - Deep Transformer Models for Time Series Forecastin.pdf},
  journal = {arXiv:2001.08317 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{wurtzPortfolioOptimizationRmetrics,
  title = {Portfolio {{Optimization}} with {{R}}/{{Rmetrics}}},
  author = {W{\"u}rtz, Diethelm},
  pages = {42},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/ELLTETT3/W√ºrtz - Portfolio Optimization with RRmetrics.pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@article{yakovenkoDeepMindSelfLearningAtari,
  title = {{{DeepMind Self}}-{{Learning Atari Agent}}  ``{{Human}}-Level Control through Deep Reinforcement Learning'' \textendash{} {{Nature Vol}} 518, {{Feb}} 26, 2015  ``{{The Deep Mind}} of {{Demis Hassabis}}'' \textendash{} {{Backchannel}} / {{Medium}}.Com \textendash{} Interview with {{David Levy}}},
  author = {Yakovenko, Nikolai},
  pages = {68},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/HRTQPDJV/Yakovenko - DeepMind Self-Learning Atari Agent  ‚ÄúHuman-level c.pdf},
  language = {en}
}

@article{yangCondConvConditionallyParameterized2019,
  title = {{{CondConv}}: {{Conditionally Parameterized Convolutions}} for {{Efficient Inference}}},
  shorttitle = {{{CondConv}}},
  author = {Yang, Brandon and Bender, Gabriel and Le, Quoc V. and Ngiam, Jiquan},
  year = {2019},
  month = oct,
  abstract = {Convolutional layers are one of the basic building blocks of modern deep neural networks. One fundamental assumption is that convolutional kernels should be shared for all examples in a dataset. We propose conditionally parameterized convolutions (CondConv), which learn specialized convolutional kernels for each example. Replacing normal convolutions with CondConv enables us to increase the size and capacity of a network, while maintaining efficient inference. We demonstrate that scaling networks with CondConv improves the performance and inference cost trade-off of several existing convolutional neural network architectures on both classification and detection tasks. On ImageNet classification, our CondConv approach applied to EfficientNet-B0 achieves state-ofthe-art performance of 78.3\% accuracy with only 413M multiply-adds. Code and checkpoints for the CondConv Tensorflow layer and CondConv-EfficientNet models are available at: https://github.com/tensorflow/tpu/tree/master/ models/official/efficientnet/condconv.},
  archivePrefix = {arXiv},
  eprint = {1904.04971},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/3E44HLC5/Yang et al. - 2019 - CondConv Conditionally Parameterized Convolutions.pdf},
  journal = {arXiv:1904.04971 [cs]},
  keywords = {‚õî No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{yehAutomaticBridgeBidding2016,
  title = {Automatic {{Bridge Bidding Using Deep Reinforcement Learning}}},
  author = {Yeh, Chih-Kuan and Lin, Hsuan-Tien},
  year = {2016},
  month = jul,
  abstract = {Bridge is among the zero-sum games for which artificial intelligence has not yet outperformed expert human players. The main difficulty lies in the bidding phase of bridge, which requires cooperative decision making under partial information. Existing artificial intelligence systems for bridge bidding rely on and are thus restricted by human-designed bidding systems or features. In this work, we propose a pioneering bridge bidding system without the aid of human domain knowledge. The system is based on a novel deep reinforcement learning model, which extracts sophisticated features and learns to bid automatically based on raw card data. The model includes an upper-confidence-bound algorithm and additional techniques to achieve a balance between exploration and exploitation. Our experiments validate the promising performance of our proposed model. In particular, the model advances from having no knowledge about bidding to achieving superior performance when compared with a champion-winning computer bridge program that implements a human-designed bidding system.},
  archivePrefix = {arXiv},
  eprint = {1607.03290},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/CF8T2Z46/Yeh and Lin - 2016 - Automatic Bridge Bidding Using Deep Reinforcement .pdf},
  journal = {arXiv:1607.03290 [cs]},
  keywords = {üîçNo DOI found,Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@article{zhangPREDICTIONFINANCIALTIME,
  title = {{{PREDICTION OF FINANCIAL TIME SERIES WITH HIDDEN MARKOV MODELS}}},
  author = {Zhang, Yingjian},
  pages = {102},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/T7NURYJ7/Zhang - PREDICTION OF FINANCIAL TIME SERIES WITH HIDDEN MA.pdf},
  keywords = {üîçNo DOI found},
  language = {en}
}

@article{zhukovskyPHOTOGRAMMETRICTECHNIQUESUNDERWATER2013,
  title = {{{PHOTOGRAMMETRIC TECHNIQUES FOR}} 3 \&ndash; {{D UNDERWATER RECORD OF THE ANTIQUE TIME SHIP FROM PHANAGORIA}}},
  author = {Zhukovsky, M. O. and Kuznetsov, V. D. and Olkhovsky, S. V.},
  year = {2013},
  month = jul,
  volume = {XL-5/W2},
  pages = {717--721},
  issn = {1682-1777},
  doi = {10.5194/isprsarchives-XL-5-W2-717-2013},
  abstract = {Phanagoria \textendash{} the largest known ancient Greek settlement on the territory of Russia is situated on the Taman peninsula on the southern side of the Taman bay. The unique feature of the site is that about 1/3 of the settlement of Phanagoria is currently flooded by waters of the Taman bay due to the transgression of the Black sea level since antiquity. In 2012 in the course of underwater prospection of the Taman bay a wooden ship buried under the 1.5 m thick bottom sediments was discovered in situ. The unique feature of the ship is excellent preservation of its wooden parts, which makes it one of the few finds of this kind ever made on the territory of Russia. This paper presents a case-study of application of photogrammetry technique for archaeological field documentation record in course of underwater excavations of the Phanagorian shipwreck. The advantages and possible underwaterspecific constraints of automated point cloud extraction algorithm which was used in the research are discussed. The paper gives an overview of the practical aspects of the workflow of photgrammetry technique application at the excavation ground: photo capture procedure and measurement of control points. Finally a resulting 3-D model of the shipwreck is presented and high potential of automated point cloud extraction algorithms for archaeological documentation record is concluded.},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/6PIQ7FQV/Zhukovsky et al. - 2013 - PHOTOGRAMMETRIC TECHNIQUES FOR 3 &ndash\; D UNDERWA.pdf},
  journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  language = {en}
}

@article{zieglerLatentNormalizingFlows2019,
  title = {Latent {{Normalizing Flows}} for {{Discrete Sequences}}},
  author = {Ziegler, Zachary M. and Rush, Alexander M.},
  year = {2019},
  month = jun,
  abstract = {Normalizing flows are a powerful class of generative models for continuous random variables, showing both strong model flexibility and the potential for non-autoregressive generation. These benefits are also desired when modeling discrete random variables such as text, but directly applying normalizing flows to discrete sequences poses significant additional challenges. We propose a VAE-based generative model which jointly learns a normalizing flow-based distribution in the latent space and a stochastic mapping to an observed discrete space. In this setting, we find that it is crucial for the flow-based distribution to be highly multimodal. To capture this property, we propose several normalizing flow architectures to maximize model flexibility. Experiments consider common discrete sequence tasks of character-level language modeling and polyphonic music generation. Our results indicate that an autoregressive flow-based model can match the performance of a comparable autoregressive baseline, and a non-autoregressive flow-based model can improve generation speed with a penalty to performance.},
  archivePrefix = {arXiv},
  eprint = {1901.10548},
  eprinttype = {arxiv},
  file = {/home/emmanuel/Documents/SyncThing/Syncthing/Personal research/Zotero/storage/V9NZ8YRZ/Ziegler and Rush - 2019 - Latent Normalizing Flows for Discrete Sequences.pdf},
  journal = {arXiv:1901.10548 [cs, stat]},
  keywords = {‚õî No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{ÔøΩ,
  keywords = {\#nosource}
}

@article{ÔøΩ,
  type = {Article}
}


