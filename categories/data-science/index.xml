<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science on Back2Numbers</title>
    <link>/categories/data-science.html</link>
    <description>Recent content in Data Science on Back2Numbers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-au</language>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Wed, 25 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/data-science/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Forecasting the progression of COVID-19</title>
      <link>/2020/03/25/2020-03-25-forecasting-covid-19.html</link>
      <pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/25/2020-03-25-forecasting-covid-19.html</guid>
      <description>The Neherlab COVID-19 forecast model using CSV, Dates; using DataFrames, DataFramesMeta; using Plots, PyPlot; using DifferentialEquations; This is more a data science post than machine learning. It was born after reading a report from Imperial College London and finding a forecasting model by NeherLab. The numbers produced by those models can only be described as terrifying.
How do those models work? How are they calibrated?
BUT
Remember that whatever concerns one can have about their precision, those models are all absolutely clear that social-distancing, quarantining have a massive impact on death rates.</description>
    </item>
    
    <item>
      <title>HarvardX Gitbooks available</title>
      <link>/2019/12/12/harvardx-gitbooks-available.html</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/12/harvardx-gitbooks-available.html</guid>
      <description>Both capstones for the HarvardX certificates are now available. Just click on the links!
If Gitbooks are not your thing, at the top of their main page, there is a download link to a pdf version.
They make for a good knock-me-asleep readingâ€¦</description>
    </item>
    
    <item>
      <title>HarvardX Final Report - LendingClub dataset</title>
      <link>/2019/12/11/harvardx-final-report-lendingclub-dataset.html</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/11/harvardx-final-report-lendingclub-dataset.html</guid>
      <description>After 3 months of work, the final report for the HarvardX Data Science course was submitted.
It is based on the LendingClub dataset. LendingClub is a peer-2-peer lender. This is a matching of private borrowers and investors. Small amounts, fairly high risk (if they could, borrowers would probably have had a bank involved). Surprisingly, after tapping a market of individual lenders, the biggest lenders are now the banks. To inform the investors, LendingClub make historical information publicly available.</description>
    </item>
    
    <item>
      <title>HarvardX Data Science course - First final project</title>
      <link>/2019/10/05/harvardx-data-science-course-first-final-project.html</link>
      <pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/05/harvardx-data-science-course-first-final-project.html</guid>
      <description>I recently finished to penultimate final assignment for the HarvardX Data Science course. The Stanford course was clearly machine learning. This one is definitely lighter on the machine learning and much heavier on the data science: how to source, clean and visualise data are key skills. The targeted knowledge is more traditional probabilities/statistics. Long-existing fundamental techniques like inference, polling are there.
This time R is the centre tool of the course.</description>
    </item>
    
  </channel>
</rss>