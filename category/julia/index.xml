<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Julia | Back2Numbers</title>
    <link>/category/julia.html</link>
      <atom:link href="/category/julia/index.xml" rel="self" type="application/rss+xml" />
    <description>Julia</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Emmanuel Rialland 2020</copyright><lastBuildDate>Fri, 14 Aug 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Julia</title>
      <link>/category/julia.html</link>
    </image>
    
    <item>
      <title>Normalising Flows and Neural ODEs</title>
      <link>/2020/08/14/normalising-flows.html</link>
      <pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/2020/08/14/normalising-flows.html</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/d3/d3.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/dagre/dagre-d3.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/mermaid/dist/mermaid.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/mermaid/dist/mermaid.slim.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/chromatography/chromatography.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/DiagrammeR-binding/DiagrammeR.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#a-few-words-about-generative-models&#34;&gt;A few words about Generative Models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#latent-variables&#34;&gt;Latent variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#examples-of-generative-models&#34;&gt;Examples of generative models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#generative-adversarial-networks-gans&#34;&gt;Generative Adversarial Networks (&lt;strong&gt;GANS&lt;/strong&gt;)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variational-autoencoders&#34;&gt;Variational autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#limitations&#34;&gt;Limitations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#normalising-flows&#34;&gt;Normalising flows&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-1&#34;&gt;Introduction&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#some-math&#34;&gt;Some math&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#training-loss-optimisation-and-information-flow&#34;&gt;Training loss optimisation and information flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#basic-flows&#34;&gt;Basic flows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#planar-flows&#34;&gt;Planar Flows&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#radial-flows&#34;&gt;Radial flows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#more-complex-flows&#34;&gt;More complex flows&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#residual-flows-discrete-flows&#34;&gt;Residual flows (discrete flows)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-versions&#34;&gt;Other versions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#continuous-flows&#34;&gt;Continuous flows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#neural-ordinary-differential-equations&#34;&gt;Neural ordinary differential equations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#continuous-flows-1&#34;&gt;Continuous flows&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#continuous-flows-means-no-crossover&#34;&gt;Continuous flows means no-crossover&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#universal-ordinary-differential-equations&#34;&gt;Universal ordinary differential equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#stochastic-differential-equations&#34;&gt;Stochastic differential equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other&#34;&gt;Other&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimisation&#34;&gt;Optimisation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#more-information&#34;&gt;More information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#one-more-thing&#34;&gt;One more thing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;One of the three best papers awarded at NIPS 2018 was &lt;em&gt;Neural Ordinary Differential Equations&lt;/em&gt; by Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt and David Duvenaud &lt;span class=&#34;citation&#34;&gt;(Chen et al. &lt;a href=&#34;#ref-chenNeuralOrdinaryDifferential2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;. Since then, the field has developed in multiple directions. This post goes through some background about generative models, normalising flows and finally a few of the underlying ideas of the paper. The form does not intend to be mathematically rigorous but convey some intuitions.&lt;/p&gt;
&lt;div id=&#34;a-few-words-about-generative-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A few words about Generative Models&lt;/h1&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Generative models&lt;/strong&gt; are about learning simple &lt;strong&gt;representations&lt;/strong&gt; of a complex datasets; how to, from a few parameters, generate realistic samples that are similar to a given dataset. Those few parameters usually follow simple distributions (e.g. uniform or Gaussian), and are transformed through complex transformations into the more complex dataset distribution. This is an unsupervised procedure which, in a sense, mirrors clustering methods: clustering starts from the dataset and summarises it into few parameters.&lt;/p&gt;
&lt;p&gt;[DIAGRAM var 1D to another var 1D]&lt;/p&gt;
&lt;p&gt;[DIAGRAM var 1D to another var 2D]&lt;/p&gt;
&lt;p&gt;Although unsupervised, the result of this learning can be used as a &lt;strong&gt;pretraining&lt;/strong&gt; step in a later supervised context, or where that dataset is a mix of labelled and un-labelled data. The properties of the well-understood starting probability distributions can then help draw conclusions about the dataset’s distribution or generate synthetic datasets.&lt;/p&gt;
&lt;p&gt;The same methods can also be used in supervised learning to learn the representation of a target dataset (categorical or continuous) as a transformation of the features dataset. The unsupervised becomes supervised.&lt;/p&gt;
&lt;p&gt;What does &lt;em&gt;representation learning&lt;/em&gt; actually mean? It is the automatic search for a few parameters that encapsulate rich enough information to generate a dataset. Generative models learn those parameters and, starting from them, how to re-create samples similar to the original dataset.&lt;/p&gt;
&lt;p&gt;Let’s use cars as an analogy.&lt;/p&gt;
&lt;p&gt;All cars have 4 wheels, an engine, brakes, seats. One could be interested in comfort or racing them or lugging things around or safety or fitting as many kids as possible. Each base vector could be express any one of those characteristics, but they will all have an engine, breaks and seats. The generation function recreates everything that is common. It doesn’t matter if the car is comfy or not; it needs seats and a driving wheel. The generative function has to create those features. However, the exact number of cylinders, its shape, the seats fabric, or stiffness of the suspension all depend on the type of car.&lt;/p&gt;
&lt;p&gt;The tru fundamentals are not obvious. For a long time, American cars had softer suspension than European cars. The definition of comfortable is relative. The performance of an old car is objectively not the same as compared to new ones. Maybe other characteristics are more relevant to generate. Maybe price? Consumption? Year of coming to market? All those factors are obviously inter-related.&lt;/p&gt;
&lt;p&gt;Generative models are more than generating samples from a few fundamental parameters. They also learn what those parameters should be.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;latent-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Latent variables&lt;/h2&gt;
&lt;p&gt;Still using the car analogy, if the year of a model was not given, the generative process might still be able to conclude that the model year &lt;em&gt;should&lt;/em&gt; be an implicit parameter to be learned since relevant to generate the dataset: year is an unstated parameter that explains the dataset. Both the Lamborghini Miura and Lamborghini Countach&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; were similar in terms of perceived performance and exclusivity at the time they were created. But actual performances and styling where incredibly different.&lt;/p&gt;
&lt;p&gt;If looking at the stock market: take a set of market prices at a given date; it would have significantly different meanings in a bull or a bear market. Market regime would be a reasonable latent variable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;examples-of-generative-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Examples of generative models&lt;/h2&gt;
&lt;p&gt;There are quite a number of generative models such restricted Boltzmann machines, deep belief networks. Refer to &lt;span class=&#34;citation&#34;&gt;(Theodoridis &lt;a href=&#34;#ref-theodoridisMachineLearningBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(Russell and Norvig &lt;a href=&#34;#ref-russellArtificialIntelligenceModern2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; for example. Let’s consider generative adversarial networks and variational auto-encoders.&lt;/p&gt;
&lt;div id=&#34;generative-adversarial-networks-gans&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generative Adversarial Networks (&lt;strong&gt;GANS&lt;/strong&gt;)&lt;/h3&gt;
&lt;p&gt;Recently, GANs have risen to the fore as a way to generate artificial datasets that are, for some definition, indistinguishable from a real dataset. They consist of two parts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Editor at 
# https://mermaid-js.github.io/mermaid-live-editor
DiagrammeR::mermaid(&amp;quot;GAN.mmd&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;\ngraph LR\n\n  subgraph Generative Model\n    Representation[\&#34;Representation &lt;br/&gt; (latent space)\&#34;] --&gt; Generator[&lt;b&gt;Generator&lt;\/b&gt;]\n  end\n\n  Generator--&gt; Discriminator[&lt;b&gt;Discriminator&lt;\/b&gt;]\n\n  Sample\n\n  Sample[Sample] --&gt; Discriminator\n\n  Discriminator --&gt; ID[Identification as &lt;br/&gt; generated or true]\n\n  ID -.-&gt;|Feedback| Discriminator\n  ID -.-&gt;|Feedback| Generator&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A generator which is the generative model itself: given a simple representation, the generator proposes samples that aim to be undistinguishable from the dataset sample.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A discriminator whose job is to identify whether a sample comes from the generator or from the dataset.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both are trained simultaneously:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;if the discriminator finds it obvious to guess, the generator is not doing a good job and needs to improve;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if the discriminator guesses 50/50 (does not do better than flipping a coin), it has to discover which true dataset features are truly relevant.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;variational-autoencoders&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variational autoencoders&lt;/h3&gt;
&lt;p&gt;A successful GAN can replicate the richness of a dataset, but not the its distribution. A GAN can generate a large number of correct sentences, but will not tell how likely to occur that sentence is (or at least guarantee that the distributions match). &lt;em&gt;‘The dog chases the cat’&lt;/em&gt; and &lt;em&gt;‘The Chihuahua chases the cat’&lt;/em&gt; are both perfectly valid, but the latter less unlikely to appear.&lt;/p&gt;
&lt;p&gt;Variable autoencoders (VAEs) take another approach by learning a generator (called &lt;em&gt;decoder&lt;/em&gt;) &lt;em&gt;and&lt;/em&gt; learning the distribution of the parameters to reflect the distribution of the samples within the dataset (the &lt;em&gt;encoder&lt;/em&gt;). Both are trained simultaneously on the dataset samples by projecting samples on the latent variables’ space, proposing a generated sample from that projection and training on the reconstruction loss. The encoder actually learns means and standard deviations of the each latent variable, wach being a normal distribution. The samples generated will be as rich as the GAN’s, but the probability of a sample being generated will depend on the learned distributions.&lt;/p&gt;
&lt;p&gt;See &lt;span class=&#34;citation&#34;&gt;(Kingma and Welling &lt;a href=&#34;#ref-kingmaIntroductionVariationalAutoencoders2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; for an approachable extensive introduction. The details of VAEs include implementation aspects (in particular the &lt;em&gt;reparametrization trick&lt;/em&gt;) that are critical to the success of this approach.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;limitations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Limitations&lt;/h2&gt;
&lt;p&gt;We limited the introduction to those two techniques to merely highlight two fundamental aspect that generative models aim at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;explore and replicate the richness of the dataset;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;replicate the probability distribution of the dataset.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that depending on the circumstances, the latter aim may not necessarily be important.&lt;/p&gt;
&lt;p&gt;As usual, training and optimisation methods are at risk of getting stuck at local optima. In the case of those two techniques, this manifests itself in different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;GANs Mode collapse&lt;/em&gt;: Mode collapse occurs in GANs when the generator only explores limited domains. Imagine training a GAN to recognise mammals (the dataset would contain kangaroos, whales, dogs and cats…). If the generator proposes everything but kangoaroos, it is still properly generate mammals, but obviously misses out on a few possibilities. Essentially, the generator reaches a local minimum where the gradient becomes too small to explore alternatives. This is in part due to the difficulty of progressing the training of both the generator and the discriminator in a way that does not lock any one of them in a local optimum while the other still needs improving: if either converges too rapidly, the other will struggle improving.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;VAEs Posterior collapse&lt;/em&gt;: Posterior collapse in VAEs arises when the generative model learns to ignore a subset of the latent variables (although the encoder generates those variables) &lt;span class=&#34;citation&#34;&gt;(Lucas et al. &lt;a href=&#34;#ref-lucasDonBlameELBO2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;&lt;a href=&#34;#ref-lucasDonBlameELBO2019&#34; role=&#34;doc-biblioref&#34;&gt;b&lt;/a&gt;)&lt;/span&gt;. More technically, it happens when the variational distribution closely matches the uninformative prior for a subset of latent variables &lt;span class=&#34;citation&#34;&gt;(Lucas et al. &lt;a href=&#34;#ref-lucasUnderstandingPosteriorCollapse2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;&lt;a href=&#34;#ref-lucasUnderstandingPosteriorCollapse2019&#34; role=&#34;doc-biblioref&#34;&gt;a&lt;/a&gt;)&lt;/span&gt;. The exact reasons for this are not entirely understood and this remains an active area of research (refer this extensive list of &lt;a href=&#34;https://github.com/sajadn/posterior-collapse-list&#34;&gt;papers&lt;/a&gt; on the topic).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we will see, normalising flows address those two difficulties. Intuitively:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Mode collapse reflects that the generative process does not generate enough possibilities; that the spectrum of possibilities is not as rich as that of the dataset. Normalising flows attempt to address this in two ways. Firstly, the optimising process aims as optimising (and matching) the amount of generated information to that of the dataset. Secondly, normalising flows allow to start from a sample in the dataset, flow back to the simple distribution and estimate how (un)likely the generative model would have generated this sample. If the dataset has a lot of whales, generating only dogs is clearly not good enough…&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The posterior collapse could simply be a mismatch between the number of latent variables and the dimensionality of the dataset, difficulties to specify an effective loss function (and its gradients) or a local optima. As we will see, normalising flows impose that the generative model be a bijection. In a sense, this means that there should be the same ‘complexity’ in the latent variables as in the dataset (although not the same amount of information in the sense of information theory).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;lt;!–&amp;gt;
## &lt;strong&gt;CHECK FOLLOWING&lt;/strong&gt; Revival of neural networks&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In this section, the discussion setting is to learn representations that are independent of a specific
target task; the goal is to extract such information using the input data only. The reason for such a focus
is twofold. First, learning a model representation of the input data can be used subsequently in different
tasks in order to facilitate the training. Sometimes, this is also known as pretraining, where parameters
learned using unlabeled data can be used as initial estimates of the parameters for another supervised
learning. This can be useful when the number of labeled examples is not large enough (see, e.g., [148]
for a discussion). It is worth pointing out that such a pretraining rationale is of a historical importance,
because it led to the revival of neural networks, as will be discussed soon [86].&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;86 is &lt;span class=&#34;citation&#34;&gt;(“A Fast Learning Algorithm for Deep Belief Nets” &lt;a href=&#34;#ref-FastLearningAlgorithm2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt; from Bengio&lt;/p&gt;
&lt;p&gt;&amp;lt;!–&amp;gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All methodology involve finding a function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. Neural networks have long been known to be able to generate artificially complicated functions and the idea of using them as a way to represent &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is one of the reasons that led to their revivals.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;normalising-flows&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Normalising flows&lt;/h1&gt;
&lt;p&gt;Normalising Flows became popular around 2015 with two papers on density estimation &lt;span class=&#34;citation&#34;&gt;(Dinh, Krueger, and Bengio &lt;a href=&#34;#ref-dinhNICENonlinearIndependent2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; and use of variational inference &lt;span class=&#34;citation&#34;&gt;(Rezende and Mohamed &lt;a href=&#34;#ref-rezendeVariationalInferenceNormalizing2016&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;. However, the concepts predated those papers. (See &lt;span class=&#34;citation&#34;&gt;(Kobyzev, Prince, and Brubaker &lt;a href=&#34;#ref-kobyzevNormalizingFlowsIntroduction2020a&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(Papamakarios et al. &lt;a href=&#34;#ref-papamakariosNormalizingFlowsProbabilistic2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; for recent survey papers.)&lt;/p&gt;
&lt;div id=&#34;introduction-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;One important limitations of the approaches described above is that the generation flow is unidirectional: one starts from a source distribution, sometimes with well-known properties, and generates a richer target distribution. However, given a particular sample in the target distribution, there is no guaranteed way to identify where it would fall in the original source distribution. That flow of transformation from source to target is not guaranteed to be bijective or invertible (same meaning, different crowds). (In the case of VAEs, this is the case in the encoder.)&lt;/p&gt;
&lt;p&gt;Normalising flows are a generic solution to that issue: it is a transformation of an original distribution into a more complex distribution by an invertible and differentiable mapping, where the probability density of a sample can be evaluated by transforming it back to the original distribution. The density is evaluated by computing the density of the normalised inverse-transformed sample.&lt;/p&gt;
&lt;p&gt;In practice, this is a bit too general to be of any use. Breaking it down:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The original distribution is simple with well-known statistical properties: i.i.d. Gaussian or uniform distributions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The transformation function is expected to be complicated, and is normally specified as a series of successive transformations, each simpler (though expressive enough) and easy to parametrise.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each simple transformation is itself invertible and differentiable, therefore guaranteeing that the overall transformation is too.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We want the transformation to be &lt;em&gt;normalised&lt;/em&gt;: the cumulative probability density of the generated targets from latent variables has to be equal 1. Otherwise, flowing backwards to use the properties of the original would make no sense. The word &lt;em&gt;normalising&lt;/em&gt; refers to this operation, and not to the fact that the original distribution &lt;em&gt;could&lt;/em&gt; be normal.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Geometrically, the probability distribution around each point in the latent variables space has a certain volume that is successively transformed with each transformation. Keeping track of all the volume changes ensures that we can relate probability density functions in the original space and the target space.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to keep track? This is where the condition of having invertible and differentiable transformation becomes important. (Math-speak: we have a series of diffeomorphisms which are transformations from one infinitesimal volume to another. They are invertible and differentiable, and their inverses are also differentiable.) If one imagines a small volume of space around a starting point, that volume gets distorted along the way. At each point, the transformation is differentiable and can be approximated by a linear transformation (a matrix). That matrix is the Jacobian of the transformation at that point (diffeomorphims also means that the Jacobian matrix exists and is invertible). Being invertible, the matrix has no zero eigenvalues and the change of volume is locally equal to the product of all the eigenvalues: the volume gets squeezed along some dimensions, expanded along others, rotations are irrelevant. The product of the eigenvalues is the determinant of the matrix. A negative eigenvalue would mean that the infinitesimal volume is ‘flipped’ along that direction. That sign is irrelevant: the local volume change is therefore the absolute value of the determinant.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can already anticipate a calculation nightmare: determinants are computationally very heavy. Additionally, in order to backpropagate a loss to optimise the transformations’ parameters, we will need the Jacobians of the inverse transformations (the inverse of the transformmation Jacobian). Without further simplifying assumptions or tricks, normalising flows would be impractical for large dimensions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;some-math&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some math&lt;/h3&gt;
&lt;p&gt;The starting distribution is a random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; with a support in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^D\)&lt;/span&gt;. For simplicity, we will assume just assume that the support is &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^D\)&lt;/span&gt; since using measurable supports does not change the results. If &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is transformed into &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; by an invertible function/mapping &lt;span class=&#34;math inline&#34;&gt;\(f: \mathbb{R}^D \rightarrow \mathbb{R}^D\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(Y=f(X)\)&lt;/span&gt;), then the density function of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P_Y(\vec{y}) &amp;amp; = P_X(\vec{x}) \left| \det \nabla f^{-1}(\vec{y})  \right| \\
                &amp;amp; = P_X(\vec{x}) \left| \det\nabla f(\vec{x}) \right|^{-1}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\vec{x} = f^{-1}(\vec{y})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nabla\)&lt;/span&gt; represents the Jacobian operator. Note the use of &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; to denote vectors instead of the normal &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; which I find on-screen easy to mistake for a constant.&lt;/p&gt;
&lt;p&gt;Following &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is the &lt;em&gt;generative&lt;/em&gt; direction; following &lt;span class=&#34;math inline&#34;&gt;\(f^{-1}\)&lt;/span&gt; is the &lt;em&gt;normalising&lt;/em&gt; direction (as well as being the &lt;em&gt;inference&lt;/em&gt; direction in a more general context).&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; was a series of individual transformation &lt;span class=&#34;math inline&#34;&gt;\(f = f_1 \circ f_i \circ \cdots \ f_N\)&lt;/span&gt;, then it naturally follows that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\det\nabla f(\vec{x})      &amp;amp; = \prod_{i=1}^N{\det \nabla f_i(\vec{x}_i)} \\
\det\nabla f^{-1}(\vec{x}) &amp;amp; = \prod_{i=1}^N{\det \nabla f_i^{-1}(\vec{x}_i)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In order to make clear that the Jacobian is &lt;em&gt;not&lt;/em&gt; taken wrt the starting latent variables &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, we use the notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\vec{x}_i = f_{i-1}(\vec{x}_{i-1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;training-loss-optimisation-and-information-flow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training loss optimisation and information flow&lt;/h2&gt;
&lt;p&gt;Before moving into examples for normalising flows, we need to comment on the loss function optimisation. How do we determine the generative model’s parameters so that the generated distribution is as close as possible to the real distribution (or at least to the distribution of the samples drawn from that true distribution)?&lt;/p&gt;
&lt;p&gt;A standard way to do this is to calculate the Kullback-Leibler divergence between the two. Recall that the KL divergence &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{KL}(P \vert \vert Q)\)&lt;/span&gt; is &lt;em&gt;not&lt;/em&gt; a distance as it is not symmetric. I personally read &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{KL}(P \vert \vert Q)\)&lt;/span&gt; as “the loss of information on the true &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; if using the approximation &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;” as a way to keep the two distributions at their right place (writing &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{KL}(P_{true} \vert \vert Q_{est.})\)&lt;/span&gt; helps clarify the proper order).&lt;/p&gt;
&lt;p&gt;The KL divergence is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathbb{KL}(P_{true} \vert \vert Q_{est.}) = \mathbb{E}_{P_{true}(\vec{x})} \log \frac{P_{true}(\vec{x})}{Q_{est.}(\vec{x})}
\end{aligned}
\]&lt;/span&gt;
Or for a discrete distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathbb{KL}(P_{true} \vert \vert Q_{est}) &amp;amp; =  \sum_{\vec{x} \in X} P_{true}(\vec{x}) \log \frac{P_{true}(\vec{x})}{Q_{est}(\vec{x})} \\
                                          &amp;amp; =  \sum_{\vec{x} \in X} P_{true}(\vec{x}) \left[ \log P_{true}(\vec{x}) - \log Q_{est}(\vec{x}) \right] 
\end{aligned} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In our particular case, this becomes:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathbb{KL}(P_{true} \vert \vert P_Y) &amp;amp; = \sum_{\vec{x} \in X} {P_{true}(\vec{x}) \log \frac{P_{true}(\vec{x})}{P_Y(\vec{y})}} \\
                                      &amp;amp; = \sum_{\vec{x} \in X} {P_{true}(\vec{x}) \left[ \log P_{true}(\vec{x}) - \log P_Y(\vec{y}) \right] }
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;since:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P_Y(\vec{y}) &amp;amp; = P_X(\vec{x}) \left| det \nabla f^{-1}(\vec{y})  \right| \\
&amp;amp; = P_X(\vec{x}) \left| det\nabla f(\vec{x}) \right|^{-1}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We end up with:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{split}
\mathbb{KL}(P_{true} \vert \vert P_Y) &amp;amp; = \sum_{\vec{x} \in X} {P_{true}(\vec{x}) \left[ \log P_{true}(\vec{x})  - \log \left( P_X(\vec{x}) \left| det \nabla f(\vec{y})  \right|^{-1} \right) \right] }
\end{split}
\]&lt;/span&gt;
Minimising this divergence is achieved by changing the parameters which generate &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The divergence is one of many measures that can be used to measure the distance (in the loose sense of the word) between the true and generated distributions. But the KL divergence illustrates how logarithms of the probability distributions naturally appear. A common formulation of the loss is the Wasserstein distance. In the setting of the normalising flows (and VAEs), we have two transformations: the inference direction (the encoder) and the generative direction (the decoder). Given the back-and-forth nature, it makes sense to &lt;em&gt;not&lt;/em&gt; favour one direction over the other. Instead of using the KL divergence which is not symmetric, use the mutual information (this is equivalent to using free energy as in &lt;span class=&#34;citation&#34;&gt;(Rezende and Mohamed &lt;a href=&#34;#ref-rezendeVariationalInferenceNormalizing2016&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Regardless of the choice of loss function, it is obvious that optimising &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{KL}(P_{true} \vert \vert P_Y)\)&lt;/span&gt; cannot be contemplated without serious optimisations. Alternatively, finding more tractable alternative distance measurements is an active research topic.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-flows&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic flows&lt;/h2&gt;
&lt;p&gt;When normalizing flows were introduced by &lt;span class=&#34;citation&#34;&gt;(Rezende and Mohamed &lt;a href=&#34;#ref-rezendeVariationalInferenceNormalizing2016&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;, they experimented with simple transformations: a linear transformation (with a simple non-linear function) called &lt;em&gt;planar flows&lt;/em&gt; and flows within a space centered on a reference latent variable called &lt;em&gt;radial flows&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;planar-flows&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Planar Flows&lt;/h2&gt;
&lt;p&gt;A planar flow is formulated as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f_i(\vec{x}_i) = \vec{x}_i + \vec{u_i}  h(\vec{w}_i^\intercal \vec{x}_i + b_i)
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\vec{u}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\vec{w}_i\)&lt;/span&gt; are vectors, &lt;span class=&#34;math inline&#34;&gt;\(h()\)&lt;/span&gt; is a non-linear real function and &lt;span class=&#34;math inline&#34;&gt;\(b_i\)&lt;/span&gt; is a scalar.&lt;/p&gt;
&lt;p&gt;By defining:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\psi_i(\vec{z}) = h&amp;#39;(\vec{w}^\intercal \vec{z} + b_i) \vec{w}_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;the determinant required to normalize the flow can be simplified to (see original paper for the short steps involved):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left| \det \frac{\partial f_i}{\partial x_i}  \right| = \left| \det \left( \mathbb{I} + \vec{u_i} \psi_i(\vec{x}_i)^\intercal \right) \right| = \left| 1 + \vec{u_i}^\intercal \psi_i(\vec{x}_i)  \right|
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is a more tractable expression.&lt;/p&gt;
&lt;div id=&#34;radial-flows&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Radial flows&lt;/h3&gt;
&lt;p&gt;The formulation of the radial flows takes a reference hyper-ball centered at a reference point &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}_0\)&lt;/span&gt;. Any point &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; gets moved in the direction of &lt;span class=&#34;math inline&#34;&gt;\(\vec{x} - \vec{x}_0\)&lt;/span&gt;. That move is dependent on &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt;. In other words, imagine a plain hyper-ball, after many such transformations, you obtain a hyper-potato.&lt;/p&gt;
&lt;p&gt;The flows are defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f_i(\vec{x}_i) = \vec{x}_i + \beta_i h(\alpha_i, \rho_i) \left( \vec{x}_i - \vec{x}_0 \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; is a strictly positive scalar, &lt;span class=&#34;math inline&#34;&gt;\(\beta_i\)&lt;/span&gt; is a scalar, &lt;span class=&#34;math inline&#34;&gt;\(\rho_i = \left|| \vec{x}_i - \vec{x}_0 \right||\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(h(\alpha_i, \rho_i) = \frac{1}{\alpha_i + \rho_i}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This family of functions gives the following expression of the determinant:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left| \det \nabla f_i(\vec{x}_i) \right| = \left[ 1 + \beta_i h(\alpha_i, \rho_i) \right] ^{D-1} \left[ 1 + \beta_i h(\alpha_i, \rho_i) +  \beta_i \rho_i h&amp;#39;(\alpha_i, \rho_i) \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, this is a more tractable expression.&lt;/p&gt;
&lt;p&gt;Unfortunately, it was found that those transformations do not scale well to high-dimensional latent space.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;more-complex-flows&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;More complex flows&lt;/h2&gt;
&lt;div id=&#34;residual-flows-discrete-flows&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Residual flows (discrete flows)&lt;/h3&gt;
&lt;p&gt;Various proposals were initially put forward with common aims: replacing &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; by a series of sequentially composed simpler but expressive base functions and paying particular attention the computational costs. (refer &lt;span class=&#34;citation&#34;&gt;(Kobyzev, Prince, and Brubaker &lt;a href=&#34;#ref-kobyzevNormalizingFlowsIntroduction2020a&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(Papamakarios et al. &lt;a href=&#34;#ref-papamakariosNormalizingFlowsProbabilistic2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; for details).&lt;/p&gt;
&lt;p&gt;Residual flows &lt;span class=&#34;citation&#34;&gt;(He et al. &lt;a href=&#34;#ref-heDeepResidualLearning2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; were a key development. As the name suggests, the transformations mirror the neural networks RevNet structure. Explicitly, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is defined as &lt;span class=&#34;math inline&#34;&gt;\(f(x) = x + \phi(x)\)&lt;/span&gt;. The left-hand side identity term is a matrix where all the eigenvalues are 1 (duh). If &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt; represented a simple matrix multiplication, imposing the condition that all its eigenvalues of the righthand side term are strictly strictly between 0 and 1 ensure that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; remains invertible. An equivalent, and more general condition, is to impose that &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is Lipschitz-continuous with a constant below 1. That is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\forall x, y \qquad  0 &amp;lt; \left| \phi(x) - \phi(y) \right| &amp;lt; \left| x - y \right|
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\forall x, h&amp;gt;0 \qquad  0 &amp;lt; \frac{\left| \phi(x+h) - \phi(x) \right|}{h} &amp;lt; 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thanks to this condition, not only &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is invertible, but all the eigenvalues of &lt;span class=&#34;math inline&#34;&gt;\(\nabla f = \mathbb{I} + \nabla \phi(x)\)&lt;/span&gt; are strictly positive (adding a transformation with unity eigenvalues (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{I}\)&lt;/span&gt;) and a transformation with eigenvalues strictly below unity (in norm) cannot result in a transformation with nil eigenvalues). Therefore, we can be certain that $f = ( + (x)) = ( + (x)) $ (no negative eigenvalues).&lt;/p&gt;
&lt;p&gt;Recalling that &lt;span class=&#34;math inline&#34;&gt;\(det(e^A) = e^{tr(A)}\)&lt;/span&gt; and the Taylor expansion of &lt;span class=&#34;math inline&#34;&gt;\(\log\)&lt;/span&gt;, we obtain the following simplification:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\log \enspace \vert \det \nabla f \vert &amp;amp; = \log \enspace \det(\mathbb{I} + \nabla \phi) \\ 
                                       &amp;amp; = tr(\log (\mathbb{I} + \nabla \phi)) \\
\log \enspace \vert \det \nabla f \vert &amp;amp; = \sum_{k=1}^{\infty}{(-1)^{k+1} \frac{tr(\nabla \phi)^k}{k}}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Obviously a trace is much easier to calculate than a determinant. However, the expression now becomes an infinite series. One of the core result of the cited paper is an algorithm to limit the number of terms to calculate in this infinite series.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;other-versions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other versions&lt;/h2&gt;
&lt;p&gt;Table from Papamakorios&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;continuous-flows&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous flows&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;neural-ordinary-differential-equations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Neural ordinary differential equations&lt;/h1&gt;
&lt;p&gt;One of the Best Papers of NeurIPS 2018&lt;/p&gt;
&lt;div id=&#34;continuous-flows-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous flows&lt;/h2&gt;
&lt;div id=&#34;continuous-flows-means-no-crossover&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Continuous flows means no-crossover&lt;/h3&gt;
&lt;p&gt;Not the case for residual which have discrete steps.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;universal-ordinary-differential-equations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Universal ordinary differential equations&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;stochastic-differential-equations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stochastic differential equations&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;other&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other&lt;/h2&gt;
&lt;p&gt;Previously mentioned generative models can be improved with normalising flows&lt;/p&gt;
&lt;p&gt;Flow-GAN Grover, Dhan Ermon, Flow-GAN combining Mx Likelihood and adversarial learning and generative model&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;optimisation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Optimisation&lt;/h2&gt;
&lt;p&gt;Divergence / distance measures&lt;/p&gt;
&lt;p&gt;Kullback-Leibner / Jensen-Shannon divergence / Wasserstein distance&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references hanging-indent&#34;&gt;
&lt;div id=&#34;ref-FastLearningAlgorithm2006&#34;&gt;
&lt;p&gt;“A Fast Learning Algorithm for Deep Belief Nets.” 2006. &lt;em&gt;MIT Press Journals&lt;/em&gt;, Neural Computation,, May. &lt;a href=&#34;https://www.mitpressjournals.org/doi/10.1162/neco.2006.18.7.1527&#34;&gt;https://www.mitpressjournals.org/doi/10.1162/neco.2006.18.7.1527&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-chenNeuralOrdinaryDifferential2019&#34;&gt;
&lt;p&gt;Chen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations,” December. &lt;a href=&#34;http://arxiv.org/abs/1806.07366&#34;&gt;http://arxiv.org/abs/1806.07366&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-dinhNICENonlinearIndependent2015&#34;&gt;
&lt;p&gt;Dinh, Laurent, David Krueger, and Yoshua Bengio. 2015. “NICE: Non-Linear Independent Components Estimation,” April. &lt;a href=&#34;http://arxiv.org/abs/1410.8516&#34;&gt;http://arxiv.org/abs/1410.8516&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-GoodfellowDeepLearning2016&#34;&gt;
&lt;p&gt;Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. &lt;em&gt;Deep Learning&lt;/em&gt;. MIT Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-heDeepResidualLearning2015&#34;&gt;
&lt;p&gt;He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition,” December. &lt;a href=&#34;http://arxiv.org/abs/1512.03385&#34;&gt;http://arxiv.org/abs/1512.03385&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kingmaIntroductionVariationalAutoencoders2019&#34;&gt;
&lt;p&gt;Kingma, Diederik P., and Max Welling. 2019. “An Introduction to Variational Autoencoders.” &lt;em&gt;Foundations and Trends in Machine Learning&lt;/em&gt; 12 (4): 307–92. &lt;a href=&#34;https://doi.org/10/ggfm34&#34;&gt;https://doi.org/10/ggfm34&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kobyzevNormalizingFlowsIntroduction2020a&#34;&gt;
&lt;p&gt;Kobyzev, Ivan, Simon J. D. Prince, and Marcus A. Brubaker. 2020. “Normalizing Flows: An Introduction and Review of Current Methods.” &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/em&gt;, 1–1. &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2020.2992934&#34;&gt;https://doi.org/10.1109/TPAMI.2020.2992934&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lucasUnderstandingPosteriorCollapse2019&#34;&gt;
&lt;p&gt;Lucas, James, George Tucker, Roger Grosse, and Mohammad Norouzi. 2019a. “Understanding Posterior Collapse in Generative Latent Variable Models | Semantic Scholar.” In &lt;em&gt;DeepGenStruct Worshop&lt;/em&gt;. &lt;a href=&#34;https://www.semanticscholar.org/paper/Understanding-Posterior-Collapse-in-Generative-Lucas-Tucker/7e2f5af5d44890c08ef72a5070340e0ffd3643ea&#34;&gt;https://www.semanticscholar.org/paper/Understanding-Posterior-Collapse-in-Generative-Lucas-Tucker/7e2f5af5d44890c08ef72a5070340e0ffd3643ea&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lucasDonBlameELBO2019&#34;&gt;
&lt;p&gt;———. 2019b. “Don’t Blame the ELBO! A Linear VAE Perspective on Posterior Collapse,” November. &lt;a href=&#34;http://arxiv.org/abs/1911.02469&#34;&gt;http://arxiv.org/abs/1911.02469&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-papamakariosNormalizingFlowsProbabilistic2019&#34;&gt;
&lt;p&gt;Papamakarios, George, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. 2019. “Normalizing Flows for Probabilistic Modeling and Inference,” December. &lt;a href=&#34;http://arxiv.org/abs/1912.02762&#34;&gt;http://arxiv.org/abs/1912.02762&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rezendeVariationalInferenceNormalizing2016&#34;&gt;
&lt;p&gt;Rezende, Danilo Jimenez, and Shakir Mohamed. 2016. “Variational Inference with Normalizing Flows,” June. &lt;a href=&#34;http://arxiv.org/abs/1505.05770&#34;&gt;http://arxiv.org/abs/1505.05770&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-russellArtificialIntelligenceModern2020&#34;&gt;
&lt;p&gt;Russell, Stuart, and Peter Norvig. 2020. &lt;em&gt;Artificial Intelligence: A Modern Approach&lt;/em&gt;. 4th ed. Pearson Series on Artificial Intelligence. Pearson. &lt;a href=&#34;http://aima.cs.berkeley.edu/&#34;&gt;http://aima.cs.berkeley.edu/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-theodoridisMachineLearningBayesian2020&#34;&gt;
&lt;p&gt;Theodoridis, Sergios. 2020. &lt;em&gt;Machine Learning: A Bayesian and Optimization Perspective&lt;/em&gt;. Amsterdam Boston Heidelberg London New York Oxford Paris San Diego San Francisco Singapore Sydney Tokyo: Elsevier, AP. &lt;a href=&#34;https://doi.org/10.1016/C2019-0-03772-7&#34;&gt;https://doi.org/10.1016/C2019-0-03772-7&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix-appendix&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;more-information&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;More information&lt;/h1&gt;
&lt;p&gt;This will be Appendix A.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-more-thing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;One more thing&lt;/h1&gt;
&lt;p&gt;This will be Appendix B.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Piece of trivia: It seems that this is pronounced Counta-tch instead of counta-sh.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Incidentally, this observation is made in the last sentence of the last paragraph of the last chapter of the &lt;a href=&#34;https://www.deeplearningbook.org/&#34;&gt;Deep Learning Book&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(Goodfellow, Bengio, and Courville &lt;a href=&#34;#ref-GoodfellowDeepLearning2016&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Presentation at the Kong Kong Machine Learning meetup</title>
      <link>/2020/04/30/presentation-at-the-hong-kong-machine-learning-meetup.html</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/2020/04/30/presentation-at-the-hong-kong-machine-learning-meetup.html</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I recently made a presentation at the regular &lt;a href=&#34;https://www.meetup.com/Hong-Kong-Machine-Learning-Meetup&#34;&gt;Hong Kong Machine Learning meetup&lt;/a&gt; organised by &lt;a href=&#34;https://gmarti.gitlab.io/&#34;&gt;Gautier Marti&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The presentation was an introduction to &lt;a href=&#34;https://julialang.org/&#34;&gt;Julia&lt;/a&gt; and used as an example a &lt;a href=&#34;https://github.com/Emmanuel-R8/COVID-19-Julia&#34;&gt;SEIR model COVID-19&lt;/a&gt; I had written. The presentation is available on &lt;a href=&#34;https://github.com/Emmanuel-R8/Presentation_HKML_2020_04/raw/master/HKML_Julia_Xarrigan_2020_04_29.pdf&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It seems to have had some &lt;a href=&#34;https://www.linkedin.com/posts/hong-kong-machine-learning_bye-bye-python-hello-julia-activity-6663079161676075009-rWik&#34;&gt;effect&lt;/a&gt;!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Forecasting the progression of COVID-19</title>
      <link>/2020/03/25/2020-03-25-forecasting-covid-19.html</link>
      <pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/2020/03/25/2020-03-25-forecasting-covid-19.html</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-neherlab-covid-19-forecast-model&#34;&gt;The &lt;span&gt;Neherlab COVID-19&lt;/span&gt; forecast model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#basic-assumptions&#34;&gt;Basic assumptions&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#age-cohorts&#34;&gt;Age cohorts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#severity&#34;&gt;Severity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#seasonality&#34;&gt;Seasonality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transmission-reduction&#34;&gt;Transmission reduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#details-of-the-model&#34;&gt;Details of the model&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#population-compartments&#34;&gt;Population compartments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-parameters&#34;&gt;Model parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#infection&#34;&gt;Infection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#after-infection&#34;&gt;After infection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#load-data&#34;&gt;Load data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#initialise-parameters&#34;&gt;Initialise parameters&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#fixed-constants&#34;&gt;Fixed constants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#infrastructure&#34;&gt;Infrastructure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameter-vector&#34;&gt;Parameter vector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#population&#34;&gt;Population&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameters-vector&#34;&gt;Parameters vector&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#differential-equation-solver&#34;&gt;Differential equation solver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bilibliography&#34;&gt;Bilibliography&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;the-neherlab-covid-19-forecast-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The &lt;a href=&#34;https://neherlab.org/covid19/&#34;&gt;Neherlab COVID-19&lt;/a&gt; forecast model&lt;/h1&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;using CSV, Dates;
using DataFrames, DataFramesMeta;
using Plots, PyPlot;
using DifferentialEquations;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is more a data science post than machine learning. It was born after reading a &lt;a href=&#34;https://www.imperial.ac.uk/media/imperial-college/medicine/sph/ide/gida-fellowships/Imperial-College-COVID19-NPI-modelling-16-03-2020.pdf&#34;&gt;report&lt;/a&gt; from Imperial College London and finding a forecasting model by &lt;a href=&#34;https://neherlab.org/covid19/&#34;&gt;NeherLab&lt;/a&gt;. The numbers produced by those models can only be described as terrifying.&lt;/p&gt;
&lt;p&gt;How do those models work? How are they calibrated?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BUT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Remember that whatever concerns one can have about their precision, those models are all absolutely clear that social-distancing, quarantining have a massive impact on death rates. Being careful saves lives. If anybody feels like ignoring those precautions out of excess testosterone, they are at risk of killing others.&lt;/p&gt;
&lt;p&gt;This post started from one of the pages of the NeherLab site describing their methodology. The work that team is achieving deserves more credit than I can give them.&lt;/p&gt;
&lt;p&gt;The NeherLab website, including the model, is entirely written in Javascript. This is difficul to understand and audit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-assumptions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Basic assumptions&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: This is not an introduction to SEIR (and variant) compartment modelling of epidemies. For an introduction (difficult to avoid the maths), see a presentation by the &lt;a href=&#34;http://indico.ictp.it/event/7960/session/3/contribution/19/material/slides/0.pdf&#34;&gt;Swiss Tropical and Public Health Institute&lt;/a&gt;. &lt;a href=&#34;https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology&#34;&gt;Wikipedia&lt;/a&gt; is always an option.&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;The model works as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;susceptible individuals are exposed/infected through contact with infectious individuals. Each infectious individual causes on average &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt; secondary infections while they are infectious.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Transmissibility of the virus could have seasonal variation which is parameterized with the parameter “seasonal forcing” (amplitude) and “peak month” (month of most efficient transmission).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;exposed individuals progress to a symptomatic/infectious state after an average latency&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;infectious individuals recover or progress to severe disease. The ratio of recovery to severe progression depends on age&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;severely sick individuals either recover or deteriorate and turn critical. Again, this depends on the age&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;critically ill individuals either return to regular hospital or die. Again, this depends on the age&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The individual parameters of the model can be changed to allow exploration of different scenarios.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;age-cohorts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Age cohorts&lt;/h3&gt;
&lt;p&gt;COVID-19 is much more severe in the elderly and proportion of elderly in a community is therefore an important determinant of the overall burden on the health care system and the death toll. We collected age distributions for many countries from data provided by the UN and make those available as input parameters. Furthermore, we use data provided by the epidemiology group by the &lt;a href=&#34;http://weekly.chinacdc.cn/en/article/id/e53946e2-c6c4-41e9-9a9b-fea8db1a8f51&#34;&gt;Chinese CDC&lt;/a&gt; to estimate the fraction of severe and fatal cases by age group.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;severity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Severity&lt;/h3&gt;
&lt;p&gt;The basic model deals with 3 levels of severity: slow, moderate and fast transmissions.&lt;/p&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# severityLevel = :slow;
severityLevel = :moderate;
# severityLevel = :fast;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;seasonality&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Seasonality&lt;/h2&gt;
&lt;p&gt;Many respiratory viruses such as influenza, common cold viruses (including other coronaviruses) have a pronounced seasonal variation in incidence which is in part driven by climate variation through the year. We model this seasonal variation using a sinusoidal function with an annual period. This is a simplistic way to capture seasonality. Furthermore, we don’t know yet how seasonality will affect COVID-19 transmission.&lt;/p&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# Northern or southern hemisphere
latitude = :north;
# latitude = :tropical;
# latitude = :south;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# The time unit is days (as floating point)
# Day 0 is taken at 1 March 2020
BASE_DATE = Date(2020, 3, 1);
BASE_DAYS = 0;

function date2days(d) 
    return convert(Float64, datetime2rata(d) - datetime2rata(BASE_DATE))
end;

function days2date(d) 
    return BASE_DATE + Day(d)
end;    &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# Default values for R_0
baseR₀ = Dict( (:north,    :slow)     =&amp;gt; 2.2, 
               (:north,    :moderate) =&amp;gt; 2.7, 
               (:north,    :fast)     =&amp;gt; 3.2, 
               (:tropical, :slow)     =&amp;gt; 2.0, 
               (:tropical, :moderate) =&amp;gt; 2.5, 
               (:tropical, :fast)     =&amp;gt; 3.0,
               (:south,    :slow)     =&amp;gt; 2.2, 
               (:south,    :moderate) =&amp;gt; 2.7, 
               (:south,    :fast)     =&amp;gt; 3.2);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# Peak date
peakDate = Dict( :north     =&amp;gt; date2days(Date(2020, 1, 1)), 
                 :tropical  =&amp;gt; date2days(Date(2020, 1, 1)),    # although no impact
                 :south     =&amp;gt; date2days(Date(2020, 7, 1)));&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# Seasonal forcing parameter \epsilon
ϵ = Dict( (:north,    :slow)     =&amp;gt; 0.2, 
          (:north,    :moderate) =&amp;gt; 0.2, 
          (:north,    :fast)     =&amp;gt; 0.1, 
          (:tropical, :slow)     =&amp;gt; 0.0, 
          (:tropical, :moderate) =&amp;gt; 0.0, 
          (:tropical, :fast)     =&amp;gt; 0.0,
          (:south,    :slow)     =&amp;gt; 0.2, 
          (:south,    :moderate) =&amp;gt; 0.2, 
          (:south,    :fast)     =&amp;gt; 0.1);&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# Gives R_0 at a given date
function R₀(d; r_0 = missing, latitude = :north, severity = :moderate)
    if ismissing(r_0)
        r₀ = baseR₀[(latitude, severity)]
    else
        r₀ = r_0
    end
    eps = ϵ[(latitude, severity)]
    peak = peakDate[latitude]
    
    return r₀ * (1 + eps * cos(2.0 * π * (d - peak) / 365.25))
end;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;transmission-reduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transmission reduction&lt;/h2&gt;
&lt;p&gt;The tool allows one to explore temporal variation in the reduction of transmission by infection
control measures. This is implemented as a curve through time that can be dragged by the mouse to
modify the assumed transmission. The curve is read out and used to change the transmission relative
to the base line parameters for &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt; and seasonality. Several studies attempt to estimate the
effect of different aspects of social distancing and infection control on the rate of transmission.
A report by &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.03.03.20030593v1&#34;&gt;Wang et al&lt;/a&gt; estimates a
step-wise reduction of &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt; from above three to around 1 and then to around 0.3 due to successive
measures implemented in Wuhan. &lt;a href=&#34;https://www.pnas.org/content/116/27/13174&#34;&gt;This study&lt;/a&gt; investigates
the effect of school closures on influenza transmission.&lt;/p&gt;
&lt;p&gt;This curve is presented as a list of tuples: (days from start date, ratio). The month starts from the start date. Between dates, the ration is interpolated linearly. After the last date, the ration remains constant.&lt;/p&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;startDate = date2days(Date(2020, 3, 1));

mitigationRatio = [(0, 1.00), (30, 0.80), (60, 0.20), (150, 0.50)];

function getCurrentRatio(d; start = BASE_DAYS, schedule = mitigationRatio)
    l = length(schedule)
    
    # If l = 1, ratio will be the only one
    if l == 1 
        return schedule[1][2]
    else
        for i in 2:l
            d1 = schedule[i-1][1]
            d2 = schedule[i  ][1]
            
            if d &amp;lt; d2 
                deltaR = schedule[i][2] - schedule[i-1][2]
                return schedule[i-1][2] + deltaR * (d - d1) / (d2 - d1)
            end
        end
    
        # Last possible choice
        return schedule[l][2]
    end
end;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;details-of-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Details of the model&lt;/h2&gt;
&lt;p&gt;Age strongly influences an individual’s response to the virus. The general population is sub-divided in to age classes, indexed by &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, to allow for variable transition rates dependent upon age.&lt;/p&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# The population will be modeled as a single vector. 
# The vector will be a stack of several vectors, each of them represents a compartment.
# Each compartment vector has a size $nAgeGroup$ representing each age group.
# The compartments are: S, E, I, H, C, R, D, K, L

# We also track the hospital bed usage BED and ICU

# Population to compartments
function Pop2Comp(P)
    
    # To make copy/paste less prone to error 
    g = 0
    
    S = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1
    E = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1
    I = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1
    J = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1
    H = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1
    C = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1
    R = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1
    D = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1
    K = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1
    L = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1
    
    BED = P[ g*nAgeGroup + 1: g*nAgeGroup + 1]
    ICU = P[ g*nAgeGroup + 2: g*nAgeGroup + 2]
    
    return S, E, I, J, H, C, R, D, K, L, BED, ICU
end;&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;population-compartments&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Population compartments&lt;/h3&gt;
&lt;p&gt;Qualitatively, the epidemy model dynamics tracks several sub-groups (compartments):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/media/post/2020-COVID/images/States.svg&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Susceptible individuals (&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;) are healthy and susceptible to being exposed to the virus by contact with an infected individual.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Exposed individuals (&lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;) are infected but asymptomatic. They progress towards a symptomatic state on average time &lt;span class=&#34;math inline&#34;&gt;\(t_l\)&lt;/span&gt;. Reports are that asymptomatic individuals are contagious. We will assume that they are proportionally less contagious than symptomatic individuals as a percentage &lt;span class=&#34;math inline&#34;&gt;\(\gamma_E\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt;. For the purposes of modelling we will assume (without supporting evidence, but will be the object of parameter estimation):&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;γₑ = 0.50;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Infected individuals (&lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;) infect an average of &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt; secondary infections. On a time-scale of &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt;, infected individuals either recover or progress towards severe infection.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From here on, the compartments differ from the NeherLab model is that we split compartments depending on the severity of the symptoms (Severe or Critical) and the location of the individual (out of the hospital infrastructure, isolated in hospital, or isolated in intensive care units). The transitions reflect the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Transition between locations is purely a function of bed availability: as soon as beds are available, they are filled by all age groups in their respective proportions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Transition from severe to critical is assumed to be independent from the location of the patient. For severe patients, the relevance of the location is whether they are isolated or not, that is the possibility to infect susceptible individual. The same way an asymptomatic individual’s attracts a ratio &lt;span class=&#34;math inline&#34;&gt;\(\gamma_e\)&lt;/span&gt;, the other compartments will. The transition from &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; to recovery or criticality has a time-scale of &lt;span class=&#34;math inline&#34;&gt;\(t_h\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# R_0 multipliers depending on severity. Subscript matches the compartment&amp;#39;s name.
# Infected / symptomatic individuals
γᵢ=1.0;

# Severe symptoms
γⱼ=1.0;

# Critical symptoms
γₖ = 2.0;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Once critical, the location of a patient influences their chances of recovery. Although we will assume that the time to recovery is identical in all cases, we will assume that the risks will double and triple if a patient is in simple isolation (receiving care but without ICU equipmment) or out of hospital.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# Fatality mulitplier.

# In ICU
δᵤ = 1.0;

# In hospital
δₗ = 2.0;

# Out of hospital
δₖ = 3.0;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The time-scale to recovery (&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;) or death (&lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;) is &lt;span class=&#34;math inline&#34;&gt;\(t_u\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Recovering and recovered individuals [&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;] can not be infected again. We will assume that recovering individual are not contagious (no medical experience for this assumption for recovering individual).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-parameters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model parameters&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Many estimates of &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt; are in the &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7001239/&#34;&gt;range of 2-3&lt;/a&gt; with some estimates pointing to considerably &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.02.10.20021675v1&#34;&gt;higher values&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The serial interval, that is the time between subsequent infections in a transmission chain, was &lt;a href=&#34;https://www.nejm.org/doi/full/10.1056/NEJMoa2001316&#34;&gt;estimated to be 7-8 days&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The China CDC compiled &lt;a href=&#34;http://weekly.chinacdc.cn/en/article/id/e53946e2-c6c4-41e9-9a9b-fea8db1a8f51&#34;&gt;extensive data on severity and fatality of more than 40 thousand confirmed cases&lt;/a&gt;.
In addition, we assume that a substantial fraction of infections, especially in the young, go unreported. This is encoded in the columns “Confirmed [% of total]”.&lt;/li&gt;
&lt;li&gt;Seasonal variation in transmission is common for many respiratory viruses but the strength of seasonal forcing for COVID19 are uncertain. For more information, see a &lt;a href=&#34;https://smw.ch/article/doi/smw.2020.20224&#34;&gt;study by us&lt;/a&gt; and by &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.03.04.20031112v1&#34;&gt;Kissler et al&lt;/a&gt;.
The parameters of this model fall into three categories: transition time scales, age-specfic parameters and a time-dependent infection rate.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;transition-time-scales&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Transition time scales&lt;/h4&gt;
&lt;p&gt;The time scales of transition from a compartment to the next: &lt;span class=&#34;math inline&#34;&gt;\(t_l\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(t_h\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(t_c\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(t_l\)&lt;/span&gt;: latency time from infection to infectiousness&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt;: the time an individual is infectious after which he/she either recovers or falls severely ill&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(t_h\)&lt;/span&gt;: the time a sick person recovers or deteriorates into a critical state&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(t_u\)&lt;/span&gt;: the time a person remains critical before dying or stabilizing (Neherlab uses &lt;span class=&#34;math inline&#34;&gt;\(t_c\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(t_u\)&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# Time to infectiousness (written t\_l)
tₗ = Dict(  :slow     =&amp;gt; 5.0, 
            :moderate =&amp;gt; 5.0, 
            :fast     =&amp;gt; 4.0);

# Time to infectiousness (written t\_i)
tᵢ = Dict(  :slow     =&amp;gt; 3.0, 
            :moderate =&amp;gt; 3.0, 
            :fast     =&amp;gt; 3.0);

# Time in hospital bed (not ICU)
tₕ = 4.0;

# Time in ICU 
tᵤ = 14.0;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;age-specfic-parameters&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Age-specfic parameters&lt;/h4&gt;
&lt;p&gt;The age-specific parameters &lt;span class=&#34;math inline&#34;&gt;\(z_a\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(m_a\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(c_a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f_a\)&lt;/span&gt; that determine relative rates of different outcomes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_a\)&lt;/span&gt;: a set of numbers reflecting to which extent an age group is susceptible to initial contagion. Note that NeherLab denotes this vector by &lt;span class=&#34;math inline&#34;&gt;\(I_a\)&lt;/span&gt; which is confusing with the compartmment evolution &lt;span class=&#34;math inline&#34;&gt;\(I_a(t)\)&lt;/span&gt; notation. (This sort of defeats the purpose of &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt;.)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(m_a\)&lt;/span&gt;: fraction of infectious becoming severe (&lt;strong&gt;Hospitalisation Rate&lt;/strong&gt;) or recovers immediately (&lt;strong&gt;Recovery Rate&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(c_a\)&lt;/span&gt;: fraction of severe cases that turn critical (&lt;strong&gt;Critical Rate&lt;/strong&gt;) or can leave hospital (&lt;strong&gt;Discharge Rate&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(f_a\)&lt;/span&gt;: fraction of critical cases that are fatal (&lt;strong&gt;Death Rate&lt;/strong&gt;) or recover (&lt;strong&gt;Stabilisation Rate&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;AgeGroup = [&amp;quot;0-9&amp;quot;, &amp;quot;10-19&amp;quot;, &amp;quot;20-29&amp;quot;, &amp;quot;30-39&amp;quot;, &amp;quot;40-49&amp;quot;, &amp;quot;50-59&amp;quot;, &amp;quot;60-69&amp;quot;, &amp;quot;70-79&amp;quot;, &amp;quot;80+&amp;quot;];
zₐ =       [0.05,   0.05,   0.10,    0.15,    0.20,    0.25,    0.30,    0.40,    0.50];
mₐ =       [0.01,   0.03,   0.03,    0.03,    0.06,    0.10,    0.25,    0.35,    0.50];
cₐ =       [0.05,   0.10,   0.10,    0.15,    0.20,    0.25,    0.35,    0.45,    0.55];
fₐ =       [0.30,   0.30,   0.30,    0.30,    0.30,    0.40,    0.40,    0.50,    0.50];

nAgeGroup = length(AgeGroup);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;infrastruture&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Infrastruture&lt;/h4&gt;
&lt;p&gt;The number of beds available is assumed as a fixed resource in time. The number of hospital (resp. ICU) beds in use will be denoted &lt;span class=&#34;math inline&#34;&gt;\(\mathscr{H}(t)\)&lt;/span&gt; (resp. &lt;span class=&#34;math inline&#34;&gt;\(\mathscr{U}(t)\)&lt;/span&gt;) up to a maximum of &lt;span class=&#34;math inline&#34;&gt;\(\mathscr{H}_{max}\)&lt;/span&gt; (resp. &lt;span class=&#34;math inline&#34;&gt;\(\mathscr{U}_{max}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Although the initial infections took place via dommestic and international travellers (apart from the initial infections in Wuhan obviously), we will assume no net flow of population in and out of a country of interest.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;infection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Infection&lt;/h3&gt;
&lt;div id=&#34;susceptible&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Susceptible:&lt;/h4&gt;
&lt;p&gt;The &lt;em&gt;base&lt;/em&gt; rate of contagion is denoted as &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt;. The actual rate varies with time (to reflect seasons and impact of temperature on virus resilience) and the effectiveness of the mitigation measures such as social distancing. Separately, each age group will have a different sensitivity to infection.&lt;/p&gt;
&lt;p&gt;The infection rate &lt;span class=&#34;math inline&#34;&gt;\(\beta_a(t)\)&lt;/span&gt; is age- and time-dependent. It is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\beta_a(t) = z_a M(t) R_0 \left( 1+\varepsilon \cos \left( 2\pi \frac{t-t_{max}}{t_i} \right) \right) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(z_a\)&lt;/span&gt; is the degree to which particular age groups are sensitive to initial infection. It reflects bioligical sensitivity and to which degree it is isolated from the rest of the population (denoted &lt;span class=&#34;math inline&#34;&gt;\(I_a\)&lt;/span&gt; in NeherLab).&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(M(t)\)&lt;/span&gt; is a time-dependent ratio reflecting the effectiveness of mitigation measures.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is the amplitude of seasonal variation in transmissibility.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(t_{max}\)&lt;/span&gt; is the time of the year of peak transmission.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Susceptible individuals are exposed to a number of contagious individuals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;asymptomatic infected: &lt;span class=&#34;math display&#34;&gt;\[\gamma_e \beta_a(t) E_a(t)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;symptomatic infected: &lt;span class=&#34;math display&#34;&gt;\[\gamma_i \beta_a(t) I_a(t)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;severe not in hospital: &lt;span class=&#34;math display&#34;&gt;\[\gamma_j \beta_a(t) J_a(t)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;critical not in hospital: &lt;span class=&#34;math display&#34;&gt;\[\gamma_k \beta_a(t) K_a(t)\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The sum of those will be a flow from susceptible (&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;) to (&lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;) exposed individuals.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
S2E_a(t) &amp;amp; = \gamma_e \beta_a(t) E_a(t) + \gamma_i \beta_a(t) I_a(t) + \gamma_j \beta_a(t) K_a(t) + \gamma_k \beta_a(t) L_a(t) \\ 
S2E_a(t) &amp;amp; = \beta_a(t) \left( \gamma_e  E_a(t) + \gamma_i I_a(t) + \gamma_j J_a(t) + \gamma_k K_a(t) \right) \\
E2S_a(t) &amp;amp; = -S2E_a(t) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{dS_{a}(t)}{dt} = - S2E_a(t) = E2S_a(t)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;after-infection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;After infection&lt;/h3&gt;
&lt;p&gt;Quantitatively, the model expresses how many individuals transfer from one situation/compartment to another. Flows from compartment X to Y are written as &lt;span class=&#34;math inline&#34;&gt;\(X2Y\)&lt;/span&gt; (obviously &lt;span class=&#34;math inline&#34;&gt;\(X2Y = - Y2X\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Note that the compartments are split into age groups.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/media/post/2020-COVID/images/Transitions.svg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Transitions between compartments&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;epidemiology&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Epidemiology&lt;/h4&gt;
&lt;p&gt;Instead of expressing the sum of the flows at each node, it is easier to express the arrows, and summing them afterwards. For example, arrow from &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; will be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[JK_a(t) = \frac{c_a}{t_h} J_a(t)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with a positive flow following the direction of the arrow.&lt;/p&gt;
&lt;p&gt;In {julia}, this will become:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;        JK = cₐ .* J / tₕ&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;J&lt;/code&gt; is a vector representing an age group, &lt;code&gt;.*&lt;/code&gt; is the element-wise multiplication.&lt;/p&gt;
&lt;p&gt;After defining the arrows &lt;span class=&#34;math inline&#34;&gt;\(IJ\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(JK\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(JH\)&lt;/span&gt;, the change in &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; will simply be:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;        dJ = IJ - JK - JH&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bed-transfers&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Bed transfers&lt;/h4&gt;
&lt;p&gt;Individuals are transferred into hospital beds then into ICU beds in the order indicated by the red numbers.&lt;/p&gt;
&lt;p&gt;Critical patients already in hospital go into ICU as spots become available. The freed bed are first made available to critical patients out of hospital (&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;). Then, any free beds will receive patients in severe condition.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;safeguards&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Safeguards:&lt;/h4&gt;
&lt;p&gt;Note the need to ensure a few common sense rules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;No compartment can have a negative number of people.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The total population figure should remain unchanged. This is done by adjusting the number of susceptible individuals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Careful accounting of the use of fixed number of hospital beds.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The number of infected people should always be above the number of reported cases.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# Helper function to never change the number of individuals in a compartment in a way that would 
# make it below 0.1 (to avoid rounding errors around 0)
function ensurePositive(d,s)
    return max.(d .+ s, 0.1) .- s
end;

    
    
# The dynamics of the epidemy is a function that mutates its argument with a precise signature
# Don&amp;#39;t pay too much attetion to the print debugs/

function epiDynamics!(dP, P, params, t)
    
    S, E, I, J, H, C, R, D, K, L, BED, ICU = Pop2Comp(P)
    
    BED = BED[1]
    ICU = ICU[1]
    
    r₀, tₗ, tᵢ, tₕ, tᵤ, γₑ, γᵢ, γⱼ, γₖ, δₖ, δₗ, δᵤ, startDays = params 
    
    
    ####################################
    # Arrows reflecting epidemiology - Check signs (just in case)
    EI = ones(nAgeGroup) .* E / tₗ;  EI = max.(EI, 0.0); IE = -EI; 
    IJ = mₐ              .* I / tᵢ;  IJ = max.(IJ, 0.0); JI = -IJ
    JK = cₐ              .* J / tₕ;  JK = max.(JK, 0.0); KJ = -JK
    HL = cₐ              .* H / tₕ;  HL = max.(HL, 0.0); LH = -HL
    
    # Recovery arrows
    IR = (1 .- mₐ)       .* I / tᵢ;  IR = max.(IR, 0.0); RI = -IR
    JR = (1 .- cₐ)       .* J / tₕ;  JR = max.(JR, 0.0); RJ = -JR
    HR = (1 .- cₐ)       .* H / tₕ;  HR = max.(HR, 0.0); RH = -HR
    KR = (1 .- δₖ .* fₐ) .* K / tᵤ;  KR = max.(KR, 0.0); RK = -KR
    LR = (1 .- δₗ .* fₐ) .* L / tᵤ;  LR = max.(LR, 0.0); RL = -LR
    CR = (1 .- δᵤ .* fₐ) .* C / tᵤ;  CR = max.(CR, 0.0); RC = -CR
    
    # Deaths
    KD = δₖ .* fₐ        .* K / tᵤ;  KD = max.(KD, 0.0); DK = -KD
    LD = δₗ .* fₐ        .* L / tᵤ;  LD = max.(LD, 0.0); DL = -LD
    CD = δᵤ .* fₐ        .* C / tᵤ;  CD = max.(CD, 0.0); DC = -CD
    
    
    ####################################
    # Bed transfers
    
    ####### Step 1:
    # Decrease in bed usage is (recall that CD and CR are vectors over the age groups) 
    dICU = - (sum(CD) + sum(CR));                 dICU = ensurePositive(dICU, ICU)
    
    # ICU beds available
    ICU_free = ICU_max - (ICU + dICU)
    
    # Move as many patients as possible from $L$ to $C$ in proportion of each group
    ICU_transfer = min(sum(L), ICU_free)
    LC = ICU_transfer / sum(L) .* L;    CL = -LC
    
    # Overall change in ICU bed becomes
    dICU = dICU + ICU_transfer;                   dICU = ensurePositive(dICU, ICU)
    
    # And some normal beds are freed
    dBED = -ICU_transfer;                         dBED = ensurePositive(dBED, BED)
    #print(&amp;quot; dBed step 1 &amp;quot;); println(floor.(sum(dBED)))

    ####### Step 2:
    # Beds available
    BED_free = BED_max - (BED + dBED)
    
    # Move as many patients as possible from $K$ to $L$ in proportion of each group
    BED_transfer = min(sum(K), BED_free)
    KL = BED_transfer / sum(K) .* K;   LK = -KL
    
    # Overall change in normal bed becomes
    dBED = dBED + BED_transfer;                   dBED = ensurePositive(dBED, BED)
    #print(&amp;quot; dBed step 2 &amp;quot;); println(floor.(sum(dBED)))
    

    ####### Step 3:
    # Beds available
    BED_free = BED_max - (BED + dBED)
    
    # Move as many patients as possible from $J$ to $H$ in proportion of each group
    BED_transfer = min(sum(J), BED_free)
    JH = BED_transfer / sum(J) .* J;   HJ = -JH 
    
    # Overall change in ICU bed becomes
    dBED = dBED + BED_transfer;                   dBED = ensurePositive(dBED, BED)
    #print(&amp;quot; dBed step 3 &amp;quot;); println(floor.(sum(dBED)))
    

    ####################################
    # Sum of all flows + Check never negative compartment
    
    # Susceptible    
    # Calculation of β
    β = getCurrentRatio(t; start = BASE_DAYS, schedule = mitigationRatio) .* zₐ .* 
        R₀(t; r_0 = r₀, latitude = Latitude, severity = SeverityLevel)
    
    #print(&amp;quot;r₀&amp;quot;); println(r₀); println(&amp;quot;R₀&amp;quot;); 
    #println(R₀(t; r_0 = r₀, latitude = Latitude, severity = SeverityLevel)); print()
    
    dS = -β .* (γₑ.*E + γᵢ.*I + γⱼ.*J + γₖ.*K);   dS = min.(-0.01, dS); dS = ensurePositive(dS, S)
    
    #print(&amp;quot;dS&amp;quot;); println(floor.(dS)); println(); 
    
    # Exposed
    dE = -dS + IE;                                dE = ensurePositive(dE, E)
    
    # Infected. 
    dI = EI + JI + RI;                            dI = ensurePositive(dI, I)
    
    # Infected no hospital
    dJ = IJ + HJ + KJ + RJ;                       dJ = ensurePositive(dJ, J)
    
    #print(&amp;quot;I &amp;quot;); println(floor.(IJ)); print(&amp;quot;H &amp;quot;); println(floor.(HJ))
    #print(&amp;quot;K &amp;quot;); println(floor.(KJ)); print(&amp;quot;R &amp;quot;); println(floor.(RJ))
    
    # Infected in hospital
    dH = JH + LH + RH ;                           dH = ensurePositive(dH, H)
    
    # Critical no hospital
    dK = JK + LK + DK + RK;                       dK = ensurePositive(dK, K)
    
    # Critical in hospital
    dL = KL + HL + CL + DL + RL;                  dL = ensurePositive(dL, L)
    
    # Critical in ICU
    dC = LC + DC + RC;                            dC = ensurePositive(dC, C)
    
    # Recovery (can only increase)
    dR = IR + JR + HR + KR + LR + CR;             dR = max.(dR, 0.01)
    
    # Dead (can only increase)
    dD = KD + LD + CD;                            dD = max.(dD, 0.01)
    
    # Vector change of population and update in place
    result = vcat(dS, dE, dI, dJ, dH, dC, dR, dD, dK, dL, [dBED], [dICU])
    #print(&amp;quot; dS &amp;quot;); print(floor.(sum(dS))); print(&amp;quot; dE &amp;quot;); print(floor.(sum(dE))); 
    #print(&amp;quot; dI &amp;quot;); print(floor.(sum(dI))); print(&amp;quot; dJ &amp;quot;); println(floor.(sum(dJ))); 
    #print(&amp;quot; dH &amp;quot;); print(floor.(sum(dH))); print(&amp;quot; dC &amp;quot;); print(floor.(sum(dC))); 
    #print(&amp;quot; dR &amp;quot;); print(floor.(sum(dR))); print(&amp;quot; dD &amp;quot;); print(floor.(sum(dD))); 
    #print(&amp;quot; dK &amp;quot;); print(floor.(sum(dK))); print(&amp;quot; dL &amp;quot;); println(floor.(sum(dL))); println(); 
    for i = 1:length(result)
        dP[i] = result[i]
    end

end;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;load-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Load data&lt;/h1&gt;
&lt;p&gt;The data comes from Neherlab’s data repository on &lt;a href=&#34;https://github.com/neherlab/covid19_scenarios_data&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will use Italy as an example&lt;/p&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;country = &amp;quot;Italy&amp;quot;;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This file contains a record of cases day by day.&lt;/p&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;cases = DataFrame(CSV.read(&amp;quot;data/World.tsv&amp;quot;, header = 4));
cases = @where(cases, occursin.(country, :location));
sort!(cases, :time);

# Add a time column in the same format as the other dataframes
cases = hcat(DataFrame(t = date2days.(cases[:, :time])), cases);

# Remove any row with no recorded death
cases = cases[cases.deaths .&amp;gt; 0, :];

last(cases[:, [:time, :cases, :deaths]], 6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last rows shows the number of cases and deaths up to the last date in the dataset.&lt;/p&gt;
&lt;p&gt;Plotting the number of death shows an almost exponential increase in numbers (straight line in logarithmic scale).&lt;/p&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;using PyPlot;

pyplot();
clf();
ioff();
plot_x = cases.time;
plot_y = cases.deaths;

fig, ax = PyPlot.subplots();

ax.plot(plot_x, plot_y, &amp;quot;ro&amp;quot;);
ax.fill_between(plot_x, plot_y, color=&amp;quot;red&amp;quot;, linewidth=2, label=&amp;quot;Deaths&amp;quot;, alpha=0.3);
ax.legend(loc=&amp;quot;upper left&amp;quot;);
ax.set_xlabel(&amp;quot;time&amp;quot;);
ax.set_ylabel(&amp;quot;Deaths&amp;quot;);
ax.set_yscale(&amp;quot;log&amp;quot;);

PyPlot.savefig(&amp;quot;images/Deaths.png&amp;quot;);&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/media/post/2020-COVID/images/Deaths.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Deaths&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This file contains ICU beds figures.&lt;/p&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;ICU_capacity = select(CSV.read(&amp;quot;data/ICU_capacity.tsv&amp;quot;; delim = &amp;quot;\t&amp;quot;), :country, :CriticalCare);
ICU_capacity = @where(ICU_capacity, occursin.(country, :country))[!, :CriticalCare][1];
ICU_capacity = convert(Float64, ICU_capacity);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Country codes are necessary to load the another file.&lt;/p&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;country_codes = select(CSV.read(&amp;quot;data/country_codes.csv&amp;quot;), :name, Symbol(&amp;quot;alpha-3&amp;quot;));
country_codes = @where(country_codes, occursin.(country, :name));
countryShort = country_codes[:, Symbol(&amp;quot;alpha-3&amp;quot;)][1];&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This file contains hospital beds figures.&lt;/p&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;hospital_capacity = select(CSV.read(&amp;quot;data/hospital_capacity.csv&amp;quot;, 
                                    types = Dict(:COUNTRY =&amp;gt; String), limit = 1267), :COUNTRY, :YEAR, :VALUE);
hospital_capacity = @where(hospital_capacity, Not(ismissing.(:COUNTRY)));
hospital_capacity = last(@where(hospital_capacity, occursin.(countryShort, :COUNTRY)), 1)[!, :VALUE][1];
hospital_capacity = convert(Float64, hospital_capacity);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This file contains a distribution of the population in age groups.&lt;/p&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;age_distribution = CSV.read(&amp;quot;data/country_age_distribution.csv&amp;quot;);
age_distribution = @where(age_distribution, occursin.(country, :_key))[!, 2:10];

# Convert to simple matrix
age_distribution = Matrix(age_distribution);
show(age_distribution);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;initialise-parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Initialise parameters&lt;/h1&gt;
&lt;div id=&#34;fixed-constants&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fixed constants&lt;/h2&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;SeverityLevel = :moderate;
Latitude = :north;

StartDate = Date(2020, 3, 1);
StartDays = date2days(StartDate);

EndDate = Date(2020, 9, 1);
EndDays = date2days(EndDate);

tSpan = (StartDays, EndDays);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;infrastructure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Infrastructure&lt;/h2&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;BED_max = hospital_capacity&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;ICU_max = ICU_capacity&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;parameter-vector&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameter vector&lt;/h2&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# r₀, tₗ, tᵢ, tₕ, tᵤ, γᵢ, γⱼ, γₖ, δₖ, δₗ, δᵤ, startDate = params 

parameters = [  baseR₀[Latitude, SeverityLevel], 
                tₗ[SeverityLevel], tᵢ[SeverityLevel], tₕ, tᵤ, 
                γₑ, γᵢ, γⱼ, γₖ, 
                δₖ, δₗ, δᵤ, 
                StartDays];&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;population&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Population&lt;/h2&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;Age_Pyramid = transpose(age_distribution);
Age_Pyramid_frac = Age_Pyramid / sum(Age_Pyramid);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We do not know the number of actual number of infections cases at the start of the model. We only know confirmed cases (almost certainly far below the number of actual infections).&lt;/p&gt;
&lt;p&gt;We assume that actual infections are 3 time more numerous.&lt;/p&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;DeathsAtStart = @where(cases, :time .== StartDate)[!, :deaths][1];
ConfirmedAtStart = @where(cases, :time .== StartDate)[!, :cases][1];
EstimatedAtStart = 3.0 * ConfirmedAtStart;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;parameters-vector&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameters vector&lt;/h2&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# Note that values are inintialised at 1 to avoid division by zero

S0 = Age_Pyramid;
E0 = ones(nAgeGroup);
I0 = EstimatedAtStart * Age_Pyramid_frac;
J0 = ones(nAgeGroup);
H0 = ones(nAgeGroup);
C0 = ones(nAgeGroup);
R0 = ones(nAgeGroup);
D0 = DeathsAtStart * Age_Pyramid_frac;
K0 = ones(nAgeGroup);
L0 = ones(nAgeGroup);

# Everybody confirmed is in hospital
BED = [ConfirmedAtStart];
ICU = [1.0];

P0 = vcat(S0, E0, I0, J0, H0, C0, R0, D0, K0, L0, BED, ICU);
dP = 0 * P0;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;differential-equation-solver&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Differential equation solver&lt;/h1&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;model = ODEProblem(epiDynamics!, P0, tSpan, parameters);&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# Note: progress steps might be too quick to see!
sol = solve(model, Tsit5(); progress = false, progress_steps = 5);&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# The solutions are returned as an Array of Arrays: 
#  - it is a vector of size the number of timesteps
#  - each element of the vector is a vector of all the variables
nSteps = length(sol.t);
nVars  = length(sol.u[1]);

# Empty dataframe to contain all the numbers
# (When running a loop at top-level, the global keywrod is necessary to modify global variables.)
solDF = zeros((nSteps, nVars));
for i = 1:nSteps
    global solDF
    solDF[i, :] = sol.u[i]
end;

solDF = hcat(DataFrame(t = sol.t), DataFrame(solDF));

# Let&amp;#39;s clean the names
compartments =  [&amp;quot;S&amp;quot;, &amp;quot;E&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;J&amp;quot;, &amp;quot;H&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;R&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;K&amp;quot;, &amp;quot;L&amp;quot;];
solnames = vcat([:t], [Symbol(c * repr(n)) for c in compartments for n in 0:(nAgeGroup-1)], [:Beds], [:ICU]);
rename!(solDF, solnames);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;# Create sums for each compartment
# (Consider solDF[!, r&amp;quot;S&amp;quot;])
# 
for c in compartments
    col =  [Symbol(c * repr(n)) for n in 0:(nAgeGroup-1)]
    s = DataFrame(C = sum.(eachrow(solDF[:, col])))
    rename!(s, [Symbol(c)])
        
    global solDF = hcat(solDF, s)
end;

# The D column gives the final number of dead.
println(last(solDF[:, Symbol.(compartments)], 5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last row shows the final sizes of the various compartments.&lt;/p&gt;
&lt;p&gt;Next is the evolution of the over time.&lt;/p&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;pyplot();
clf();
ioff();

fig, ax = PyPlot.subplots();

ax.plot(solDF.t, solDF.D, label = &amp;quot;Forecast&amp;quot;);
ax.plot(solDF.t, solDF.R, label = &amp;quot;Recoveries&amp;quot;);
ax.plot(cases.t, cases.deaths, &amp;quot;ro&amp;quot;, label = &amp;quot;Actual&amp;quot;, alpha = 0.3);

ax.legend(loc=&amp;quot;lower right&amp;quot;);
ax.set_xlabel(&amp;quot;time&amp;quot;);
ax.set_ylabel(&amp;quot;Individuals&amp;quot;);
ax.set_yscale(&amp;quot;log&amp;quot;);

PyPlot.savefig(&amp;quot;images/DeathsForecast.png&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/media/post/2020-COVID/images/DeathsForecast.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Increase in Recoveries and Deaths over time&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It is clear the model forecasts a faster growth than reality. A parameter estimation is necessary.&lt;/p&gt;
&lt;pre class=&#34;julia&#34;&gt;&lt;code&gt;pyplot();
clf();
ioff();

fig, ax = PyPlot.subplots();

ax.plot(solDF.t, solDF.Beds, label = &amp;quot;Beds&amp;quot;);
ax.plot(solDF.t, solDF.ICU, label = &amp;quot;ICU&amp;quot;);

ax.legend(loc=&amp;quot;lower right&amp;quot;);
ax.set_xlabel(&amp;quot;time&amp;quot;);
ax.set_ylabel(&amp;quot;Number of beds&amp;quot;);
ax.set_yscale(&amp;quot;linear&amp;quot;);

PyPlot.savefig(&amp;quot;images/BedUsage.png&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/media/post/2020-COVID/images/BedUsage.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Bed Usage over time&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It is clear that the requirements for beds quickly hits the available capacity&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bilibliography&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bilibliography&lt;/h1&gt;
&lt;p&gt;The Novel Coronavirus Pneumonia Emergency Response Epidemiology Team. The Epidemiological Characteristics of an Outbreak of 2019 Novel Coronavirus Diseases (COVID-19) — China, 2020[J]. China CDC Weekly, 2020, 2(8): 113-122. &lt;a href=&#34;http://weekly.chinacdc.cn/en/article/id/e53946e2-c6c4-41e9-9a9b-fea8db1a8f51&#34;&gt;LINK&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
