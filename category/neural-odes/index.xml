<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neural ODEs | Back2Numbers</title>
    <link>/category/neural-odes.html</link>
      <atom:link href="/category/neural-odes/index.xml" rel="self" type="application/rss+xml" />
    <description>Neural ODEs</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Emmanuel Rialland 2020</copyright><lastBuildDate>Fri, 14 Aug 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Neural ODEs</title>
      <link>/category/neural-odes.html</link>
    </image>
    
    <item>
      <title>Normalising Flows and Neural ODEs</title>
      <link>/2020/08/14/normalising-flows.html</link>
      <pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/2020/08/14/normalising-flows.html</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/d3/d3.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/dagre/dagre-d3.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/mermaid/dist/mermaid.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/mermaid/dist/mermaid.slim.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/chromatography/chromatography.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/DiagrammeR-binding/DiagrammeR.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#a-few-words-about-generative-models&#34;&gt;A few words about Generative Models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#latent-variables&#34;&gt;Latent variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#examples-of-generative-models&#34;&gt;Examples of generative models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#generative-adversarial-networks-gans&#34;&gt;Generative Adversarial Networks (&lt;strong&gt;GANS&lt;/strong&gt;)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variational-autoencoders&#34;&gt;Variational autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#limitations&#34;&gt;Limitations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#normalising-flows&#34;&gt;Normalising flows&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-1&#34;&gt;Introduction&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#some-math&#34;&gt;Some math&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#training-loss-optimisation-and-information-flow&#34;&gt;Training loss optimisation and information flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#basic-flows&#34;&gt;Basic flows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#planar-flows&#34;&gt;Planar Flows&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#radial-flows&#34;&gt;Radial flows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#more-complex-flows&#34;&gt;More complex flows&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#residual-flows-discrete-flows&#34;&gt;Residual flows (discrete flows)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-versions&#34;&gt;Other versions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#continuous-flows&#34;&gt;Continuous flows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#neural-ordinary-differential-equations&#34;&gt;Neural ordinary differential equations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#continuous-flows-1&#34;&gt;Continuous flows&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#continuous-flows-means-no-crossover&#34;&gt;Continuous flows means no-crossover&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#universal-ordinary-differential-equations&#34;&gt;Universal ordinary differential equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#stochastic-differential-equations&#34;&gt;Stochastic differential equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other&#34;&gt;Other&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#optimisation&#34;&gt;Optimisation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#more-information&#34;&gt;More information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#one-more-thing&#34;&gt;One more thing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;One of the three best papers awarded at NIPS 2018 was &lt;em&gt;Neural Ordinary Differential Equations&lt;/em&gt; by Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt and David Duvenaud &lt;span class=&#34;citation&#34;&gt;(Chen et al. &lt;a href=&#34;#ref-chenNeuralOrdinaryDifferential2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;. Since then, the field has developed in multiple directions. This post goes through some background about generative models, normalising flows and finally a few of the underlying ideas of the paper. The form does not intend to be mathematically rigorous but convey some intuitions.&lt;/p&gt;
&lt;div id=&#34;a-few-words-about-generative-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A few words about Generative Models&lt;/h1&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Generative models&lt;/strong&gt; are about learning simple &lt;strong&gt;representations&lt;/strong&gt; of a complex datasets; how to, from a few parameters, generate realistic samples that are similar to a given dataset. Those few parameters usually follow simple distributions (e.g. uniform or Gaussian), and are transformed through complex transformations into the more complex dataset distribution. This is an unsupervised procedure which, in a sense, mirrors clustering methods: clustering starts from the dataset and summarises it into few parameters.&lt;/p&gt;
&lt;p&gt;[DIAGRAM var 1D to another var 1D]&lt;/p&gt;
&lt;p&gt;[DIAGRAM var 1D to another var 2D]&lt;/p&gt;
&lt;p&gt;Although unsupervised, the result of this learning can be used as a &lt;strong&gt;pretraining&lt;/strong&gt; step in a later supervised context, or where that dataset is a mix of labelled and un-labelled data. The properties of the well-understood starting probability distributions can then help draw conclusions about the dataset’s distribution or generate synthetic datasets.&lt;/p&gt;
&lt;p&gt;The same methods can also be used in supervised learning to learn the representation of a target dataset (categorical or continuous) as a transformation of the features dataset. The unsupervised becomes supervised.&lt;/p&gt;
&lt;p&gt;What does &lt;em&gt;representation learning&lt;/em&gt; actually mean? It is the automatic search for a few parameters that encapsulate rich enough information to generate a dataset. Generative models learn those parameters and, starting from them, how to re-create samples similar to the original dataset.&lt;/p&gt;
&lt;p&gt;Let’s use cars as an analogy.&lt;/p&gt;
&lt;p&gt;All cars have 4 wheels, an engine, brakes, seats. One could be interested in comfort or racing them or lugging things around or safety or fitting as many kids as possible. Each base vector could be express any one of those characteristics, but they will all have an engine, breaks and seats. The generation function recreates everything that is common. It doesn’t matter if the car is comfy or not; it needs seats and a driving wheel. The generative function has to create those features. However, the exact number of cylinders, its shape, the seats fabric, or stiffness of the suspension all depend on the type of car.&lt;/p&gt;
&lt;p&gt;The tru fundamentals are not obvious. For a long time, American cars had softer suspension than European cars. The definition of comfortable is relative. The performance of an old car is objectively not the same as compared to new ones. Maybe other characteristics are more relevant to generate. Maybe price? Consumption? Year of coming to market? All those factors are obviously inter-related.&lt;/p&gt;
&lt;p&gt;Generative models are more than generating samples from a few fundamental parameters. They also learn what those parameters should be.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;latent-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Latent variables&lt;/h2&gt;
&lt;p&gt;Still using the car analogy, if the year of a model was not given, the generative process might still be able to conclude that the model year &lt;em&gt;should&lt;/em&gt; be an implicit parameter to be learned since relevant to generate the dataset: year is an unstated parameter that explains the dataset. Both the Lamborghini Miura and Lamborghini Countach&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; were similar in terms of perceived performance and exclusivity at the time they were created. But actual performances and styling where incredibly different.&lt;/p&gt;
&lt;p&gt;If looking at the stock market: take a set of market prices at a given date; it would have significantly different meanings in a bull or a bear market. Market regime would be a reasonable latent variable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;examples-of-generative-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Examples of generative models&lt;/h2&gt;
&lt;p&gt;There are quite a number of generative models such restricted Boltzmann machines, deep belief networks. Refer to &lt;span class=&#34;citation&#34;&gt;(Theodoridis &lt;a href=&#34;#ref-theodoridisMachineLearningBayesian2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(Russell and Norvig &lt;a href=&#34;#ref-russellArtificialIntelligenceModern2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; for example. Let’s consider generative adversarial networks and variational auto-encoders.&lt;/p&gt;
&lt;div id=&#34;generative-adversarial-networks-gans&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generative Adversarial Networks (&lt;strong&gt;GANS&lt;/strong&gt;)&lt;/h3&gt;
&lt;p&gt;Recently, GANs have risen to the fore as a way to generate artificial datasets that are, for some definition, indistinguishable from a real dataset. They consist of two parts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Editor at 
# https://mermaid-js.github.io/mermaid-live-editor
DiagrammeR::mermaid(&amp;quot;GAN.mmd&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;DiagrammeR html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;\ngraph LR\n\n  subgraph Generative Model\n    Representation[\&#34;Representation &lt;br/&gt; (latent space)\&#34;] --&gt; Generator[&lt;b&gt;Generator&lt;\/b&gt;]\n  end\n\n  Generator--&gt; Discriminator[&lt;b&gt;Discriminator&lt;\/b&gt;]\n\n  Sample\n\n  Sample[Sample] --&gt; Discriminator\n\n  Discriminator --&gt; ID[Identification as &lt;br/&gt; generated or true]\n\n  ID -.-&gt;|Feedback| Discriminator\n  ID -.-&gt;|Feedback| Generator&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A generator which is the generative model itself: given a simple representation, the generator proposes samples that aim to be undistinguishable from the dataset sample.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A discriminator whose job is to identify whether a sample comes from the generator or from the dataset.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both are trained simultaneously:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;if the discriminator finds it obvious to guess, the generator is not doing a good job and needs to improve;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if the discriminator guesses 50/50 (does not do better than flipping a coin), it has to discover which true dataset features are truly relevant.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;variational-autoencoders&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variational autoencoders&lt;/h3&gt;
&lt;p&gt;A successful GAN can replicate the richness of a dataset, but not the its distribution. A GAN can generate a large number of correct sentences, but will not tell how likely to occur that sentence is (or at least guarantee that the distributions match). &lt;em&gt;‘The dog chases the cat’&lt;/em&gt; and &lt;em&gt;‘The Chihuahua chases the cat’&lt;/em&gt; are both perfectly valid, but the latter less unlikely to appear.&lt;/p&gt;
&lt;p&gt;Variable autoencoders (VAEs) take another approach by learning a generator (called &lt;em&gt;decoder&lt;/em&gt;) &lt;em&gt;and&lt;/em&gt; learning the distribution of the parameters to reflect the distribution of the samples within the dataset (the &lt;em&gt;encoder&lt;/em&gt;). Both are trained simultaneously on the dataset samples by projecting samples on the latent variables’ space, proposing a generated sample from that projection and training on the reconstruction loss. The encoder actually learns means and standard deviations of the each latent variable, wach being a normal distribution. The samples generated will be as rich as the GAN’s, but the probability of a sample being generated will depend on the learned distributions.&lt;/p&gt;
&lt;p&gt;See &lt;span class=&#34;citation&#34;&gt;(Kingma and Welling &lt;a href=&#34;#ref-kingmaIntroductionVariationalAutoencoders2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; for an approachable extensive introduction. The details of VAEs include implementation aspects (in particular the &lt;em&gt;reparametrization trick&lt;/em&gt;) that are critical to the success of this approach.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;limitations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Limitations&lt;/h2&gt;
&lt;p&gt;We limited the introduction to those two techniques to merely highlight two fundamental aspect that generative models aim at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;explore and replicate the richness of the dataset;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;replicate the probability distribution of the dataset.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that depending on the circumstances, the latter aim may not necessarily be important.&lt;/p&gt;
&lt;p&gt;As usual, training and optimisation methods are at risk of getting stuck at local optima. In the case of those two techniques, this manifests itself in different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;GANs Mode collapse&lt;/em&gt;: Mode collapse occurs in GANs when the generator only explores limited domains. Imagine training a GAN to recognise mammals (the dataset would contain kangaroos, whales, dogs and cats…). If the generator proposes everything but kangoaroos, it is still properly generate mammals, but obviously misses out on a few possibilities. Essentially, the generator reaches a local minimum where the gradient becomes too small to explore alternatives. This is in part due to the difficulty of progressing the training of both the generator and the discriminator in a way that does not lock any one of them in a local optimum while the other still needs improving: if either converges too rapidly, the other will struggle improving.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;VAEs Posterior collapse&lt;/em&gt;: Posterior collapse in VAEs arises when the generative model learns to ignore a subset of the latent variables (although the encoder generates those variables) &lt;span class=&#34;citation&#34;&gt;(Lucas et al. &lt;a href=&#34;#ref-lucasDonBlameELBO2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;&lt;a href=&#34;#ref-lucasDonBlameELBO2019&#34; role=&#34;doc-biblioref&#34;&gt;b&lt;/a&gt;)&lt;/span&gt;. More technically, it happens when the variational distribution closely matches the uninformative prior for a subset of latent variables &lt;span class=&#34;citation&#34;&gt;(Lucas et al. &lt;a href=&#34;#ref-lucasUnderstandingPosteriorCollapse2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;&lt;a href=&#34;#ref-lucasUnderstandingPosteriorCollapse2019&#34; role=&#34;doc-biblioref&#34;&gt;a&lt;/a&gt;)&lt;/span&gt;. The exact reasons for this are not entirely understood and this remains an active area of research (refer this extensive list of &lt;a href=&#34;https://github.com/sajadn/posterior-collapse-list&#34;&gt;papers&lt;/a&gt; on the topic).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we will see, normalising flows address those two difficulties. Intuitively:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Mode collapse reflects that the generative process does not generate enough possibilities; that the spectrum of possibilities is not as rich as that of the dataset. Normalising flows attempt to address this in two ways. Firstly, the optimising process aims as optimising (and matching) the amount of generated information to that of the dataset. Secondly, normalising flows allow to start from a sample in the dataset, flow back to the simple distribution and estimate how (un)likely the generative model would have generated this sample. If the dataset has a lot of whales, generating only dogs is clearly not good enough…&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The posterior collapse could simply be a mismatch between the number of latent variables and the dimensionality of the dataset, difficulties to specify an effective loss function (and its gradients) or a local optima. As we will see, normalising flows impose that the generative model be a bijection. In a sense, this means that there should be the same ‘complexity’ in the latent variables as in the dataset (although not the same amount of information in the sense of information theory).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;lt;!–&amp;gt;
## &lt;strong&gt;CHECK FOLLOWING&lt;/strong&gt; Revival of neural networks&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In this section, the discussion setting is to learn representations that are independent of a specific
target task; the goal is to extract such information using the input data only. The reason for such a focus
is twofold. First, learning a model representation of the input data can be used subsequently in different
tasks in order to facilitate the training. Sometimes, this is also known as pretraining, where parameters
learned using unlabeled data can be used as initial estimates of the parameters for another supervised
learning. This can be useful when the number of labeled examples is not large enough (see, e.g., [148]
for a discussion). It is worth pointing out that such a pretraining rationale is of a historical importance,
because it led to the revival of neural networks, as will be discussed soon [86].&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;86 is &lt;span class=&#34;citation&#34;&gt;(“A Fast Learning Algorithm for Deep Belief Nets” &lt;a href=&#34;#ref-FastLearningAlgorithm2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt; from Bengio&lt;/p&gt;
&lt;p&gt;&amp;lt;!–&amp;gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All methodology involve finding a function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. Neural networks have long been known to be able to generate artificially complicated functions and the idea of using them as a way to represent &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is one of the reasons that led to their revivals.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;normalising-flows&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Normalising flows&lt;/h1&gt;
&lt;p&gt;Normalising Flows became popular around 2015 with two papers on density estimation &lt;span class=&#34;citation&#34;&gt;(Dinh, Krueger, and Bengio &lt;a href=&#34;#ref-dinhNICENonlinearIndependent2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; and use of variational inference &lt;span class=&#34;citation&#34;&gt;(Rezende and Mohamed &lt;a href=&#34;#ref-rezendeVariationalInferenceNormalizing2016&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;. However, the concepts predated those papers. (See &lt;span class=&#34;citation&#34;&gt;(Kobyzev, Prince, and Brubaker &lt;a href=&#34;#ref-kobyzevNormalizingFlowsIntroduction2020a&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(Papamakarios et al. &lt;a href=&#34;#ref-papamakariosNormalizingFlowsProbabilistic2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; for recent survey papers.)&lt;/p&gt;
&lt;div id=&#34;introduction-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;One important limitations of the approaches described above is that the generation flow is unidirectional: one starts from a source distribution, sometimes with well-known properties, and generates a richer target distribution. However, given a particular sample in the target distribution, there is no guaranteed way to identify where it would fall in the original source distribution. That flow of transformation from source to target is not guaranteed to be bijective or invertible (same meaning, different crowds). (In the case of VAEs, this is the case in the encoder.)&lt;/p&gt;
&lt;p&gt;Normalising flows are a generic solution to that issue: it is a transformation of an original distribution into a more complex distribution by an invertible and differentiable mapping, where the probability density of a sample can be evaluated by transforming it back to the original distribution. The density is evaluated by computing the density of the normalised inverse-transformed sample.&lt;/p&gt;
&lt;p&gt;In practice, this is a bit too general to be of any use. Breaking it down:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The original distribution is simple with well-known statistical properties: i.i.d. Gaussian or uniform distributions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The transformation function is expected to be complicated, and is normally specified as a series of successive transformations, each simpler (though expressive enough) and easy to parametrise.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each simple transformation is itself invertible and differentiable, therefore guaranteeing that the overall transformation is too.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We want the transformation to be &lt;em&gt;normalised&lt;/em&gt;: the cumulative probability density of the generated targets from latent variables has to be equal 1. Otherwise, flowing backwards to use the properties of the original would make no sense. The word &lt;em&gt;normalising&lt;/em&gt; refers to this operation, and not to the fact that the original distribution &lt;em&gt;could&lt;/em&gt; be normal.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Geometrically, the probability distribution around each point in the latent variables space has a certain volume that is successively transformed with each transformation. Keeping track of all the volume changes ensures that we can relate probability density functions in the original space and the target space.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How to keep track? This is where the condition of having invertible and differentiable transformation becomes important. (Math-speak: we have a series of diffeomorphisms which are transformations from one infinitesimal volume to another. They are invertible and differentiable, and their inverses are also differentiable.) If one imagines a small volume of space around a starting point, that volume gets distorted along the way. At each point, the transformation is differentiable and can be approximated by a linear transformation (a matrix). That matrix is the Jacobian of the transformation at that point (diffeomorphims also means that the Jacobian matrix exists and is invertible). Being invertible, the matrix has no zero eigenvalues and the change of volume is locally equal to the product of all the eigenvalues: the volume gets squeezed along some dimensions, expanded along others, rotations are irrelevant. The product of the eigenvalues is the determinant of the matrix. A negative eigenvalue would mean that the infinitesimal volume is ‘flipped’ along that direction. That sign is irrelevant: the local volume change is therefore the absolute value of the determinant.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can already anticipate a calculation nightmare: determinants are computationally very heavy. Additionally, in order to backpropagate a loss to optimise the transformations’ parameters, we will need the Jacobians of the inverse transformations (the inverse of the transformmation Jacobian). Without further simplifying assumptions or tricks, normalising flows would be impractical for large dimensions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;some-math&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some math&lt;/h3&gt;
&lt;p&gt;The starting distribution is a random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; with a support in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^D\)&lt;/span&gt;. For simplicity, we will assume just assume that the support is &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^D\)&lt;/span&gt; since using measurable supports does not change the results. If &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is transformed into &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; by an invertible function/mapping &lt;span class=&#34;math inline&#34;&gt;\(f: \mathbb{R}^D \rightarrow \mathbb{R}^D\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(Y=f(X)\)&lt;/span&gt;), then the density function of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P_Y(\vec{y}) &amp;amp; = P_X(\vec{x}) \left| \det \nabla f^{-1}(\vec{y})  \right| \\
                &amp;amp; = P_X(\vec{x}) \left| \det\nabla f(\vec{x}) \right|^{-1}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\vec{x} = f^{-1}(\vec{y})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nabla\)&lt;/span&gt; represents the Jacobian operator. Note the use of &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; to denote vectors instead of the normal &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; which I find on-screen easy to mistake for a constant.&lt;/p&gt;
&lt;p&gt;Following &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is the &lt;em&gt;generative&lt;/em&gt; direction; following &lt;span class=&#34;math inline&#34;&gt;\(f^{-1}\)&lt;/span&gt; is the &lt;em&gt;normalising&lt;/em&gt; direction (as well as being the &lt;em&gt;inference&lt;/em&gt; direction in a more general context).&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; was a series of individual transformation &lt;span class=&#34;math inline&#34;&gt;\(f = f_1 \circ f_i \circ \cdots \ f_N\)&lt;/span&gt;, then it naturally follows that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\det\nabla f(\vec{x})      &amp;amp; = \prod_{i=1}^N{\det \nabla f_i(\vec{x}_i)} \\
\det\nabla f^{-1}(\vec{x}) &amp;amp; = \prod_{i=1}^N{\det \nabla f_i^{-1}(\vec{x}_i)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In order to make clear that the Jacobian is &lt;em&gt;not&lt;/em&gt; taken wrt the starting latent variables &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, we use the notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\vec{x}_i = f_{i-1}(\vec{x}_{i-1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;training-loss-optimisation-and-information-flow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training loss optimisation and information flow&lt;/h2&gt;
&lt;p&gt;Before moving into examples for normalising flows, we need to comment on the loss function optimisation. How do we determine the generative model’s parameters so that the generated distribution is as close as possible to the real distribution (or at least to the distribution of the samples drawn from that true distribution)?&lt;/p&gt;
&lt;p&gt;A standard way to do this is to calculate the Kullback-Leibler divergence between the two. Recall that the KL divergence &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{KL}(P \vert \vert Q)\)&lt;/span&gt; is &lt;em&gt;not&lt;/em&gt; a distance as it is not symmetric. I personally read &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{KL}(P \vert \vert Q)\)&lt;/span&gt; as “the loss of information on the true &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; if using the approximation &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;” as a way to keep the two distributions at their right place (writing &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{KL}(P_{true} \vert \vert Q_{est.})\)&lt;/span&gt; helps clarify the proper order).&lt;/p&gt;
&lt;p&gt;The KL divergence is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathbb{KL}(P_{true} \vert \vert Q_{est.}) = \mathbb{E}_{P_{true}(\vec{x})} \log \frac{P_{true}(\vec{x})}{Q_{est.}(\vec{x})}
\end{aligned}
\]&lt;/span&gt;
Or for a discrete distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathbb{KL}(P_{true} \vert \vert Q_{est}) &amp;amp; =  \sum_{\vec{x} \in X} P_{true}(\vec{x}) \log \frac{P_{true}(\vec{x})}{Q_{est}(\vec{x})} \\
                                          &amp;amp; =  \sum_{\vec{x} \in X} P_{true}(\vec{x}) \left[ \log P_{true}(\vec{x}) - \log Q_{est}(\vec{x}) \right] 
\end{aligned} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In our particular case, this becomes:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathbb{KL}(P_{true} \vert \vert P_Y) &amp;amp; = \sum_{\vec{x} \in X} {P_{true}(\vec{x}) \log \frac{P_{true}(\vec{x})}{P_Y(\vec{y})}} \\
                                      &amp;amp; = \sum_{\vec{x} \in X} {P_{true}(\vec{x}) \left[ \log P_{true}(\vec{x}) - \log P_Y(\vec{y}) \right] }
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;since:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P_Y(\vec{y}) &amp;amp; = P_X(\vec{x}) \left| det \nabla f^{-1}(\vec{y})  \right| \\
&amp;amp; = P_X(\vec{x}) \left| det\nabla f(\vec{x}) \right|^{-1}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We end up with:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{split}
\mathbb{KL}(P_{true} \vert \vert P_Y) &amp;amp; = \sum_{\vec{x} \in X} {P_{true}(\vec{x}) \left[ \log P_{true}(\vec{x})  - \log \left( P_X(\vec{x}) \left| det \nabla f(\vec{y})  \right|^{-1} \right) \right] }
\end{split}
\]&lt;/span&gt;
Minimising this divergence is achieved by changing the parameters which generate &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The divergence is one of many measures that can be used to measure the distance (in the loose sense of the word) between the true and generated distributions. But the KL divergence illustrates how logarithms of the probability distributions naturally appear. A common formulation of the loss is the Wasserstein distance. In the setting of the normalising flows (and VAEs), we have two transformations: the inference direction (the encoder) and the generative direction (the decoder). Given the back-and-forth nature, it makes sense to &lt;em&gt;not&lt;/em&gt; favour one direction over the other. Instead of using the KL divergence which is not symmetric, use the mutual information (this is equivalent to using free energy as in &lt;span class=&#34;citation&#34;&gt;(Rezende and Mohamed &lt;a href=&#34;#ref-rezendeVariationalInferenceNormalizing2016&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Regardless of the choice of loss function, it is obvious that optimising &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{KL}(P_{true} \vert \vert P_Y)\)&lt;/span&gt; cannot be contemplated without serious optimisations. Alternatively, finding more tractable alternative distance measurements is an active research topic.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-flows&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic flows&lt;/h2&gt;
&lt;p&gt;When normalizing flows were introduced by &lt;span class=&#34;citation&#34;&gt;(Rezende and Mohamed &lt;a href=&#34;#ref-rezendeVariationalInferenceNormalizing2016&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;, they experimented with simple transformations: a linear transformation (with a simple non-linear function) called &lt;em&gt;planar flows&lt;/em&gt; and flows within a space centered on a reference latent variable called &lt;em&gt;radial flows&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;planar-flows&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Planar Flows&lt;/h2&gt;
&lt;p&gt;A planar flow is formulated as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f_i(\vec{x}_i) = \vec{x}_i + \vec{u_i}  h(\vec{w}_i^\intercal \vec{x}_i + b_i)
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\vec{u}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\vec{w}_i\)&lt;/span&gt; are vectors, &lt;span class=&#34;math inline&#34;&gt;\(h()\)&lt;/span&gt; is a non-linear real function and &lt;span class=&#34;math inline&#34;&gt;\(b_i\)&lt;/span&gt; is a scalar.&lt;/p&gt;
&lt;p&gt;By defining:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\psi_i(\vec{z}) = h&amp;#39;(\vec{w}^\intercal \vec{z} + b_i) \vec{w}_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;the determinant required to normalize the flow can be simplified to (see original paper for the short steps involved):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left| \det \frac{\partial f_i}{\partial x_i}  \right| = \left| \det \left( \mathbb{I} + \vec{u_i} \psi_i(\vec{x}_i)^\intercal \right) \right| = \left| 1 + \vec{u_i}^\intercal \psi_i(\vec{x}_i)  \right|
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is a more tractable expression.&lt;/p&gt;
&lt;div id=&#34;radial-flows&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Radial flows&lt;/h3&gt;
&lt;p&gt;The formulation of the radial flows takes a reference hyper-ball centered at a reference point &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}_0\)&lt;/span&gt;. Any point &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; gets moved in the direction of &lt;span class=&#34;math inline&#34;&gt;\(\vec{x} - \vec{x}_0\)&lt;/span&gt;. That move is dependent on &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt;. In other words, imagine a plain hyper-ball, after many such transformations, you obtain a hyper-potato.&lt;/p&gt;
&lt;p&gt;The flows are defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f_i(\vec{x}_i) = \vec{x}_i + \beta_i h(\alpha_i, \rho_i) \left( \vec{x}_i - \vec{x}_0 \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; is a strictly positive scalar, &lt;span class=&#34;math inline&#34;&gt;\(\beta_i\)&lt;/span&gt; is a scalar, &lt;span class=&#34;math inline&#34;&gt;\(\rho_i = \left|| \vec{x}_i - \vec{x}_0 \right||\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(h(\alpha_i, \rho_i) = \frac{1}{\alpha_i + \rho_i}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This family of functions gives the following expression of the determinant:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left| \det \nabla f_i(\vec{x}_i) \right| = \left[ 1 + \beta_i h(\alpha_i, \rho_i) \right] ^{D-1} \left[ 1 + \beta_i h(\alpha_i, \rho_i) +  \beta_i \rho_i h&amp;#39;(\alpha_i, \rho_i) \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, this is a more tractable expression.&lt;/p&gt;
&lt;p&gt;Unfortunately, it was found that those transformations do not scale well to high-dimensional latent space.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;more-complex-flows&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;More complex flows&lt;/h2&gt;
&lt;div id=&#34;residual-flows-discrete-flows&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Residual flows (discrete flows)&lt;/h3&gt;
&lt;p&gt;Various proposals were initially put forward with common aims: replacing &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; by a series of sequentially composed simpler but expressive base functions and paying particular attention the computational costs. (refer &lt;span class=&#34;citation&#34;&gt;(Kobyzev, Prince, and Brubaker &lt;a href=&#34;#ref-kobyzevNormalizingFlowsIntroduction2020a&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(Papamakarios et al. &lt;a href=&#34;#ref-papamakariosNormalizingFlowsProbabilistic2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; for details).&lt;/p&gt;
&lt;p&gt;Residual flows &lt;span class=&#34;citation&#34;&gt;(He et al. &lt;a href=&#34;#ref-heDeepResidualLearning2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; were a key development. As the name suggests, the transformations mirror the neural networks RevNet structure. Explicitly, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is defined as &lt;span class=&#34;math inline&#34;&gt;\(f(x) = x + \phi(x)\)&lt;/span&gt;. The left-hand side identity term is a matrix where all the eigenvalues are 1 (duh). If &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt; represented a simple matrix multiplication, imposing the condition that all its eigenvalues of the righthand side term are strictly strictly between 0 and 1 ensure that &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; remains invertible. An equivalent, and more general condition, is to impose that &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is Lipschitz-continuous with a constant below 1. That is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\forall x, y \qquad  0 &amp;lt; \left| \phi(x) - \phi(y) \right| &amp;lt; \left| x - y \right|
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\forall x, h&amp;gt;0 \qquad  0 &amp;lt; \frac{\left| \phi(x+h) - \phi(x) \right|}{h} &amp;lt; 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thanks to this condition, not only &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is invertible, but all the eigenvalues of &lt;span class=&#34;math inline&#34;&gt;\(\nabla f = \mathbb{I} + \nabla \phi(x)\)&lt;/span&gt; are strictly positive (adding a transformation with unity eigenvalues (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{I}\)&lt;/span&gt;) and a transformation with eigenvalues strictly below unity (in norm) cannot result in a transformation with nil eigenvalues). Therefore, we can be certain that $f = ( + (x)) = ( + (x)) $ (no negative eigenvalues).&lt;/p&gt;
&lt;p&gt;Recalling that &lt;span class=&#34;math inline&#34;&gt;\(det(e^A) = e^{tr(A)}\)&lt;/span&gt; and the Taylor expansion of &lt;span class=&#34;math inline&#34;&gt;\(\log\)&lt;/span&gt;, we obtain the following simplification:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\log \enspace \vert \det \nabla f \vert &amp;amp; = \log \enspace \det(\mathbb{I} + \nabla \phi) \\ 
                                       &amp;amp; = tr(\log (\mathbb{I} + \nabla \phi)) \\
\log \enspace \vert \det \nabla f \vert &amp;amp; = \sum_{k=1}^{\infty}{(-1)^{k+1} \frac{tr(\nabla \phi)^k}{k}}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Obviously a trace is much easier to calculate than a determinant. However, the expression now becomes an infinite series. One of the core result of the cited paper is an algorithm to limit the number of terms to calculate in this infinite series.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;other-versions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other versions&lt;/h2&gt;
&lt;p&gt;Table from Papamakorios&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;continuous-flows&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous flows&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;neural-ordinary-differential-equations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Neural ordinary differential equations&lt;/h1&gt;
&lt;p&gt;One of the Best Papers of NeurIPS 2018&lt;/p&gt;
&lt;div id=&#34;continuous-flows-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous flows&lt;/h2&gt;
&lt;div id=&#34;continuous-flows-means-no-crossover&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Continuous flows means no-crossover&lt;/h3&gt;
&lt;p&gt;Not the case for residual which have discrete steps.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;universal-ordinary-differential-equations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Universal ordinary differential equations&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;stochastic-differential-equations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stochastic differential equations&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;other&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other&lt;/h2&gt;
&lt;p&gt;Previously mentioned generative models can be improved with normalising flows&lt;/p&gt;
&lt;p&gt;Flow-GAN Grover, Dhan Ermon, Flow-GAN combining Mx Likelihood and adversarial learning and generative model&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;optimisation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Optimisation&lt;/h2&gt;
&lt;p&gt;Divergence / distance measures&lt;/p&gt;
&lt;p&gt;Kullback-Leibner / Jensen-Shannon divergence / Wasserstein distance&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references hanging-indent&#34;&gt;
&lt;div id=&#34;ref-FastLearningAlgorithm2006&#34;&gt;
&lt;p&gt;“A Fast Learning Algorithm for Deep Belief Nets.” 2006. &lt;em&gt;MIT Press Journals&lt;/em&gt;, Neural Computation,, May. &lt;a href=&#34;https://www.mitpressjournals.org/doi/10.1162/neco.2006.18.7.1527&#34;&gt;https://www.mitpressjournals.org/doi/10.1162/neco.2006.18.7.1527&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-chenNeuralOrdinaryDifferential2019&#34;&gt;
&lt;p&gt;Chen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations,” December. &lt;a href=&#34;http://arxiv.org/abs/1806.07366&#34;&gt;http://arxiv.org/abs/1806.07366&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-dinhNICENonlinearIndependent2015&#34;&gt;
&lt;p&gt;Dinh, Laurent, David Krueger, and Yoshua Bengio. 2015. “NICE: Non-Linear Independent Components Estimation,” April. &lt;a href=&#34;http://arxiv.org/abs/1410.8516&#34;&gt;http://arxiv.org/abs/1410.8516&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-GoodfellowDeepLearning2016&#34;&gt;
&lt;p&gt;Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. &lt;em&gt;Deep Learning&lt;/em&gt;. MIT Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-heDeepResidualLearning2015&#34;&gt;
&lt;p&gt;He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition,” December. &lt;a href=&#34;http://arxiv.org/abs/1512.03385&#34;&gt;http://arxiv.org/abs/1512.03385&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kingmaIntroductionVariationalAutoencoders2019&#34;&gt;
&lt;p&gt;Kingma, Diederik P., and Max Welling. 2019. “An Introduction to Variational Autoencoders.” &lt;em&gt;Foundations and Trends in Machine Learning&lt;/em&gt; 12 (4): 307–92. &lt;a href=&#34;https://doi.org/10/ggfm34&#34;&gt;https://doi.org/10/ggfm34&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kobyzevNormalizingFlowsIntroduction2020a&#34;&gt;
&lt;p&gt;Kobyzev, Ivan, Simon J. D. Prince, and Marcus A. Brubaker. 2020. “Normalizing Flows: An Introduction and Review of Current Methods.” &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/em&gt;, 1–1. &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2020.2992934&#34;&gt;https://doi.org/10.1109/TPAMI.2020.2992934&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lucasUnderstandingPosteriorCollapse2019&#34;&gt;
&lt;p&gt;Lucas, James, George Tucker, Roger Grosse, and Mohammad Norouzi. 2019a. “Understanding Posterior Collapse in Generative Latent Variable Models | Semantic Scholar.” In &lt;em&gt;DeepGenStruct Worshop&lt;/em&gt;. &lt;a href=&#34;https://www.semanticscholar.org/paper/Understanding-Posterior-Collapse-in-Generative-Lucas-Tucker/7e2f5af5d44890c08ef72a5070340e0ffd3643ea&#34;&gt;https://www.semanticscholar.org/paper/Understanding-Posterior-Collapse-in-Generative-Lucas-Tucker/7e2f5af5d44890c08ef72a5070340e0ffd3643ea&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lucasDonBlameELBO2019&#34;&gt;
&lt;p&gt;———. 2019b. “Don’t Blame the ELBO! A Linear VAE Perspective on Posterior Collapse,” November. &lt;a href=&#34;http://arxiv.org/abs/1911.02469&#34;&gt;http://arxiv.org/abs/1911.02469&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-papamakariosNormalizingFlowsProbabilistic2019&#34;&gt;
&lt;p&gt;Papamakarios, George, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. 2019. “Normalizing Flows for Probabilistic Modeling and Inference,” December. &lt;a href=&#34;http://arxiv.org/abs/1912.02762&#34;&gt;http://arxiv.org/abs/1912.02762&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rezendeVariationalInferenceNormalizing2016&#34;&gt;
&lt;p&gt;Rezende, Danilo Jimenez, and Shakir Mohamed. 2016. “Variational Inference with Normalizing Flows,” June. &lt;a href=&#34;http://arxiv.org/abs/1505.05770&#34;&gt;http://arxiv.org/abs/1505.05770&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-russellArtificialIntelligenceModern2020&#34;&gt;
&lt;p&gt;Russell, Stuart, and Peter Norvig. 2020. &lt;em&gt;Artificial Intelligence: A Modern Approach&lt;/em&gt;. 4th ed. Pearson Series on Artificial Intelligence. Pearson. &lt;a href=&#34;http://aima.cs.berkeley.edu/&#34;&gt;http://aima.cs.berkeley.edu/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-theodoridisMachineLearningBayesian2020&#34;&gt;
&lt;p&gt;Theodoridis, Sergios. 2020. &lt;em&gt;Machine Learning: A Bayesian and Optimization Perspective&lt;/em&gt;. Amsterdam Boston Heidelberg London New York Oxford Paris San Diego San Francisco Singapore Sydney Tokyo: Elsevier, AP. &lt;a href=&#34;https://doi.org/10.1016/C2019-0-03772-7&#34;&gt;https://doi.org/10.1016/C2019-0-03772-7&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix-appendix&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;more-information&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;More information&lt;/h1&gt;
&lt;p&gt;This will be Appendix A.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-more-thing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;One more thing&lt;/h1&gt;
&lt;p&gt;This will be Appendix B.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Piece of trivia: It seems that this is pronounced Counta-tch instead of counta-sh.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Incidentally, this observation is made in the last sentence of the last paragraph of the last chapter of the &lt;a href=&#34;https://www.deeplearningbook.org/&#34;&gt;Deep Learning Book&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(Goodfellow, Bengio, and Courville &lt;a href=&#34;#ref-GoodfellowDeepLearning2016&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt; &lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
