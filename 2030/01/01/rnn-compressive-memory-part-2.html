<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>RNN Compressive Memory Part 2: The TransformerXL - Back2Numbers</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="shortcut icon" href="/favicon.ico">
		
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-150743827-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="Back2Numbers" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">Back2Numbers</div>
					
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/whoami.html">
				
				<span class="menu__text">#&gt;whoami</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/categories">
				<i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
				<span class="menu__text">Categories</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/tags">
				<i class="sidebar-button-icon fa fa-lg fa-tags"></i>
				<span class="menu__text">Tags</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/archives">
				<i class="sidebar-button-icon fa fa-lg fa-archive"></i>
				<span class="menu__text">Archives</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/about.html">
				<i class="sidebar-button-icon fa fa-lg fa-question"></i>
				<span class="menu__text">About this site</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/gitbooks/movielens/introduction.html">
				<i class="sidebar-button-icon fa fa-lg fa-question"></i>
				<span class="menu__text">Gitbook Movielens</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/gitbooks/lendingclub/introduction.html">
				<i class="sidebar-button-icon fa fa-lg fa-question"></i>
				<span class="menu__text">Gitbook LendingClub</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/">
				<i class="sidebar-button-icon fa fa-lg fa-home"></i>
				<span class="menu__text">Home</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">RNN Compressive Memory Part 2: The TransformerXL</h1>
			
		</header><div class="content post__content clearfix">
			


<p>This is part 2 of a discussion of the DeepMind paper about <a href="https://arxiv.org/abs/1911.05507">Compressive Transformers for Long-Range Sequence Modelling</a>.</p>
<ul>
<li><p><a href="../../03/07/rnn-compressive-memory-part-1.html">Part 1</a>: A high level introduction to Compressive Memory mechanics starting from basic RNNS;</p></li>
<li><p>Part 2 (here): more details about the TransformerXL;</p></li>
<li><p>Part 3: an implementation using PyTorch (soon);</p></li>
<li><p>Part 4: finally, its application to time series (soon).</p></li>
</ul>
<p>This post describes the building blocks up to what makes a <a href="https://arxiv.org/abs/1901.02860">TransformerXL</a> model as presented in June 2019. The compression mechanics are an add-on to the TransformerXL which therefore need to be detailed. Along the way, it will become clear that, in many ways, the implementation described only applies to embeddings, i.e.Â one-hot encodings, and not to to truly multi-dimensional variable inputs.</p>
<div id="the-transformerxl" class="section level2">
<h2>The TransformerXL</h2>
<p><span class="math display">\[
\begin{array}{l}
\text{Notation:} &amp; \\
\tau &amp; \text{time step or index on successive segments} \\
b &amp; \text{batch size} \\
h &amp; \text{number of heads} \\
s_s &amp; \text{length of a segment} \\
s_m &amp; \text{length of the memory} \\
s = s_m + s_s &amp; \text{length of segment + memory} \\
d &amp; \text{size of an embedding} \\
d_{in} &amp; \text{size of the embedding as input to a given layer} \\
d_{out} &amp; \text{size of the embedding as output from a given layer} \\
(b, h, s, d): &amp; \text{tensor of 4 dimension reflecting those dimensions}\\
l &amp; \text{index of a layer} \\
\end{array}
\]</span></p>
<p>The number of parameters for this type of model is extremely large.</p>
</div>
<div id="algorithm-for-a-single-layer-with-matrix-dimension" class="section level2">
<h2>Algorithm for a single layer with matrix dimension</h2>
<p>The implementation will use the <a href="https://pytorch-lightning.readthedocs.io/en/latest/">Pytorch Lightning</a> library. The entire code is on <a href="https://github.com/Emmanuel-R8/Time_Series_Transformers">Github</a>.</p>
<pre class="python"><code>import sys
import inspect

from typing import *

import torch
import torch.nn as nn

from pytorch_lightning.core.lightning import LightningModule

from utils.exp_utils import logging</code></pre>
<pre class="python"><code># From https://github.com/harvardnlp/annotated-transformer
class PositionalEncoding(LightningModule):
    &quot;Implement the edited PE function, depends on sequence length rather than input dimensionnality.&quot;

    def __init__(self, runParam, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        batch

        # Compute the positional encodings once in log_2 space ceiled to sequence_length.
        b = math.ceil(math.log(max_sequence_length * 4, 2))
        a = int(2**b / 4)  # Up to a quarter of a sine wave
        x1 = np.array([[math.cos(0.5**i*x*2*math.pi) for x in range(max_sequence_length, 0, -1)] for i in range(1, b+1)])
        x2 = np.array([[math.sin(0.5**i*x*2*math.pi) for x in range(max_sequence_length, 0, -1)] for i in range(2, b+2)])
        x = np.concatenate([x1, x2], axis=0)
        print(&quot;x.shape():&quot;, x.shape)
        x = np.expand_dims(x, 0).repeat(repeats=batch_size, axis=0)
        print(&quot;x.shape():&quot;, x.shape)

        # Register it into PyTorch
        pe = torch.from_numpy(x).float()
        pe = pe.transpose(-1, -2)
        print(&quot;pe.size():&quot;, pe.size())
        self.register_buffer(&#39;pe&#39;, pe)

    def forward(self, x):
        pos = Variable(self.pe, requires_grad=False)
        # print(pos.size(), x.size())  # [batch_size, -1, sequence_length], [batch_size, sequence_length, hidden_size]
        pe = self.pe[:, -x.size(1):]  # limiting positional encoding to a poentially shorter sequence_length
        print(&quot;pe.size(), x.size():&quot;, pe.size(), x.size())
        x = torch.cat([x, pe], dim=-1)
        return self.dropout(x), pos</code></pre>
<p><span class="math display">\[
\begin{array}{l}
\text{Step for a} &amp; &amp; \text{Dimensions for multiple layers} \\
\text{single layer} &amp; &amp; \text{(with Einstein summation)} \\
\hline \\
h^{l-1}_\tau &amp; &amp; (l, h, s_s, d_{in}) \\
\hline \\
\tilde{h}^{l-1}_\tau = &amp; \left[ StopGradient(m^{l-1}_\tau) \circ  h^{l-1}_\tau \right]  &amp; (l, h, s, d_{in}) = (l, h, s_m, d_{in}) \circ (l, h, s_s, d_{in}) \\
\hline \\
q^l_\tau = &amp; h^{l-1}_\tau \cdot {W^l_Q}^\top &amp; (l, h, s_s, d_{in}) = (l, h, s_s, d_{in}) \dot {(l, h, d_{in}, d_{in})}^\top \\
&amp; &amp; lhsd_1,lhsd_1d_2 \rightarrow lhsd_2 \\
\hline \\
k^l_\tau = &amp; \tilde{h}^{l-1}_\tau \cdot {W^l_K}^\top &amp; (l, h, s, d_{in}) = (l, h, s, d_{in}) \dot {(l, h, d_{in}, d_{in})}^\top \\
&amp; &amp; lhsd_1,lhsd_1d_2 \rightarrow lhd_2 \\
\hline \\
v^l_\tau = &amp; \tilde{h}^{l-1}_\tau \cdot {W^l_V}^\top &amp; (l, h, s, d_{in}) = (l, h, s, d_{in}) \dot {(l, h, d_{in}, d_{in})}^\top \\
&amp; &amp; lhsd_1,lhsd_1d_2 \rightarrow lhd_2 \\
\hline \\
A^l_\tau = &amp; {Q^l}^\top \dot \\
\end{array}
\]</span></p>
</div>
<div id="notation-summary" class="section level2">
<h2>Notation summary</h2>
<p><strong>Size parameters</strong></p>
<p><span class="math display">\[
\begin{array}{ll}
l:        &amp;   \text{number of layers, starting from 1} \\
d:        &amp;   \text{dimension of all input (including embeddings) AND output vectors passed from a layer to the next} \\
n_{head}: &amp;   \text{number of self-attention heads in a TransformerXL layer} \\
n_s:      &amp;   \text{size of a training segment = number of inputs into and outputs from a layer} \\
s = n_s:  &amp;   s \text{ will be used as index name to represent the input dimension when using named tensors,}  \\
&amp; \text{i.e. Einstein summation.}  \\
b:        &amp;   \text{size of a training batch, i.e. number of segments in a batch. Used in named tensors as well} \\
\\
n_m:      &amp;   \text{number of hidden states kept in the primary memory. n_m is a multiple of n_s} \\
c:        &amp;   \text{memory compression ratio} \\
n_{cm}:   &amp;   \text{number of states kept in the compressed memory. n_{cm} }  \\
\end{array}
\]</span></p>
</div>
<div id="useful-references" class="section level1">
<h1>Useful references</h1>
<ul>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li>
<li><a href="https://www.fromkk.com/posts/the-annotated-the-annotated-transformer/">The Annotated The Annotated Transformer</a></li>
<li><a href="https://d2l.ai/chapter%5Fattention-mechanisms/transformer.html">Dive into Deep Learning - 10.3 Transformer</a></li>
</ul>
</div>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/machine-learning/" rel="tag">Machine Learning</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/neural-network/" rel="tag">Neural Network</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>



<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "https-emmanuel-r8-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2020 Back2Numbers.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>