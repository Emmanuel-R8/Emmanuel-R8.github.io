---
title: 'RNN Compressive Memory Part 3: The Compression algorithm'
author: Emmanuel Rialland
date: '2020-12-31'
#publishdate: '2030-01-03'
#slug: rnn-compressive-memory-part-3
categories:
  - Machine Learning
  - Neural Network
tags:
  - Machine Learning
  - Neural Network  
mathjax: true
description: ''
thumbnail: ''
draft: true

output:
  blogdown::html_page:
    toc: true
---

This is part 3 of a discussion of the DeepMind paper about [Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507).


- [Part 1](../07/rnn-compressive-memory-part-1.html): A high level introduction to Compressive Memory mechanics  starting from basic RNNS;

- Part 2 (here): a detailed explanation of the TransformerXL;

- Part 3:  an implementation using PyTorch;

- Part 4: finally, its application to time series.


This post describes the building blocks up to what makes a [TransformerXL](https://arxiv.org/abs/1901.02860) model as presented in June 2019. The compression mechanics are an add-on to the TransformerXL which therefore need to be detailed. Along the way, it will become clear that, in many ways, the implementation described only applies to embeddings, i.e. one-hot encodings, and not to to truly multi-dimensional variable inputs.


## The TransformerXL

### Background 

The previous part introduced a chronological introduction starting with RNNs. However, it would have been more accurate to present those models as Sequential Neural Network: They are concerned with dealing with sequential pieces of information (individual elements) and capturing their respective interactions. _Recurrent_ networks are but one type of sequential networks.

The first generation Recurrent Neural Networks passes contextual information from one sequential unit to the next within a layer. A unit does not see anything past its preceding neighbour. Bidirectional networks do not feed a unit with information coming from both sides within a single layer, they are a combination of two layers going in opposite directions. RNNs do not capture information as a whole. 

The addition of _attention_ is the first mechanism that captured the entire sequence. First, within an RNN layer, each unit receives an input from the preceding layer (or a training segment). Then, an attention layer (an _attention head_) is fed with each and every output produced by the RNN layer. Each hidden state is weighted by trained parameters and the attention layer produces 
Several attention heads can be trained in parallel and combine their results using trained parameters into a final output.

The number of parameters for this type of model is extremely large.  



## Compressive Memory

### Background

From the last post, recall that compressive transformer are an extension to the TransformerXL. TransformerXL trains using segments extracted from all the sequences available in the dataset. It also uses hidden states calculated when training prior segments. In a sense, it seeks to indirectly increase the size of the training segments. Naturally, the model stores past hidden states (past memories) in a buffer of limited size. It has to discard discarding the oldest memories from that primary memory when moving from a segment to the next. 

The key addition of the compressive model is to avoid immediately the discarding by compressing those past states in a compressed representation (which itself will eventually be discarded). The training needs to cover the original TransformerXL and the quality of the compression mechanics. When training a typical network, the values of all the parameters are simultaneously optimised using the gradients calculated for each of those parameters. Instead, this model is trained as 2 separate models:

- The Transformer-XL layers are trained using a current segment, and the primary and compressed memories to train the cells' parameters (and only those!). During that stage, the compressed memory parameters is given as a constant: no gradients are calculated. Conceptually, this is the normal TransformerXL where the inputs are the concatenation of the segment embeddings and all the memories (primary and compressed).

- _Separately_, another training process looks for an optimal compressed representation of past memories that would otherwise be discarded. Training the compression mechanism assumes that the transformer cells' parameters are fixed and constant. This stage is only concerned with the efficiency of the representation. Note that certain compression mechanics, like pooling, have no parameters to optimise (although some hyperparameters might be involved).


### Model overall structure

The authors describe the overall algorithm at a high level. We will retain the same notations, which are summarised at the end of of the post, but we will use a slightly different vocabulary. In particular, a _training set_ (for example a full text) is split into ordered _sequences_ that for a conceptual whole (a full sentence). Each sequence is fed into the model which has a limited number of inputs. If the size of a sequence is too large, it is split into _segments_. The size of a segment is the number of input of the model. The key attraction of the compressive memory is to lengthen the memory of a TransformerXL: for an identical computation budget, the paper shows that it is valuable to shorten the segments (fewer self-attention heads), in exchange for keeping longer compressed past memories (the self-attention heads become larger to give attention to compressed memories). 


![__Compressive TransformerXL__](/post/2020-03-RNN-compressive-memory/Compressive-template.png)

To complete the notation, we use the following form of indices: $h^{(l), v}_t$ which should be read as $h^{(\text{Layer index}), \text{Segment index}}_{\text{Time step}}$. In addition, $h^{(1)}$ represents the inputs into the first layer, i.e. equals the current training segment $x$.

(a) The model starts with transformer layers ($l$ layers). (see step (a) on the diagram).

(b) Each layer takes a number of entries (a segment of $n_s$ inputs), each being a vector of dimension $d$. Note that $d$ is the dimension of the initial input embeddings, and of all input/output vectors of further layers. This could however be a flexible model parameter.

(c) Each individual transformer layer has an identical structure with the same number of self-attention heads ($n_{head}$ heads). Recall that an attention head is a triplet of matrices $(W^{(i)}_q, W^{(i)}_k, W^{(i)}_v)$ and a multi-layer perceptron. ^[We only consider a wide self-attention model, in part having in mind the application to time series where we will work with embeddings of smaller size. We therefore ignore the narrow self-attention model variant.] 

(d) At each iteration, the self-attention heads are trained using the outputs from the previous layer, and primary and compressed memories.

(e) After an input is processed by a layer, it is pushed into memory. The memory size $n_m$ has the capacity to store a number $n_{slots}$ of segments. Therefore  $n_m = n_{slots} \times n_s$.

(f) The new memory pushes the oldest one out, which is compressed into a compressed memory with a $c$ compression ratio. the compressed memory can hold $n_{old}$ compressed memories. $n_{cm}$ is a multiple of $n_{old}$.


## Memory compression

Let's focus on Step (f). The compression mechanics are optimised independently from the TransformerXL layers and require choosing a compression scheme (function $f_c$) and, if there are parameters to be trained, a loss function to minimise.

### Compression function

The authors reviewed several choices for the function $f^{(i)}_c$.

#### Pooling

The simplest is _Max and/or mean pooling_, where the kernel and stride is set to the compression rate c. This is fast, requires no training (no parameters) and was used as a simple baseline.

#### Convolution

_1D-convolution_ also with kernel and stride set to c. This is defined by parameters which require training.

#### Dilated convolution

_Dilated convolutions_ which were proposed in 2016 as an alternative to traditional image convolution (for image classification) when dealing with semantic networks. See [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122). In essence, dilated convolutions are large scale convolutions (large kernel) defined with the same number of parameters as a small kernel. They are better suited to capture distant relationships present in semantic networks.

#### Most-used compression

_Most-used_ compression where the memories are sorted by their average attention(usage) and the most-used are preserved. The most-used compression scheme is inspired from the garbage collection mechanism in the Differentiable Neural Computer (_DNC_)  where low-usage memories are erased. This is described in the 2016 DeepMind paper [Hybrid computing using a neural network with dynamic external memory](https://www.gwern.net/docs/rl/2016-graves.pdf). A DNC is network augmented with memory, where a controller can read and write memory banks. The DNC article is very detailed on the technical aspects. We understand that most-used algorithm is presented in appendix under the _Dynamic memory allocation_ subsection.

#### Auto-encoders

As we understand, the authors did not consider auto-encoding networks with $c$ fewer parameters. It might be that they were considered but rejected. 

#### Compression ratio

If the compression ratio is $c$, what are the dimensions of the compressed memory tensors? 

The paper leaves this unspecified. 

- Pooling and 1D-convolution will produce tensors where the dimension of the attention heads (along the $d$ axis) are divided by $c$.

- Our understanding of the most-used algorithm would be to zero all values save for a $1 / c$ proportion, but retaining identical dimensions. 

- However, we do not see $c$ memory attention heads compressed into a single vector (compression along the $n_s$ axis).


### Loss function

Several loss functions were considered.

1. A whole-model optimisation where all parameters (compression mechanics + TransformerXL Layers) are trained at once using the training set error.  However they found the size overwhelming, especially when old compressed memories were unrolled into full-form memories. They then used _backpropagating-through-time_ (BPTT), which by definition does not do full-model optimisation to avoid vanishing/exploding gradients. See [BPTT](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html) for a good overview. Note that this is not what we introduced as a two-step optimisation.

2. The compression is a form of representation learning / auto-encoding which optimises an encoder (presumably $c$ fewer parameters) by simultaneously training a decoder. The paper tried a $\mathcal{L}_2$ (square) error on the ability to reconstruct the memory tensors. It does not go into detail aside from mentioning it. The posts will not explore this.

3. The paper goes into more detail about the two-step training of an _attention-reconstruction loss_. Transformers optimise attention parameters (the attention head matrices). Optimising for the ability of retaining that information makes complete sense. This loss function is the $\mathcal{L}_2$ error between the attention matrix of a memory slot and reconstructed attention. Next section focuses on the loss function. 


### Model timing

Let's be clearer about the details of what takes place as time goes by (as segments are presented) and how the compression optimisation proceeds (at the risk of being repetitive). 

- The set of the model's state vectors (the $h^{(i)}$) is the model's reaction to a particular input. They are an immediate reaction to an current input. They reflect the values of the current attention heads' parameters.

- A memory slot is a set of a layer's state vectors created in the past for a past input when the model's parameters were different.^[We ignore the possible impact of training in batches.] It is the past model's reaction to something that happened in the past (the input at that time).

- That memory slot reflects the experience of the model at that time (the attention heads parameters). Since then, the model has been trained further. The recent updated model would have yielded a different set of state vectors for that past input.

- Attention heads are not state vectors. They are functions to be trained.^[We ignore the multi-perceptrons.] Those functions, for any input state vectors, will produce a different answer, a different set of state vectors. 

- Attention heads are trained and after training they will eventually be constants. However, the the state vectors will change as and when the inputs do.

- The compression is also a function that will be frozen after training which will use (if it relies on parameters).

Those precisions are relevant because a compression takes place now (i.e. at the time of a current segment). It is applied to a past memory which reflects a past reaction to a past input with a past model. We optimise compression parameters _now_ using values and parameters that are all from the past.

Optimising for attention reconstruction means: 

- Right now, the model gives a particular set of current state vectors for a given current input.

- This model has a past memory that is about to be lost (as in a normal TransformerXL).

- In this model, the old memory is about to be compressed. If compressed, the original memory could be reconstructed. 

- If the compression (and reconstruction) algorithm is perfect, it should not matter whether the compressed memory (after reconstruction) or the old memory is used. 

The optimisation is therefore to minimise the difference what the model does with the past memory (not yet discarded) and what would happen if the past memory was reconstructed from its compressed form. 

The difference between the 2 situations is the loss function to be minimised. Transformers are about attention; the compression algorithm is judged by its ability to retain past attention (what was worthy of attention in the past).

As the paper indicates, this is a lossy objective. The reconstruction is not optimised to replicate the old memory. It is instead optimised to reconstruct an _alternative memory_ that gives a better reconstruction of the attentions. The paper asserts that this gives superior results to the reconstruction of the memory. Keeping in mind that the ultimate purpose of the model is to optimise self-attentions, possible explanations are:

- Memories are used as input into the attention heads to determine the self-attention. So will a reconstructed memory. Therefore, primary memory reconstruction errors are then compounded with the modelling errors of the attention heads' matrices. Skipping the intermediary avoids this.

- Maybe more interesting, the past memory is mechanically imperfect: it is based on old model parameters. Optimising the reconstruction to reproduce an imperfect memory implicitly means incorporating attention heads' parameters that were used at the time the memory was created. The reconstructed memory includes the errors of the past attention heads which have since been reduced (hopefully). In other words, the reconstructed memory is an _alternative memory_ which might include later corrections to the model attention heads.
 

### Attention-Reconstruction Loss

For sake of completeness, we include the _attention-reconstruction loss_ algorithm detailed in the paper (_Algorithm 2_ on page 4).  ^[Forgive the algorithm formatting. R markdown resisted naive efforts to use Latex algorithms.]. 

Its overall organisation is independent from the actual loss calculation (the stop-gradient mechanics), therefore remains of value. It is replicated here with slight modifications for type consistency. Note that at Step 6 the published algorithm uses a $\sigma$. This could suggest that a sigmoid activation is used, but it would make no sense to compare attentions calculated with different activation functions ($softmax$ in the layers vs. $sigmoid$ in the the reconstruction).


$$
\begin{array}{ll}
& \textit{Reset the attention loss} \\
& \hline \\
1: & \mathcal{L} \leftarrow 0 \\
\\
& \textit{Main Loop} \\
& \hline \\
& \text{Now that the loss is reset, loop through the layers, one at a time} \\
2: & \textit{for layer }i = 1, 2, \cdots , l \textit{ do} \\
\\
& \textit{Stop all gradients for anything but the compression function parameters} \\
& \hline \\
& \text{TensorFlow uses stop_gradient()} \\
& \text{PyTorch uses .detach()} \\
& \text{We'll use DetachGradient() to make both happy!} \\
3:  &  DetachGradient(h^{(i)}) \\
4:  &  DetachGradient(M^{(i)}_{t-n_{slots}}) \text{ --- oldest Primary memory slot about to be discarded}  \\
5:  &  DetachGradient(Q, K, V) \text{ --- parameters of all the attention heads matrices}  \\
\\
& \textit{Define the content-based attention being the same calculation as inside the layers}\\
& \hline \\
6: &   Attention(h, m) \leftarrow softmax((hQ).(mK))(mV)  \\
\\
& \textit{Calculate the new compressed memory}\\
& \hline \\
7: &   CM^{(i)}_{1} \leftarrow f^{(i)}_c(M^{(i)}_{t-n_{slots}})   \\
\\
& \textit{Calculate a reconstructed attention from the proposed compressed memory}\\
& \hline \\
7: &   R^{(i)} \leftarrow Reconstruction(CM^{(i)}_{1})  \\
\\
& \textit{Update the loss for the loss at the current layer}\\
& \hline \\
8: &   \mathcal{L} \leftarrow \mathcal{L} + \left| Attention(h^{(i)}, M^{(i)}_{t-n_{slots}}) - Attention(h^{(i)}, R^{(i)}) \right|_2 \\
\\
& \hline \\
& \textit{End loop} \\
& \hline \\
\end{array}
$$


## Conclusion

This post presents the _Compressive Transformers_ in more details, including the articulation between training the attention heads and training the compression mechanics. 

To come in part 3, the code.


## Notation summary

__Size parameters__

$$
\begin{array}{ll}
l:        &   \text{number of layers, starting from 1} \\
d:        &   \text{dimension of all input (including embeddings) AND output vectors passed from a layer to the next} \\
n_{head}: &   \text{number of self-attention heads in a TransformerXL layer} \\
n_s:      &   \text{size of a training segment = number of inputs into and outputs from a layer} \\
s = n_s:  &   s \text{ will be used as index name to represent the input dimension when using named tensors,}  \\
& \text{i.e. Einstein summation.}  \\
b:        &   \text{size of a training batch, i.e. number of segments in a batch. Used in named tensors as well} \\
\\
n_m:      &   \text{number of hidden states kept in the primary memory. n_m is a multiple of n_s} \\
c:        &   \text{memory compression ratio} \\
n_{cm}:   &   \text{number of states kept in the compressed memory. n_{cm} }  \\
\end{array}
$$



