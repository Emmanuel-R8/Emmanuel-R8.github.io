---
title: 'RNN Compressive Memory Part 2: The TransformerXL'
author: Emmanuel Rialland
date: '2020-12-30' 
#publishdate: '2030-01-02' 
#slug: rnn-compressive-memory-part-2
categories:
  - Machine Learning
  - Neural Network
tags:
  - Machine Learning
  - Neural Network
mathjax: true
description: ''
thumbnail: ''
draft: true

output:
  blogdown::html_page:
    toc: true
---
 
This is part 2 of a discussion of the DeepMind paper about [Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507).


- [Part 1](../../03/07/rnn-compressive-memory-part-1.html): A high level introduction to Compressive Memory mechanics  starting from basic RNNS;

- Part 2 (here): more details about the TransformerXL;

- Part 3:  an implementation using PyTorch (soon);

- Part 4: finally, its application to time series (soon).

This post describes the building blocks up to what makes a [TransformerXL](https://arxiv.org/abs/1901.02860) model as presented in June 2019. The compression mechanics are an add-on to the TransformerXL which therefore need to be detailed. Along the way, it will become clear that, in many ways, the implementation described only applies to embeddings, i.e. one-hot encodings, and not to to truly multi-dimensional variable inputs.


## The TransformerXL


$$
\begin{array}{l}
\text{Notation:} & \\
\tau & \text{time step or index on successive segments} \\
b & \text{batch size} \\
h & \text{number of heads} \\
s_s & \text{length of a segment} \\
s_m & \text{length of the memory} \\
s = s_m + s_s & \text{length of segment + memory} \\
d & \text{size of an embedding} \\
d_{in} & \text{size of the embedding as input to a given layer} \\
d_{out} & \text{size of the embedding as output from a given layer} \\
(b, h, s, d): & \text{tensor of 4 dimension reflecting those dimensions}\\
l & \text{index of a layer} \\
\end{array}
$$

The number of parameters for this type of model is extremely large.  

## Algorithm for a single layer with matrix dimension

The implementation will use the [Pytorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/) library. The entire code is on [Github](https://github.com/Emmanuel-R8/Time_Series_Transformers).

```{python,eval=FALSE}
import sys
import inspect

from typing import *

import torch
import torch.nn as nn

from pytorch_lightning.core.lightning import LightningModule

from utils.exp_utils import logging
```



```{python,eval=FALSE}
# From https://github.com/harvardnlp/annotated-transformer
class PositionalEncoding(LightningModule):
    "Implement the edited PE function, depends on sequence length rather than input dimensionnality."

    def __init__(self, runParam, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        batch

        # Compute the positional encodings once in log_2 space ceiled to sequence_length.
        b = math.ceil(math.log(max_sequence_length * 4, 2))
        a = int(2**b / 4)  # Up to a quarter of a sine wave
        x1 = np.array([[math.cos(0.5**i*x*2*math.pi) for x in range(max_sequence_length, 0, -1)] for i in range(1, b+1)])
        x2 = np.array([[math.sin(0.5**i*x*2*math.pi) for x in range(max_sequence_length, 0, -1)] for i in range(2, b+2)])
        x = np.concatenate([x1, x2], axis=0)
        print("x.shape():", x.shape)
        x = np.expand_dims(x, 0).repeat(repeats=batch_size, axis=0)
        print("x.shape():", x.shape)

        # Register it into PyTorch
        pe = torch.from_numpy(x).float()
        pe = pe.transpose(-1, -2)
        print("pe.size():", pe.size())
        self.register_buffer('pe', pe)

    def forward(self, x):
        pos = Variable(self.pe, requires_grad=False)
        # print(pos.size(), x.size())  # [batch_size, -1, sequence_length], [batch_size, sequence_length, hidden_size]
        pe = self.pe[:, -x.size(1):]  # limiting positional encoding to a poentially shorter sequence_length
        print("pe.size(), x.size():", pe.size(), x.size())
        x = torch.cat([x, pe], dim=-1)
        return self.dropout(x), pos
```


$$
\begin{array}{l}
\text{Step for a} & & \text{Dimensions for multiple layers} \\
\text{single layer} & & \text{(with Einstein summation)} \\
\hline \\
h^{l-1}_\tau & & (l, h, s_s, d_{in}) \\
\hline \\
\tilde{h}^{l-1}_\tau = & \left[ StopGradient(m^{l-1}_\tau) \circ  h^{l-1}_\tau \right]  & (l, h, s, d_{in}) = (l, h, s_m, d_{in}) \circ (l, h, s_s, d_{in}) \\
\hline \\
q^l_\tau = & h^{l-1}_\tau \cdot {W^l_Q}^\top & (l, h, s_s, d_{in}) = (l, h, s_s, d_{in}) \dot {(l, h, d_{in}, d_{in})}^\top \\
& & lhsd_1,lhsd_1d_2 \rightarrow lhsd_2 \\
\hline \\
k^l_\tau = & \tilde{h}^{l-1}_\tau \cdot {W^l_K}^\top & (l, h, s, d_{in}) = (l, h, s, d_{in}) \dot {(l, h, d_{in}, d_{in})}^\top \\
& & lhsd_1,lhsd_1d_2 \rightarrow lhd_2 \\
\hline \\
v^l_\tau = & \tilde{h}^{l-1}_\tau \cdot {W^l_V}^\top & (l, h, s, d_{in}) = (l, h, s, d_{in}) \dot {(l, h, d_{in}, d_{in})}^\top \\
& & lhsd_1,lhsd_1d_2 \rightarrow lhd_2 \\
\hline \\
A^l_\tau = & {Q^l}^\top \dot \\
\end{array}
$$






## Notation summary

__Size parameters__

$$
\begin{array}{ll}
l:        &   \text{number of layers, starting from 1} \\
d:        &   \text{dimension of all input (including embeddings) AND output vectors passed from a layer to the next} \\
n_{head}: &   \text{number of self-attention heads in a TransformerXL layer} \\
n_s:      &   \text{size of a training segment = number of inputs into and outputs from a layer} \\
s = n_s:  &   s \text{ will be used as index name to represent the input dimension when using named tensors,}  \\
& \text{i.e. Einstein summation.}  \\
b:        &   \text{size of a training batch, i.e. number of segments in a batch. Used in named tensors as well} \\
\\
n_m:      &   \text{number of hidden states kept in the primary memory. n_m is a multiple of n_s} \\
c:        &   \text{memory compression ratio} \\
n_{cm}:   &   \text{number of states kept in the compressed memory. n_{cm} }  \\
\end{array}
$$



# Useful references

* [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
* [The Annotated The Annotated Transformer](https://www.fromkk.com/posts/the-annotated-the-annotated-transformer/)
* [Dive into Deep Learning - 10.3 Transformer](https://d2l.ai/chapter%5Fattention-mechanisms/transformer.html)

