[{"authors":["admin"],"categories":null,"content":"I am a corporate banker with two decades experience and successful machine learning / data science project track record. It has been noted that my career has not been very linear\u0026hellip;\nMy education was completely scientific with a couple of engineering degrees with a particular passion for mathematics. Add to that a life-long fascination for artificial intelligence. I can trace that back to reading a particular birthday present: Gödel, Escher and Bach. This book held me riveted at 17.\nBUT….\nBut travelling and discovering the planet was a powerful magnet that I could not resist and it turned out that banking and finance first gave me that opportunity. Followed about 20 years doing, and enjoying, practising that trade in a few countries. And now time has come to go back my roots.\nI am available for consulting and, if there is a great fit, something more permanent. Get in touch and let us have a chat. 5 minutes is all it takes.\nYou can find more about me on my LinkedIn profile.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://emmanuel-r8.github.io/author/emmanuel-rialland/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/emmanuel-rialland/","section":"authors","summary":"I am a corporate banker with two decades experience and successful machine learning / data science project track record. It has been noted that my career has not been very linear\u0026hellip;","tags":null,"title":"Emmanuel Rialland","type":"authors"},{"authors":null,"categories":["miscellaneous"],"content":"  I moved to a richer Hugo template, partly to move things around under the hood, but importantly it gives a sounder platform for the future.\nHowever, it took many hours of frustration to get blogdown and the template to nicely render \\(\\LaTeX\\) formulas. In the end, it was very simple, although no documentation or blog posts helped: the mathjax: true YAML header option needs to be changed to math: true. No need to alternate between .Rmd or .md or .Rmarkdown files, mess around with config.toml or params.toml, chase down unknown pandoc binaries or add new partials snippets.\nIn addition, I finally figured out how to automatically generate table of contents. Insert the following snippet in the file header:\noutput: blogdown::html_page: toc: true ","date":1597190400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597190400,"objectID":"ace2ef1e62f583376d7ed5c499f7a137","permalink":"https://emmanuel-r8.github.io/post/2020/08/12/miscellaneous/","publishdate":"2020-08-12T00:00:00Z","relpermalink":"/post/2020/08/12/miscellaneous/","section":"post","summary":"I moved to a richer Hugo template, partly to move things around under the hood, but importantly it gives a sounder platform for the future.\nHowever, it took many hours of frustration to get blogdown and the template to nicely render \\(\\LaTeX\\) formulas.","tags":["miscellaneous"],"title":"Change of template","type":"post"},{"authors":["Emmanuel Rialland"],"categories":null,"content":"","date":1588249047,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588249047,"objectID":"72bcd6481885ebb5cc65fad988bf45fc","permalink":"https://emmanuel-r8.github.io/talk/hkml202004/","publishdate":"2020-04-30T20:17:27+08:00","relpermalink":"/talk/hkml202004/","section":"talk","summary":"Recent talk to the Hong Kong Machine Learning Meetup about the Julia programming language and an example COVID-19 model optimisation","tags":[],"title":"An Introduction to Julia","type":"talk"},{"authors":null,"categories":["Julia","Machine Learning"],"content":"  I recently made a presentation at the regular Hong Kong Machine Learning meetup organised by Gautier Marti.\nThe presentation was an introduction to Julia and used as an example a SEIR model COVID-19 I had written. The presentation is available on Github.\nIt seems to have had some effect!\n","date":1588204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588204800,"objectID":"996b8ff0eee86047937dec51d86c03f2","permalink":"https://emmanuel-r8.github.io/post/2020/04/30/presentation-at-the-hong-kong-machine-learning-meetup/","publishdate":"2020-04-30T00:00:00Z","relpermalink":"/post/2020/04/30/presentation-at-the-hong-kong-machine-learning-meetup/","section":"post","summary":"I recently made a presentation at the regular Hong Kong Machine Learning meetup organised by Gautier Marti.\nThe presentation was an introduction to Julia and used as an example a SEIR model COVID-19 I had written.","tags":["Julia","Machine Learning"],"title":"Presentation at the Kong Kong Machine Learning meetup","type":"post"},{"authors":null,"categories":["Data Science","Julia"],"content":"   The Neherlab COVID-19 forecast model Basic assumptions  Overview Age cohorts Severity Seasonality Transmission reduction Details of the model  Population compartments Model parameters Infection After infection   Load data Initialise parameters  Fixed constants Infrastructure Parameter vector Population Parameters vector  Differential equation solver Bilibliography   The Neherlab COVID-19 forecast model using CSV, Dates; using DataFrames, DataFramesMeta; using Plots, PyPlot; using DifferentialEquations; This is more a data science post than machine learning. It was born after reading a report from Imperial College London and finding a forecasting model by NeherLab. The numbers produced by those models can only be described as terrifying.\nHow do those models work? How are they calibrated?\nBUT\nRemember that whatever concerns one can have about their precision, those models are all absolutely clear that social-distancing, quarantining have a massive impact on death rates. Being careful saves lives. If anybody feels like ignoring those precautions out of excess testosterone, they are at risk of killing others.\nThis post started from one of the pages of the NeherLab site describing their methodology. The work that team is achieving deserves more credit than I can give them.\nThe NeherLab website, including the model, is entirely written in Javascript. This is difficul to understand and audit.\n Basic assumptions WARNING: This is not an introduction to SEIR (and variant) compartment modelling of epidemies. For an introduction (difficult to avoid the maths), see a presentation by the Swiss Tropical and Public Health Institute. Wikipedia is always an option.\nOverview The model works as follows:\n susceptible individuals are exposed/infected through contact with infectious individuals. Each infectious individual causes on average \\(R_0\\) secondary infections while they are infectious.  Transmissibility of the virus could have seasonal variation which is parameterized with the parameter “seasonal forcing” (amplitude) and “peak month” (month of most efficient transmission).\n exposed individuals progress to a symptomatic/infectious state after an average latency\n infectious individuals recover or progress to severe disease. The ratio of recovery to severe progression depends on age\n severely sick individuals either recover or deteriorate and turn critical. Again, this depends on the age\n critically ill individuals either return to regular hospital or die. Again, this depends on the age\n  The individual parameters of the model can be changed to allow exploration of different scenarios.\n Age cohorts COVID-19 is much more severe in the elderly and proportion of elderly in a community is therefore an important determinant of the overall burden on the health care system and the death toll. We collected age distributions for many countries from data provided by the UN and make those available as input parameters. Furthermore, we use data provided by the epidemiology group by the Chinese CDC to estimate the fraction of severe and fatal cases by age group.\n Severity The basic model deals with 3 levels of severity: slow, moderate and fast transmissions.\n# severityLevel = :slow; severityLevel = :moderate; # severityLevel = :fast;  Seasonality Many respiratory viruses such as influenza, common cold viruses (including other coronaviruses) have a pronounced seasonal variation in incidence which is in part driven by climate variation through the year. We model this seasonal variation using a sinusoidal function with an annual period. This is a simplistic way to capture seasonality. Furthermore, we don’t know yet how seasonality will affect COVID-19 transmission.\n# Northern or southern hemisphere latitude = :north; # latitude = :tropical; # latitude = :south; # The time unit is days (as floating point) # Day 0 is taken at 1 March 2020 BASE_DATE = Date(2020, 3, 1); BASE_DAYS = 0; function date2days(d) return convert(Float64, datetime2rata(d) - datetime2rata(BASE_DATE)) end; function days2date(d) return BASE_DATE + Day(d) end;  # Default values for R_0 baseR₀ = Dict( (:north, :slow) =\u0026gt; 2.2, (:north, :moderate) =\u0026gt; 2.7, (:north, :fast) =\u0026gt; 3.2, (:tropical, :slow) =\u0026gt; 2.0, (:tropical, :moderate) =\u0026gt; 2.5, (:tropical, :fast) =\u0026gt; 3.0, (:south, :slow) =\u0026gt; 2.2, (:south, :moderate) =\u0026gt; 2.7, (:south, :fast) =\u0026gt; 3.2);  # Peak date peakDate = Dict( :north =\u0026gt; date2days(Date(2020, 1, 1)), :tropical =\u0026gt; date2days(Date(2020, 1, 1)), # although no impact :south =\u0026gt; date2days(Date(2020, 7, 1))); # Seasonal forcing parameter \\epsilon ϵ = Dict( (:north, :slow) =\u0026gt; 0.2, (:north, :moderate) =\u0026gt; 0.2, (:north, :fast) =\u0026gt; 0.1, (:tropical, :slow) =\u0026gt; 0.0, (:tropical, :moderate) =\u0026gt; 0.0, (:tropical, :fast) =\u0026gt; 0.0, (:south, :slow) =\u0026gt; 0.2, (:south, :moderate) =\u0026gt; 0.2, (:south, :fast) =\u0026gt; 0.1); # Gives R_0 at a given date function R₀(d; r_0 = missing, latitude = :north, severity = :moderate) if ismissing(r_0) r₀ = baseR₀[(latitude, severity)] else r₀ = r_0 end eps = ϵ[(latitude, severity)] peak = peakDate[latitude] return r₀ * (1 + eps * cos(2.0 * π * (d - peak) / 365.25)) end;  Transmission reduction The tool allows one to explore temporal variation in the reduction of transmission by infection control measures. This is implemented as a curve through time that can be dragged by the mouse to modify the assumed transmission. The curve is read out and used to change the transmission relative to the base line parameters for \\(R_0\\) and seasonality. Several studies attempt to estimate the effect of different aspects of social distancing and infection control on the rate of transmission. A report by Wang et al estimates a step-wise reduction of \\(R_0\\) from above three to around 1 and then to around 0.3 due to successive measures implemented in Wuhan. This study investigates the effect of school closures on influenza transmission.\nThis curve is presented as a list of tuples: (days from start date, ratio). The month starts from the start date. Between dates, the ration is interpolated linearly. After the last date, the ration remains constant.\nstartDate = date2days(Date(2020, 3, 1)); mitigationRatio = [(0, 1.00), (30, 0.80), (60, 0.20), (150, 0.50)]; function getCurrentRatio(d; start = BASE_DAYS, schedule = mitigationRatio) l = length(schedule) # If l = 1, ratio will be the only one if l == 1 return schedule[1][2] else for i in 2:l d1 = schedule[i-1][1] d2 = schedule[i ][1] if d \u0026lt; d2 deltaR = schedule[i][2] - schedule[i-1][2] return schedule[i-1][2] + deltaR * (d - d1) / (d2 - d1) end end # Last possible choice return schedule[l][2] end end;  Details of the model Age strongly influences an individual’s response to the virus. The general population is sub-divided in to age classes, indexed by \\(a\\), to allow for variable transition rates dependent upon age.\n# The population will be modeled as a single vector. # The vector will be a stack of several vectors, each of them represents a compartment. # Each compartment vector has a size $nAgeGroup$ representing each age group. # The compartments are: S, E, I, H, C, R, D, K, L # We also track the hospital bed usage BED and ICU # Population to compartments function Pop2Comp(P) # To make copy/paste less prone to error g = 0 S = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1 E = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1 I = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1 J = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1 H = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1 C = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1 R = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1 D = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1 K = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1 L = P[ g*nAgeGroup + 1: (g+1)*nAgeGroup]; g += 1 BED = P[ g*nAgeGroup + 1: g*nAgeGroup + 1] ICU = P[ g*nAgeGroup + 2: g*nAgeGroup + 2] return S, E, I, J, H, C, R, D, K, L, BED, ICU end; Population compartments Qualitatively, the epidemy model dynamics tracks several sub-groups (compartments):\n Susceptible individuals (\\(S\\)) are healthy and susceptible to being exposed to the virus by contact with an infected individual.\n Exposed individuals (\\(E\\)) are infected but asymptomatic. They progress towards a symptomatic state on average time \\(t_l\\). Reports are that asymptomatic individuals are contagious. We will assume that they are proportionally less contagious than symptomatic individuals as a percentage \\(\\gamma_E\\) of \\(R_0\\). For the purposes of modelling we will assume (without supporting evidence, but will be the object of parameter estimation):\n  γₑ = 0.50;  Infected individuals (\\(I\\)) infect an average of \\(R_0\\) secondary infections. On a time-scale of \\(t_i\\), infected individuals either recover or progress towards severe infection.  From here on, the compartments differ from the NeherLab model is that we split compartments depending on the severity of the symptoms (Severe or Critical) and the location of the individual (out of the hospital infrastructure, isolated in hospital, or isolated in intensive care units). The transitions reflect the following assumptions:\n Transition between locations is purely a function of bed availability: as soon as beds are available, they are filled by all age groups in their respective proportions.\n Transition from severe to critical is assumed to be independent from the location of the patient. For severe patients, the relevance of the location is whether they are isolated or not, that is the possibility to infect susceptible individual. The same way an asymptomatic individual’s attracts a ratio \\(\\gamma_e\\), the other compartments will. The transition from \\(J\\) and \\(H\\) to recovery or criticality has a time-scale of \\(t_h\\).\n  # R_0 multipliers depending on severity. Subscript matches the compartment\u0026#39;s name. # Infected / symptomatic individuals γᵢ=1.0; # Severe symptoms γⱼ=1.0; # Critical symptoms γₖ = 2.0;  Once critical, the location of a patient influences their chances of recovery. Although we will assume that the time to recovery is identical in all cases, we will assume that the risks will double and triple if a patient is in simple isolation (receiving care but without ICU equipmment) or out of hospital.  # Fatality mulitplier. # In ICU δᵤ = 1.0; # In hospital δₗ = 2.0; # Out of hospital δₖ = 3.0;  The time-scale to recovery (\\(R\\)) or death (\\(D\\)) is \\(t_u\\).\n Recovering and recovered individuals [\\(R\\)] can not be infected again. We will assume that recovering individual are not contagious (no medical experience for this assumption for recovering individual).\n   Model parameters  Many estimates of \\(R_0\\) are in the range of 2-3 with some estimates pointing to considerably higher values. The serial interval, that is the time between subsequent infections in a transmission chain, was estimated to be 7-8 days. The China CDC compiled extensive data on severity and fatality of more than 40 thousand confirmed cases. In addition, we assume that a substantial fraction of infections, especially in the young, go unreported. This is encoded in the columns “Confirmed [% of total]”. Seasonal variation in transmission is common for many respiratory viruses but the strength of seasonal forcing for COVID19 are uncertain. For more information, see a study by us and by Kissler et al. The parameters of this model fall into three categories: transition time scales, age-specfic parameters and a time-dependent infection rate.  Transition time scales The time scales of transition from a compartment to the next: \\(t_l\\), \\(t_i\\), \\(t_h\\), \\(t_c\\).\n \\(t_l\\): latency time from infection to infectiousness \\(t_i\\): the time an individual is infectious after which he/she either recovers or falls severely ill \\(t_h\\): the time a sick person recovers or deteriorates into a critical state \\(t_u\\): the time a person remains critical before dying or stabilizing (Neherlab uses \\(t_c\\) instead of \\(t_u\\))  # Time to infectiousness (written t\\_l) tₗ = Dict( :slow =\u0026gt; 5.0, :moderate =\u0026gt; 5.0, :fast =\u0026gt; 4.0); # Time to infectiousness (written t\\_i) tᵢ = Dict( :slow =\u0026gt; 3.0, :moderate =\u0026gt; 3.0, :fast =\u0026gt; 3.0); # Time in hospital bed (not ICU) tₕ = 4.0; # Time in ICU tᵤ = 14.0;  Age-specfic parameters The age-specific parameters \\(z_a\\), \\(m_a\\), \\(c_a\\) and \\(f_a\\) that determine relative rates of different outcomes.\n \\(z_a\\): a set of numbers reflecting to which extent an age group is susceptible to initial contagion. Note that NeherLab denotes this vector by \\(I_a\\) which is confusing with the compartmment evolution \\(I_a(t)\\) notation. (This sort of defeats the purpose of \\(R_0\\).) \\(m_a\\): fraction of infectious becoming severe (Hospitalisation Rate) or recovers immediately (Recovery Rate) \\(c_a\\): fraction of severe cases that turn critical (Critical Rate) or can leave hospital (Discharge Rate) \\(f_a\\): fraction of critical cases that are fatal (Death Rate) or recover (Stabilisation Rate)  AgeGroup = [\u0026quot;0-9\u0026quot;, \u0026quot;10-19\u0026quot;, \u0026quot;20-29\u0026quot;, \u0026quot;30-39\u0026quot;, \u0026quot;40-49\u0026quot;, \u0026quot;50-59\u0026quot;, \u0026quot;60-69\u0026quot;, \u0026quot;70-79\u0026quot;, \u0026quot;80+\u0026quot;]; zₐ = [0.05, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.40, 0.50]; mₐ = [0.01, 0.03, 0.03, 0.03, 0.06, 0.10, 0.25, 0.35, 0.50]; cₐ = [0.05, 0.10, 0.10, 0.15, 0.20, 0.25, 0.35, 0.45, 0.55]; fₐ = [0.30, 0.30, 0.30, 0.30, 0.30, 0.40, 0.40, 0.50, 0.50]; nAgeGroup = length(AgeGroup);   Infrastruture The number of beds available is assumed as a fixed resource in time. The number of hospital (resp. ICU) beds in use will be denoted \\(\\mathscr{H}(t)\\) (resp. \\(\\mathscr{U}(t)\\)) up to a maximum of \\(\\mathscr{H}_{max}\\) (resp. \\(\\mathscr{U}_{max}\\)).\nAlthough the initial infections took place via dommestic and international travellers (apart from the initial infections in Wuhan obviously), we will assume no net flow of population in and out of a country of interest.\n  Infection Susceptible: The base rate of contagion is denoted as \\(R_0\\). The actual rate varies with time (to reflect seasons and impact of temperature on virus resilience) and the effectiveness of the mitigation measures such as social distancing. Separately, each age group will have a different sensitivity to infection.\nThe infection rate \\(\\beta_a(t)\\) is age- and time-dependent. It is given by:\n\\[\\beta_a(t) = z_a M(t) R_0 \\left( 1+\\varepsilon \\cos \\left( 2\\pi \\frac{t-t_{max}}{t_i} \\right) \\right) \\]\nwhere:\n \\(z_a\\) is the degree to which particular age groups are sensitive to initial infection. It reflects bioligical sensitivity and to which degree it is isolated from the rest of the population (denoted \\(I_a\\) in NeherLab). \\(M(t)\\) is a time-dependent ratio reflecting the effectiveness of mitigation measures. \\(\\varepsilon\\) is the amplitude of seasonal variation in transmissibility. \\(t_{max}\\) is the time of the year of peak transmission.  Susceptible individuals are exposed to a number of contagious individuals:\n asymptomatic infected: \\[\\gamma_e \\beta_a(t) E_a(t)\\] symptomatic infected: \\[\\gamma_i \\beta_a(t) I_a(t)\\] severe not in hospital: \\[\\gamma_j \\beta_a(t) J_a(t)\\] critical not in hospital: \\[\\gamma_k \\beta_a(t) K_a(t)\\]  The sum of those will be a flow from susceptible (\\(S\\)) to (\\(E\\)) exposed individuals.\n\\[ \\begin{aligned} S2E_a(t) \u0026amp; = \\gamma_e \\beta_a(t) E_a(t) + \\gamma_i \\beta_a(t) I_a(t) + \\gamma_j \\beta_a(t) K_a(t) + \\gamma_k \\beta_a(t) L_a(t) \\\\ S2E_a(t) \u0026amp; = \\beta_a(t) \\left( \\gamma_e E_a(t) + \\gamma_i I_a(t) + \\gamma_j J_a(t) + \\gamma_k K_a(t) \\right) \\\\ E2S_a(t) \u0026amp; = -S2E_a(t) \\\\ \\end{aligned} \\]\nand therefore:\n\\[ \\frac{dS_{a}(t)}{dt} = - S2E_a(t) = E2S_a(t) \\]\n  After infection Quantitatively, the model expresses how many individuals transfer from one situation/compartment to another. Flows from compartment X to Y are written as \\(X2Y\\) (obviously \\(X2Y = - Y2X\\)).\nNote that the compartments are split into age groups.\nTransitions between compartments\n Epidemiology Instead of expressing the sum of the flows at each node, it is easier to express the arrows, and summing them afterwards. For example, arrow from \\(J\\) to \\(K\\) will be:\n\\[JK_a(t) = \\frac{c_a}{t_h} J_a(t)\\]\nwith a positive flow following the direction of the arrow.\nIn {julia}, this will become:\n JK = cₐ .* J / tₕ where J is a vector representing an age group, .* is the element-wise multiplication.\nAfter defining the arrows \\(IJ\\), \\(JK\\) and \\(JH\\), the change in \\(J\\) will simply be:\n dJ = IJ - JK - JH  Bed transfers Individuals are transferred into hospital beds then into ICU beds in the order indicated by the red numbers.\nCritical patients already in hospital go into ICU as spots become available. The freed bed are first made available to critical patients out of hospital (\\(K\\)). Then, any free beds will receive patients in severe condition.\n Safeguards: Note the need to ensure a few common sense rules:\n No compartment can have a negative number of people.\n The total population figure should remain unchanged. This is done by adjusting the number of susceptible individuals.\n Careful accounting of the use of fixed number of hospital beds.\n The number of infected people should always be above the number of reported cases.\n  # Helper function to never change the number of individuals in a compartment in a way that would # make it below 0.1 (to avoid rounding errors around 0) function ensurePositive(d,s) return max.(d .+ s, 0.1) .- s end; # The dynamics of the epidemy is a function that mutates its argument with a precise signature # Don\u0026#39;t pay too much attetion to the print debugs/ function epiDynamics!(dP, P, params, t) S, E, I, J, H, C, R, D, K, L, BED, ICU = Pop2Comp(P) BED = BED[1] ICU = ICU[1] r₀, tₗ, tᵢ, tₕ, tᵤ, γₑ, γᵢ, γⱼ, γₖ, δₖ, δₗ, δᵤ, startDays = params #################################### # Arrows reflecting epidemiology - Check signs (just in case) EI = ones(nAgeGroup) .* E / tₗ; EI = max.(EI, 0.0); IE = -EI; IJ = mₐ .* I / tᵢ; IJ = max.(IJ, 0.0); JI = -IJ JK = cₐ .* J / tₕ; JK = max.(JK, 0.0); KJ = -JK HL = cₐ .* H / tₕ; HL = max.(HL, 0.0); LH = -HL # Recovery arrows IR = (1 .- mₐ) .* I / tᵢ; IR = max.(IR, 0.0); RI = -IR JR = (1 .- cₐ) .* J / tₕ; JR = max.(JR, 0.0); RJ = -JR HR = (1 .- cₐ) .* H / tₕ; HR = max.(HR, 0.0); RH = -HR KR = (1 .- δₖ .* fₐ) .* K / tᵤ; KR = max.(KR, 0.0); RK = -KR LR = (1 .- δₗ .* fₐ) .* L / tᵤ; LR = max.(LR, 0.0); RL = -LR CR = (1 .- δᵤ .* fₐ) .* C / tᵤ; CR = max.(CR, 0.0); RC = -CR # Deaths KD = δₖ .* fₐ .* K / tᵤ; KD = max.(KD, 0.0); DK = -KD LD = δₗ .* fₐ .* L / tᵤ; LD = max.(LD, 0.0); DL = -LD CD = δᵤ .* fₐ .* C / tᵤ; CD = max.(CD, 0.0); DC = -CD #################################### # Bed transfers ####### Step 1: # Decrease in bed usage is (recall that CD and CR are vectors over the age groups) dICU = - (sum(CD) + sum(CR)); dICU = ensurePositive(dICU, ICU) # ICU beds available ICU_free = ICU_max - (ICU + dICU) # Move as many patients as possible from $L$ to $C$ in proportion of each group ICU_transfer = min(sum(L), ICU_free) LC = ICU_transfer / sum(L) .* L; CL = -LC # Overall change in ICU bed becomes dICU = dICU + ICU_transfer; dICU = ensurePositive(dICU, ICU) # And some normal beds are freed dBED = -ICU_transfer; dBED = ensurePositive(dBED, BED) #print(\u0026quot; dBed step 1 \u0026quot;); println(floor.(sum(dBED))) ####### Step 2: # Beds available BED_free = BED_max - (BED + dBED) # Move as many patients as possible from $K$ to $L$ in proportion of each group BED_transfer = min(sum(K), BED_free) KL = BED_transfer / sum(K) .* K; LK = -KL # Overall change in normal bed becomes dBED = dBED + BED_transfer; dBED = ensurePositive(dBED, BED) #print(\u0026quot; dBed step 2 \u0026quot;); println(floor.(sum(dBED))) ####### Step 3: # Beds available BED_free = BED_max - (BED + dBED) # Move as many patients as possible from $J$ to $H$ in proportion of each group BED_transfer = min(sum(J), BED_free) JH = BED_transfer / sum(J) .* J; HJ = -JH # Overall change in ICU bed becomes dBED = dBED + BED_transfer; dBED = ensurePositive(dBED, BED) #print(\u0026quot; dBed step 3 \u0026quot;); println(floor.(sum(dBED))) #################################### # Sum of all flows + Check never negative compartment # Susceptible # Calculation of β β = getCurrentRatio(t; start = BASE_DAYS, schedule = mitigationRatio) .* zₐ .* R₀(t; r_0 = r₀, latitude = Latitude, severity = SeverityLevel) #print(\u0026quot;r₀\u0026quot;); println(r₀); println(\u0026quot;R₀\u0026quot;); #println(R₀(t; r_0 = r₀, latitude = Latitude, severity = SeverityLevel)); print() dS = -β .* (γₑ.*E + γᵢ.*I + γⱼ.*J + γₖ.*K); dS = min.(-0.01, dS); dS = ensurePositive(dS, S) #print(\u0026quot;dS\u0026quot;); println(floor.(dS)); println(); # Exposed dE = -dS + IE; dE = ensurePositive(dE, E) # Infected. dI = EI + JI + RI; dI = ensurePositive(dI, I) # Infected no hospital dJ = IJ + HJ + KJ + RJ; dJ = ensurePositive(dJ, J) #print(\u0026quot;I \u0026quot;); println(floor.(IJ)); print(\u0026quot;H \u0026quot;); println(floor.(HJ)) #print(\u0026quot;K \u0026quot;); println(floor.(KJ)); print(\u0026quot;R \u0026quot;); println(floor.(RJ)) # Infected in hospital dH = JH + LH + RH ; dH = ensurePositive(dH, H) # Critical no hospital dK = JK + LK + DK + RK; dK = ensurePositive(dK, K) # Critical in hospital dL = KL + HL + CL + DL + RL; dL = ensurePositive(dL, L) # Critical in ICU dC = LC + DC + RC; dC = ensurePositive(dC, C) # Recovery (can only increase) dR = IR + JR + HR + KR + LR + CR; dR = max.(dR, 0.01) # Dead (can only increase) dD = KD + LD + CD; dD = max.(dD, 0.01) # Vector change of population and update in place result = vcat(dS, dE, dI, dJ, dH, dC, dR, dD, dK, dL, [dBED], [dICU]) #print(\u0026quot; dS \u0026quot;); print(floor.(sum(dS))); print(\u0026quot; dE \u0026quot;); print(floor.(sum(dE))); #print(\u0026quot; dI \u0026quot;); print(floor.(sum(dI))); print(\u0026quot; dJ \u0026quot;); println(floor.(sum(dJ))); #print(\u0026quot; dH \u0026quot;); print(floor.(sum(dH))); print(\u0026quot; dC \u0026quot;); print(floor.(sum(dC))); #print(\u0026quot; dR \u0026quot;); print(floor.(sum(dR))); print(\u0026quot; dD \u0026quot;); print(floor.(sum(dD))); #print(\u0026quot; dK \u0026quot;); print(floor.(sum(dK))); print(\u0026quot; dL \u0026quot;); println(floor.(sum(dL))); println(); for i = 1:length(result) dP[i] = result[i] end end;      Load data The data comes from Neherlab’s data repository on Github.\nWe will use Italy as an example\ncountry = \u0026quot;Italy\u0026quot;; This file contains a record of cases day by day.\ncases = DataFrame(CSV.read(\u0026quot;data/World.tsv\u0026quot;, header = 4)); cases = @where(cases, occursin.(country, :location)); sort!(cases, :time); # Add a time column in the same format as the other dataframes cases = hcat(DataFrame(t = date2days.(cases[:, :time])), cases); # Remove any row with no recorded death cases = cases[cases.deaths .\u0026gt; 0, :]; last(cases[:, [:time, :cases, :deaths]], 6) The last rows shows the number of cases and deaths up to the last date in the dataset.\nPlotting the number of death shows an almost exponential increase in numbers (straight line in logarithmic scale).\nusing PyPlot; pyplot(); clf(); ioff(); plot_x = cases.time; plot_y = cases.deaths; fig, ax = PyPlot.subplots(); ax.plot(plot_x, plot_y, \u0026quot;ro\u0026quot;); ax.fill_between(plot_x, plot_y, color=\u0026quot;red\u0026quot;, linewidth=2, label=\u0026quot;Deaths\u0026quot;, alpha=0.3); ax.legend(loc=\u0026quot;upper left\u0026quot;); ax.set_xlabel(\u0026quot;time\u0026quot;); ax.set_ylabel(\u0026quot;Deaths\u0026quot;); ax.set_yscale(\u0026quot;log\u0026quot;); PyPlot.savefig(\u0026quot;images/Deaths.png\u0026quot;); Deaths\n This file contains ICU beds figures.\nICU_capacity = select(CSV.read(\u0026quot;data/ICU_capacity.tsv\u0026quot;; delim = \u0026quot;\\t\u0026quot;), :country, :CriticalCare); ICU_capacity = @where(ICU_capacity, occursin.(country, :country))[!, :CriticalCare][1]; ICU_capacity = convert(Float64, ICU_capacity); Country codes are necessary to load the another file.\ncountry_codes = select(CSV.read(\u0026quot;data/country_codes.csv\u0026quot;), :name, Symbol(\u0026quot;alpha-3\u0026quot;)); country_codes = @where(country_codes, occursin.(country, :name)); countryShort = country_codes[:, Symbol(\u0026quot;alpha-3\u0026quot;)][1]; This file contains hospital beds figures.\nhospital_capacity = select(CSV.read(\u0026quot;data/hospital_capacity.csv\u0026quot;, types = Dict(:COUNTRY =\u0026gt; String), limit = 1267), :COUNTRY, :YEAR, :VALUE); hospital_capacity = @where(hospital_capacity, Not(ismissing.(:COUNTRY))); hospital_capacity = last(@where(hospital_capacity, occursin.(countryShort, :COUNTRY)), 1)[!, :VALUE][1]; hospital_capacity = convert(Float64, hospital_capacity); This file contains a distribution of the population in age groups.\nage_distribution = CSV.read(\u0026quot;data/country_age_distribution.csv\u0026quot;); age_distribution = @where(age_distribution, occursin.(country, :_key))[!, 2:10]; # Convert to simple matrix age_distribution = Matrix(age_distribution); show(age_distribution);  Initialise parameters Fixed constants SeverityLevel = :moderate; Latitude = :north; StartDate = Date(2020, 3, 1); StartDays = date2days(StartDate); EndDate = Date(2020, 9, 1); EndDays = date2days(EndDate); tSpan = (StartDays, EndDays);  Infrastructure BED_max = hospital_capacity ICU_max = ICU_capacity  Parameter vector # r₀, tₗ, tᵢ, tₕ, tᵤ, γᵢ, γⱼ, γₖ, δₖ, δₗ, δᵤ, startDate = params parameters = [ baseR₀[Latitude, SeverityLevel], tₗ[SeverityLevel], tᵢ[SeverityLevel], tₕ, tᵤ, γₑ, γᵢ, γⱼ, γₖ, δₖ, δₗ, δᵤ, StartDays];  Population Age_Pyramid = transpose(age_distribution); Age_Pyramid_frac = Age_Pyramid / sum(Age_Pyramid); We do not know the number of actual number of infections cases at the start of the model. We only know confirmed cases (almost certainly far below the number of actual infections).\nWe assume that actual infections are 3 time more numerous.\nDeathsAtStart = @where(cases, :time .== StartDate)[!, :deaths][1]; ConfirmedAtStart = @where(cases, :time .== StartDate)[!, :cases][1]; EstimatedAtStart = 3.0 * ConfirmedAtStart;  Parameters vector # Note that values are inintialised at 1 to avoid division by zero S0 = Age_Pyramid; E0 = ones(nAgeGroup); I0 = EstimatedAtStart * Age_Pyramid_frac; J0 = ones(nAgeGroup); H0 = ones(nAgeGroup); C0 = ones(nAgeGroup); R0 = ones(nAgeGroup); D0 = DeathsAtStart * Age_Pyramid_frac; K0 = ones(nAgeGroup); L0 = ones(nAgeGroup); # Everybody confirmed is in hospital BED = [ConfirmedAtStart]; ICU = [1.0]; P0 = vcat(S0, E0, I0, J0, H0, C0, R0, D0, K0, L0, BED, ICU); dP = 0 * P0;   Differential equation solver model = ODEProblem(epiDynamics!, P0, tSpan, parameters); # Note: progress steps might be too quick to see! sol = solve(model, Tsit5(); progress = false, progress_steps = 5); # The solutions are returned as an Array of Arrays: # - it is a vector of size the number of timesteps # - each element of the vector is a vector of all the variables nSteps = length(sol.t); nVars = length(sol.u[1]); # Empty dataframe to contain all the numbers # (When running a loop at top-level, the global keywrod is necessary to modify global variables.) solDF = zeros((nSteps, nVars)); for i = 1:nSteps global solDF solDF[i, :] = sol.u[i] end; solDF = hcat(DataFrame(t = sol.t), DataFrame(solDF)); # Let\u0026#39;s clean the names compartments = [\u0026quot;S\u0026quot;, \u0026quot;E\u0026quot;, \u0026quot;I\u0026quot;, \u0026quot;J\u0026quot;, \u0026quot;H\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;R\u0026quot;, \u0026quot;D\u0026quot;, \u0026quot;K\u0026quot;, \u0026quot;L\u0026quot;]; solnames = vcat([:t], [Symbol(c * repr(n)) for c in compartments for n in 0:(nAgeGroup-1)], [:Beds], [:ICU]); rename!(solDF, solnames);  # Create sums for each compartment # (Consider solDF[!, r\u0026quot;S\u0026quot;]) # for c in compartments col = [Symbol(c * repr(n)) for n in 0:(nAgeGroup-1)] s = DataFrame(C = sum.(eachrow(solDF[:, col]))) rename!(s, [Symbol(c)]) global solDF = hcat(solDF, s) end; # The D column gives the final number of dead. println(last(solDF[:, Symbol.(compartments)], 5)) The last row shows the final sizes of the various compartments.\nNext is the evolution of the over time.\npyplot(); clf(); ioff(); fig, ax = PyPlot.subplots(); ax.plot(solDF.t, solDF.D, label = \u0026quot;Forecast\u0026quot;); ax.plot(solDF.t, solDF.R, label = \u0026quot;Recoveries\u0026quot;); ax.plot(cases.t, cases.deaths, \u0026quot;ro\u0026quot;, label = \u0026quot;Actual\u0026quot;, alpha = 0.3); ax.legend(loc=\u0026quot;lower right\u0026quot;); ax.set_xlabel(\u0026quot;time\u0026quot;); ax.set_ylabel(\u0026quot;Individuals\u0026quot;); ax.set_yscale(\u0026quot;log\u0026quot;); PyPlot.savefig(\u0026quot;images/DeathsForecast.png\u0026quot;);  Increase in Recoveries and Deaths over time\n It is clear the model forecasts a faster growth than reality. A parameter estimation is necessary.\npyplot(); clf(); ioff(); fig, ax = PyPlot.subplots(); ax.plot(solDF.t, solDF.Beds, label = \u0026quot;Beds\u0026quot;); ax.plot(solDF.t, solDF.ICU, label = \u0026quot;ICU\u0026quot;); ax.legend(loc=\u0026quot;lower right\u0026quot;); ax.set_xlabel(\u0026quot;time\u0026quot;); ax.set_ylabel(\u0026quot;Number of beds\u0026quot;); ax.set_yscale(\u0026quot;linear\u0026quot;); PyPlot.savefig(\u0026quot;images/BedUsage.png\u0026quot;);  Bed Usage over time\n It is clear that the requirements for beds quickly hits the available capacity\n Bilibliography The Novel Coronavirus Pneumonia Emergency Response Epidemiology Team. The Epidemiological Characteristics of an Outbreak of 2019 Novel Coronavirus Diseases (COVID-19) — China, 2020[J]. China CDC Weekly, 2020, 2(8): 113-122. LINK\n ","date":1585094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585094400,"objectID":"05224decdd545e6b291afeaa0b25d41f","permalink":"https://emmanuel-r8.github.io/post/2020/03/25/2020-03-25-forecasting-covid-19/","publishdate":"2020-03-25T00:00:00Z","relpermalink":"/post/2020/03/25/2020-03-25-forecasting-covid-19/","section":"post","summary":"The Neherlab COVID-19 forecast model Basic assumptions  Overview Age cohorts Severity Seasonality Transmission reduction Details of the model  Population compartments Model parameters Infection After infection   Load data Initialise parameters  Fixed constants Infrastructure Parameter vector Population Parameters vector  Differential equation solver Bilibliography   The Neherlab COVID-19 forecast model using CSV, Dates; using DataFrames, DataFramesMeta; using Plots, PyPlot; using DifferentialEquations; This is more a data science post than machine learning.","tags":["Data Science","Julia","COVID-19"],"title":"Forecasting the progression of COVID-19","type":"post"},{"authors":null,"categories":["Machine Learning","Neural Network"],"content":"   Recurrent Neural Networks (RNN)  From simple RNNs to LSTMs Long/Short Term Memory RNNs Attention Beyond LSTM: Transformers Transformer-XL  Compressive Transformers  Introduction Compression scheme Compression training  Summary   This is the first post of series dedicated to Compressive Memory of Recurrent Neural Networks. This is inspired by a recent DeepMind paper published in November 2019 on Arxiv.\nCurrently, the ambition of the series is to follow this plan:\n Part 1 (here): A high level introduction to Compressive Memory mechanics starting from basic RNNS;\n Part 2: a detailed explanation of the TransformerXL;\n Part 3: an implementation using PyTorch (soon);\n Part 4: finally, its application to time series (soon).\n  Most likely, this will be fine-tuned over time.\nBig thanks to Gautier Marti and Vincent Zoonekynd for their suggestions and proof-reading!\nUpdate: Additional diagrams (14 March 2020)\nRecurrent Neural Networks (RNN) From simple RNNs to LSTMs Traditional neural networks were developed to train/run on information provided in a single step in a consistent format (e.g. images with identical resolution). Conceptually, a neural network could similarly be taught on sequential information (e.g. a video as a series of images) looking at it as a single sample, but that would require (1) being trained on the full sequence (e.g. an entire video), (2) being able to cope with information of variable length (i.e. short vs. long video). (1) is computationally intractable, and (2) means that units analysing later parts of the video would not be receiving as much training as earlier units when ideally they should be all share the same amount of training .\nBasic RNN (source: Wikipedia)\n The original RNN address those issues:\n Sequences are chopped in small consistent sub-sequences (say, a segment of 10 images, or a group of 20 words).\n An RNN layer is a group of blocks (or cells), each receiving a single element of the segment as input. Note that here layer does not have the traditional meaning of a layer of neural units fully connected to a previous layer of units. It is a layer of RNN cells. Within each cell, quite a few things happen, including using layers of neural units. From here on, a layer will refer to an RNN layer and not a layer of neural units..\n Within a layer, cells are identical: they have the same parameters.\n  Although each element of a sequence might be of interest on its own, it only becomes really meaningful in the context of the other elements. Each cell contains a state vector (called hidden state). Each cell is trained using an individual element from a segment and the hidden state from the preceding cell. Training the network means training the creation of those states. Passing of the hidden state transfers some context or memory from prior elements of the segment. The cells receiving a segment form a single layer. Each cell would typically (but not necessarily) also include an additional sub-cell to create an output as a function of the hidden step. In that case, the output of a layer can then be used as input of new RNN layer.\nA layer is trained passing hidden states from prior cells to later cells. The hidden state from prior elements is used to contextualise a current element. To use context from later elements (e.g. in English, a noun giving context to a preceding adjective), a separate layer is trained where context instead passes from later to prior elements. Those forward and backward layers jointly create a bidirectional RNN .\nHistorically, RNNs applied to NLP deal with elements which are either one-hot encoded (either letters, or, more efficient, tokens), or word embeddings often normalised as unit vectors (for example see Word2Vec and GloVe). RNN cells therefore deal with values between 0 and 1. Typically, non-linearity is brought by \\(tanh\\) or \\(sigmoid\\) activations which guarantee unit values within that range. Those activation functions quickly have very flat gradients. Segments often have 10s or 100s of elements. Because of vanishing gradients, a hidden state receives little information from distant cells (training gradients are hardly influenced by gradients of distant cells).\n Long/Short Term Memory RNNs Basic LSTM RNN (source: Wikipedia)\n Long/Short Term Memory RNNs (LSTM) address this by passing two states:\n a hidden state \\(h\\) as described above trained with non-linearity: this is the short-term memory; and,\n another hidden state \\(c\\) (called context) weighting previous contexts with a simple exponential moving average (in Gated Recurrent Units) or a slightly more complicated version thereof in the original LSTM model structure. Determining the optimal exponential decay is part of the training process. This minimally processed state is the long-term memory.\n  LTSM can also be made bidirectional.\nWithout going into further details, note that each \\(\\sigma\\) and \\(\\tanh\\) orange block represents matrix of parameters to be learned.\n Attention Attention RNN\n RNN were further extended with an attention mechanism. Blog posts on attention by Jay Alammar and Lilian Weng are good introductions.\nA multi-layer RNN takes the output a layer and uses it as input for the next. With the attention mechanism, the outputs go through an attention unit.\n Beyond LSTM: Transformers RNNs were then simplified (insert large air quotes) with Transformers (using what is called self-attention) that significantly reduce the number of model parameters and can be efficiently parallelised with minimum model performance impact. For an extremely clear introduction to those significant improvements, you cannot do better than reading , and by Peter Bloem on transformers. The following assumes that you are broadly familiar with those ideas.\nThe basic transformer structure uses self-attention where, for a given element (the query), the transformer looks at the other elements of the segment (the keys) to determine how much ‘attention’ other elements of the segment influence the role of the query in changing the hidden state.\nBroadly:\n The query is projected in some linear space (a matrix \\(W_q\\)). That’s basically an embedding which is part of the model training.\n All the other elements, the keys, are projected in another linear space (a matrix \\(W_k\\)); another embedding which is part of the model training.\n The similarity (perharps affinity would be a better word) between the projected query and each projected key is calculated with a dot product / cosine distance. This is exactly the approach of basic recommender systems with the difference that the recommendation is between sets of completely different nature (for example affinity between users and movies). Note that although query and keys are elements of identical type, they are embedded into different spaces with different projections matrices.\n We now have a vector of the same size as the segment length (one cosine distance per input element). It goes through another layer (a matrix \\(W_v\\)) to give a value.\n  The triplet of \\(\\left( W_q, W_k, W_v \\right)\\) is called an attention head. Actual models would include multiple heads (of the order of 10), and the output of a transformer layer could then feed into a new transformer layer.\nThis model is great until you notice that the dot product / cosine similarity is commutative and does not reflect whether a key element is located before or after the query element: order is fundamental to sequential information (“quick fly” vs. “fly quick”). To address this, the input elements are always enriched with a positional embedding: the input elements are concatenated with positional information showing where they stand within a segment.\nNote that a transformer layer is trained on a segment using only the information from that segment. This is fine to train on sentences, but it cannot really account for more distant relationships between words within a lengthy paragraph, let alone a full text.\n Transformer-XL Transformers have been further improved with Tranformer-XL (XL = extra long) which are trained using hidden states from previous segments, therefore using information from several segments, to improve a model’s memory span.\nConceptually, this is an obvious extension of the basic transformer to increase its memory span. But there is a fundamental problem. Going back to the basic transformer, each element includes its absolute position within the segment. The position of the first word of the segment is 1, that of the last one is, say, 250 . Such a scheme breaks down as soon as the state of the previous segment is taken into account. Word 1 of the current segment obviously comes before word 250, but has to come after word 250 of the previous segment. The absolute position encoding does not reflect the relative position of elements located in different segments.\nThe key contribution of the Transformer-XL is to develop a relative positional encoding that allows hidden state information to cross segment boundaries. In their implementation, the authors evaluate that the attention length, being basically how many hidden states are used, is 450% longer that the basic transformer. That’s going from sentence length to full paragraph, but still far from a complete book.\nA side, but impressive, benefit is that the evaluation speed of the model, or it use once trained, is significantly increased thanks to the relative addressing (the paper states up to a 1,800-fold increase depending on the attention length).\n  Compressive Transformers Full text understanding cannot be achieved by simply lengthening segment sizes from 100s to the word count of a typical novel (about 100,000). When training a model routinely takes 10s of hours on GPU clusters, an increase by 3 orders of magnitude is not realistic.\nIn a recent paper, DeepMind proposes a new RNN model called Compressive Transformers.\nIntroduction Transformer-XL uses the hidden state of a prior segment (\\(h_{T-1}\\)) to improve the training of the current segment (\\(h_{T}\\)). When moving to the next segment, training (\\(h_{T+1}\\)) now only uses \\(h_{T}\\) and \\(h_{T-1}\\) is discarded. To increase the memory span, one could train using more past segments at the expense of increase in memory usage and computation time (quadratic). The actual Transformer-XL uses the hidden states of several previous segments, but the discarding mechanism will remain.\nThe key contribution of the Compressive Transformers is the ability to retain salient information from those otherwise discarded past states. Instead of being discarded, they are stored in compressed form.\nEach Transformer-XL layer is now trained with prior hidden states (primary memory) and the compressed memory of older hidden states.\nAs an aside, although not explicitly mentioned, we should note that the ‘-XL’ aspect of the Transformer-XL and the memory compression mechanics are conceptually independent from the actual types of RNN cell. Simple RNNs, GRUs or LSTMs could be trained using the hidden states of past segments (not dissimilar to state/context peeking into past cells in certain RNN variants). But the performance benefit of Transformer-XL is such that the paper only focuses on transformer-XL.\n Compression scheme As compared to Transformer-XL, the key difference is the compression scheme. The rest of the model seems identical.\nSize parameters The size of the model is described with a few size parameters:\n \\(n_s\\): size of a segment = the number of cells in a layer.\n \\(n_m\\): number of hidden states in the primary uncompressed memory (like the Transformer-XL). \\(n_m\\) is a multiple of \\(n_s\\). The primary memory is a FIFO buffer: the first (oldest) memories will be the first to be later compressed.\n \\(n_{cm}\\): number of compressed hidden states in the compressed memory. States in the compressed memory will compress an old segment of size \\(n_s\\) dropping out of the primary memory. \\(c\\) is an information compression ratio from \\(n_s\\) primary memory entries into compressed memory entries. There can be two ways of applying this compression ratio, which both reduce the number of hidden states by the same ratio:\n \\(c\\) uncompressed layers could create a single compressed hidden state of identical size. This merges the information of a group of elements (e.g. \\(c\\) words) into a single hidden state. In this case, \\(n_s\\) is proportional to \\(c\\) and \\(n_{cm}\\) is proportional to \\(n_s / c\\). The authors do not use this approach. It would enforce a sub-segmentation of an uncompressed segment at arbitrary intervals (why group 3 words instead of 5 or 7…)\n Instead, the authors use dimension reduction: a single uncompressed hidden state is compressed into a new hidden state with \\(c\\) times fewer hidden states. If the size of the hidden state of a Transformer-XL cell is \\(n_h\\), hidden states in the primary memory will have the same size, and the compressed memory hidden states will have a size of \\(n_h / c\\).\n   By way of example, a segment could have 100 cells (\\(n_s = 100\\)). This segment could be trained with the hidden states of the past 3 segments’ training (\\(n_m = 3 * n_s = 300\\)). When training the next segment, an old segment of size 100 becomes available for compression which will create 100 new hidden states.\nThis example is for a single layer. The same scheme would be replicated for each layer of the model\nNote that the paper only contemplates a single set of compressed memories. There could also be multiple generations of compressed memories, primary memory compresses in generation 1, then compressing into generation 2…\n Compression functions A compressed hidden state is created from \\(c\\) primary memory hidden states. When training on texts with word embeddings,the authors used a value of \\(c=3\\) or \\(c=4\\).\nSeveral compression schemes are explored in the paper:\n max or mean pooling with a stride of \\(c\\). This is typical of image convolution networks - no explanation required.\n 1-dimensional convolution with a stride of \\(c\\). This is also typical of image convolution network apart from being one-dimensional. This requires parameter training.\n dilated convolution. In practice image convolutions have shown to be inadequate for sequential information where dependencies can be at both short and long ranges: working at different scales makes sense. Dilated convolutions use convolution filters that are contracted and dilated versions of a template to be trained.\n a most-used mechanism that identifies and retains part of the hidden states according to their importance in the cells training gauged by the attention they received.\n    Compression training Training the compression parameters is done separately from the optimisation of the Transformer-XL cells.\nThe purpose of the compressed memory is to provide a compressed and lossy representation of the primary memory (hidden states) or the attention heads parameters: the quality of the compression mechanics is assessed by how well the original information can be re-generated from it. In essence, the compressed hidden states are a compressed representations to a learned representation vector in an auto-encoder. This is the training mechanics used by the authors.\nAs in an auto-encoder, the representation is learned by comparing the original information to its reconstruction. This training is kept completely independent from the training of the transformers: the auto-encoding loss and gradients do not impact the attention heads’ parameters.\nConversely, the loss and gradients of the attention heads’ training do not flow into the training of the compression scheme.\n  Summary This was a high level introduction of RNNs all the way up to Compressive Memory mechanics. Next, the algorithm’s nitty-gritty.\n ","date":1583539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583539200,"objectID":"5f8aed383aa529169ae808a9ace861f3","permalink":"https://emmanuel-r8.github.io/post/2020/03/07/rnn-compressive-memory-part-1/","publishdate":"2020-03-07T00:00:00Z","relpermalink":"/post/2020/03/07/rnn-compressive-memory-part-1/","section":"post","summary":"Recurrent Neural Networks (RNN)  From simple RNNs to LSTMs Long/Short Term Memory RNNs Attention Beyond LSTM: Transformers Transformer-XL  Compressive Transformers  Introduction Compression scheme Compression training  Summary   This is the first post of series dedicated to Compressive Memory of Recurrent Neural Networks.","tags":["Machine Learning","Neural Network"],"title":"RNN Compressive Memory Part 1: A high level introduction.","type":"post"},{"authors":["Emmanuel Rialland"],"categories":[],"content":"Click on the pdf or slides buttons above to access the materials.\n","date":1576138647,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576138647,"objectID":"cb5bfe5e09996710f3ad19b8c46e1b5e","permalink":"https://emmanuel-r8.github.io/project/lendingclub/","publishdate":"2019-12-12T16:17:27+08:00","relpermalink":"/project/lendingclub/","section":"project","summary":"Credit ratings, logistic regression, scoring systems and loan pricing using a database of peer-to-peer loans.","tags":[],"title":"Lending Club peer-to-peer loans scoring","type":"project"},{"authors":["Emmanuel Rialland"],"categories":[],"content":"Click on the pdf or slides buttons above to access the materials.\n","date":1576138066,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576138066,"objectID":"0c6b257ed4bdcddec2290b6aef8ca8c1","permalink":"https://emmanuel-r8.github.io/project/movielens/","publishdate":"2019-12-12T16:07:46+08:00","relpermalink":"/project/movielens/","section":"project","summary":"Recommender systems on movie choices, low-rank matrix factorisation with stochastic gradient descent using the Movielens dataset","tags":[],"title":"Movielens Recommender System","type":"project"},{"authors":null,"categories":["Data Science","Machine Learning","R"],"content":"  Both capstones for the HarvardX certificates are now available. Just click on the Projects link!\nIf Gitbooks are not your thing, at the top of their main page, there is a download link to a pdf version.\nThey make for a good knock-me-asleep reading…\n","date":1576108800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576108800,"objectID":"204e1c14bc637625bd4fb436c7c38eda","permalink":"https://emmanuel-r8.github.io/post/2019/12/12/harvardx-gitbooks-available/","publishdate":"2019-12-12T00:00:00Z","relpermalink":"/post/2019/12/12/harvardx-gitbooks-available/","section":"post","summary":"Both capstones for the HarvardX certificates are now available. Just click on the Projects link!\nIf Gitbooks are not your thing, at the top of their main page, there is a download link to a pdf version.","tags":["Data Science","Machine Learning","R","R Markdown","Statistical Learning"],"title":"HarvardX Gitbooks available","type":"post"},{"authors":null,"categories":["Data Science","Machine Learning","R"],"content":"  After 3 months of work, the final report for the HarvardX Data Science course was submitted.\nIt is based on the LendingClub dataset. LendingClub is a peer-2-peer lender. This is a matching of private borrowers and investors. Small amounts, fairly high risk (if they could, borrowers would probably have had a bank involved). Surprisingly, after tapping a market of individual lenders, the biggest lenders are now the banks. To inform the investors, LendingClub make historical information publicly available.\nThis work went through many blind alleys. I won’t list them, they are in the report (post-mortem section). But it was an overall enriching experience. I learned a lot, often about limitations of what I tried (the dataset is big with a few millions samples (big for an old laptop), with many (ca. 150) mispecified mixed categorical and numeric variables). The experience will be filed in the ‘it-builds-character’ category…\nOne point that is still tingling my mind is learning about Conditional Inference Trees used to bin variables. That is then used for logistic regression to predict probabilities of loan default.\nWhy are those trees interesting? They are sourced in information theory and measure the information content of a prediction variable to predict a binary response. The prediction variable is then partitioned in a few intervals (bins). What is great?\n The measurement does NOT rely on the value of the prediction variable. This means that variable NAs go from being a nuisance to being stashed in a bin of their own treated as any other bin.\n The logistic regression context predicates binary variables which were perfect for the purpose of this report. But those trees do not require binary outcomes. They rely on what are called Weight of Evidence (calculated for each bin) and Information Value (calculated for each variable).\n The calculations are very quick (about 1/10th second to bin 1 million samples) with a small memory footprint.\n  In other words, whatever comes in, we do not have to worry about scaling/z-scoring/filling NAs; it is quickly reformatted into a handful (literally of that order of magnitude depending on parameters used) based on the relevance to predicting what needs to come out.\nIf I didn’t know better, this should be called model impedance matching (electrical engineers can explain)!\nApart from that, the number of avenues to explore with this dataset (especially using data from other sources) could fill many more months. I listed a list of possible techniques in the report’s conclusion. This is what does and will keep banks’ credit risk departments busy and well-staffed…\nI am working on making the MovieLens and LendingClub reports available as gitbooks. To be announced.\n","date":1576022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576022400,"objectID":"377e0d60166f26964faebd33ec793d80","permalink":"https://emmanuel-r8.github.io/post/2019/12/11/harvardx-final-report-lendingclub-dataset/","publishdate":"2019-12-11T00:00:00Z","relpermalink":"/post/2019/12/11/harvardx-final-report-lendingclub-dataset/","section":"post","summary":"After 3 months of work, the final report for the HarvardX Data Science course was submitted.\nIt is based on the LendingClub dataset. LendingClub is a peer-2-peer lender. This is a matching of private borrowers and investors.","tags":["Data Science","Machine Learning","R","Statistical Learning"],"title":"HarvardX Final Report - LendingClub dataset","type":"post"},{"authors":null,"categories":["Quick Thought"],"content":"   Quick Thoughts are random thoughts looking for comments\n Let’s imagine a universal translator able to translate any language to any language. Sourcing a corpus of pair translation is a major hurdle. However there is an almost infinite corpus of pair translations: a language with itself; translating English to English is easy, even for a computer.\nLet’s give the blackbox universal translator three inputs: a source text, the language of the source text, the language of the desired translation. What would be the consequences for the learning system inside the blackbox of being constrained that if the languages are the same, the output has to be identical to the input?\nObviously, the blackbox could quickly learn that bypassing the translation does the trick. However, that would probably require the internal circuitry to allow for the bypass, and that could be constrained out. So:\n Could we expect any interesting result? Could the input to be eventually forced down to a language-independent universal representation? Let’s say there is a language-independent universal representation kernel. If the input comes in without information of which is the output language, and the output has no information of what the input language was, does it force the network to create a universal representation, or would it just withered away? Is it possible to invert a network? Probably not in a truly bijective way, but to model the fact that text representation \\(\\rightarrow\\) universal representation is the inverse (for some definition of the word) of universal representation \\(\\rightarrow\\) text representation of the same language?  Comments welcome.\n","date":1571961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571961600,"objectID":"cbcaee54e2102059cabb8c1b3b2e47bc","permalink":"https://emmanuel-r8.github.io/post/2019/10/25/quick-idea-universal-translator-and-same-language-translator/","publishdate":"2019-10-25T00:00:00Z","relpermalink":"/post/2019/10/25/quick-idea-universal-translator-and-same-language-translator/","section":"post","summary":"Quick Thoughts are random thoughts looking for comments\n Let’s imagine a universal translator able to translate any language to any language. Sourcing a corpus of pair translation is a major hurdle.","tags":["Quick Thought","Machine Learning","Neural Network","Statistical Learning","What About?"],"title":"Quick Thought: Universal translator and same language translator","type":"post"},{"authors":null,"categories":["Machine Learning","Neural Network"],"content":"   DRAFT 1 Background Singular matrix decomposition  Where next?  Back to SVD Regularisation  Vector coordinates Eigenvalues Threshold 2-by-2 decision matrix  [TODO] Other Principal Components methods Limitations and further questions  Limitations Further questions  Litterature   DRAFT 1 We all have laptops. But le’ts face it, even in times of 32GB of RAM and NVMe2 drives, forget about running any interesting TensorFlow model. You need to get an external GPU, build your own rig, or very quickly pay a small fortune for cloud instances.\nBack in 1993, I read a paper about growing neural networks neuron-by-neuron. I have no other precise recollection about this paper apart from the models considered being of the order of 10s of neurons and the weight optimisation being made on a global basis, i.e. not layer-by-layer like backpropagation. Nowadays, it is still too often the case that finding a network structure that solves a particular problem is a random walk: how many layers, with how many neurons, with which activation functions? Regularisation methods? Drop-out rate? Training batch size? The list goes on.\nThis got me thinking about how a training heuristic could incrementally modify a network structure given a particular training set and, apart maybe from a few hyperparameters, do that with no external intervention. At regular training intervals, a layer1 will be modified depending on what it seems able or not to achieve. As we will see, we will use unsupervised learning methods to do this: a layer modification will be independent of the actual learning problem and automatic.\nMany others have looked into that. But what I found regarding self-organising networks is pre-2000, and nothing in the context of deep learning. So it seems that the topic has gone out of fashion because of the current amounts of computing power, or has been set aside for reasons unknown. (See references at the end). In any event, it is interesting enough a question to research it.\n Background Let us look at a simple 1-D layer and decompose what it exactly does. Basically a layer does:\n\\[ \\text{ouput} = f(M \\times \\text{input}) \\]\nIf the input \\(I\\) has size \\(n_I\\), the output \\(O\\) has size \\(n_I\\), and \\(f\\) being the activation function, we have (where \\(\\odot\\) represents the matrix element-wise application of a function):\n\\[ O = f \\odot (M \\times I) \\]\nThen, looking at \\(M\\), what does it really do? At one extreme, if \\(M\\) was the identity matrix, it would essentially be useless (bar the activation function2). This would be a layer candidate for deletion. The question is then:\n Looking at the matrix representing a layer, can we identify which parts are (1) useless, (2) useful and complex enough, or (3) useful but too simplistic?.\n Here, complex enough or simplistic is basically a synonym of “one layer is enough”, or “more layers are necessary”.\nThe idea to look for important/complex information which where the network needs to grow more complex; and identify trivial information which can be discarded, or can be viewed as minor adjustments to improve error rates (basically overfitting…)\nCaveat: Note that we ignore the activation function. They are key to introduce non-linearity. Without it, a network is only a linear function, i.e. no interest. They have a clear impact on the performance of a network.\n Singular matrix decomposition There exists many ways to decompose a matrix. Singular matrix decomposition (SVD) \\(M = O \\Sigma I^\\intercal\\) is an easy and efficient way to interpret what a given matrix does. SVD builds on the eigenvectors (expressed in an orthonormal basis), and eigenvalues. (Note that \\(M\\) is real-valued, so we use the transpose notation \\(M^\\intercal\\) instead of the conjugate transpose \\(M^*\\).)\nIn a statistical world, SVD (with eigenvalues ordered by decreasing value) is how to do principal component analysis(PCA).\nIn a geometrical context, SVD:\n takes a vector (expressed in the orthonormal basis); re-expresses onto a new basis made of the eigenvectors (that would only exceptionally be orthonormal); dilates/compresses those components by the relevant eigenvalues; and returns this resulting vector expressed back onto the orthonormal basis.  As presented here, this explanation requires a bit more intellectual gymnastic when the matrix is not square (i.e. when the input and output layers have different dimensions), but the principle remains identical.\nWhere next? Taking the statistical and geometrical points of view together, the layer (matrix \\(M\\)) shuffles the input vector in its original space space where some specific directions are more important than others. Those directions are linear combinations of the input neurons, each combinations is along the eigenvectors. Those combinations are given more or less importance as expressed by the eigenvalues. (Note that the squares of the eigenvalues expressed how much information each combination brings to the table.)\nIntuitively, the simplest and most useless \\(M\\) would be the identity matrix (the input units are repeated), or zero matrix (the input units are dropped because useless). Let us repeat the caveat that the activation function is ignored.\nIf compared to the identity matrix, the SVD shows that \\(M\\) includes (at least) two types of important information identified:\n What are interesting combinations of the input units? This is expressed by how much the input vector is rotated in space. Independently from whether a combination is complicated or not (i.e. multiple units, or unit passthrough), how an input is amplified (as expressed by the eigenvalues).  The idea is then produce a 2x2 decision matrix with high/low rotation mess and high/low eigenvalues.\nA picture is gives the intuition of what we are after:\nTransformation of the Layer Matrix\n Looking from top to bottom at what the “after” matrices would be:\n Part of the original layer, immediately followed by a new one (we will see below what that would look like). The intuition is that this layer is really messing things up down the line, or seems very sensitive.\n Part of the original layer where the number of units would be increased (here doubled as an example).\n Part of the original layer kept functionally essentially as is.\n Delete the rest which is either not sensitive to input or outputs nothings. This would be within a certain precision. That is basically a form of regularisation preventing the overall model to be too sensitive. I am aware that there are other types of regularisations, but that will go in the limitations category.\n  The next layer would take as input all the transformed outputs.\nIn practice, the picture presents the matrices separated. This is for ease of understanding. In reality the same effect would be achieved if the three dark blue sub-layers are merged in a single layer.\n  Back to SVD Let us assume that there are \\(n\\) input units and \\(m\\) output units. \\(M\\) then is of dimensions \\(m \\times n\\). The matrices of the SVD have dimensions:\n\\[ \\begin{matrix} M \u0026amp; = \u0026amp; O \u0026amp; \\Sigma \u0026amp; I^\\intercal \\\\ m \\times n \u0026amp; \u0026amp; m \\times m \u0026amp; m \\times n \u0026amp; n \\times n \\\\ \\end{matrix} \\]\nNote that instead of using \\(U\\) and \\(V\\) to name the sub-matrices of the SVD, we use \\(I\\) and \\(O\\) to represent input and output.\nThe \\(I\\) and \\(O\\) can be written as:\n\\[ I = \\begin{pmatrix} | \u0026amp; \u0026amp; | \\\\ i_1 \u0026amp; \\cdots \u0026amp; i_m \\\\ | \u0026amp; \u0026amp; | \\\\ \\end{pmatrix} \\qquad \\text{and} \\qquad O = \\begin{pmatrix} | \u0026amp; \u0026amp; | \\\\ o_1 \u0026amp; \\cdots \u0026amp; o_n \\\\ | \u0026amp; \u0026amp; | \\\\ \\end{pmatrix} \\]\nThen:\n\\[ \\begin{aligned} M \u0026amp; = O \\Sigma I^\\intercal \\\\ \u0026amp; = \\begin{pmatrix} | \u0026amp; \u0026amp; | \\\\ o_1 \u0026amp; \\dots \u0026amp; o_m \\\\ | \u0026amp; \u0026amp; | \\\\ \\end{pmatrix} \\times \\\\ \u0026amp; \\times \\begin{pmatrix} \\sigma_1 \\\\ \u0026amp; \\sigma_2 \\\\ \u0026amp;\u0026amp; \\ddots \\\\ \u0026amp;\u0026amp;\u0026amp; \\sigma_r \\\\ \u0026amp;\u0026amp;\u0026amp;\u0026amp; 0 \\\\ \u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp; \\ddots \\\\ \u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp; 0 \\\\ \\end{pmatrix} \\times \\\\ \u0026amp; \\times \\begin{pmatrix} - \u0026amp; i_1 \u0026amp; - \\\\ \u0026amp; \\vdots \u0026amp; \\\\ - \u0026amp; i_n \u0026amp; - \\\\ \\end{pmatrix} \\end{aligned} \\]\nwhere \\(\\Sigma\\) has \\(r\\) non-zero eigenvalues.\n Regularisation At this stage, we can regularise all components.\nVector coordinates For each vector \\(i_k\\) or \\(o_k\\), we could zero its coordinates when below a certain threshold (in absolute value). All the coordinates will \\(-\\) and \\(1\\) since each vector has norm 1 (\\(I\\) and \\(O\\) are orthonormal), therefore all of them will be regularised in similar ways.\nAfter regularisation, the matrices will not be orthonormal anymore. They can easily be made normal by scaling up by \\(\\frac{1}{\\sum_{k}i_k^2}\\) or \\(\\frac{1}{\\sum_{k}o_k^2}\\). There is no generic way to revert to an orthogonal basis and keep the zeros.\nWe need a way to measure the rotation messiness of each vector. As a shortcut, we can use the proportion of non-zero vector coordinates (after de minimis regularisation).\n Eigenvalues The same can be done for the \\(\\sigma\\)s. As an avenue of experimentation, those values can not only be zero-ed in places, but also rescale the large values in some non-linear way (e.g. logarithmic or square root rescaling).\n Threshold Where to set the threshold is to be experimented with. Mean? Median since more robust? Some quartile?\n 2-by-2 decision matrix Based on those regularisation, we would propose the following:\n\\[ \\begin{matrix} \u0026amp; \\text{low rotation messiness} \u0026amp; \\text{high rotation messiness} \\\\ \\text{high } \\sigma \u0026amp; \\text{Double height} \u0026amp; \\text{Double depth} \\\\ \\text{low } \\sigma \u0026amp; \\text{Delete} \u0026amp; \\text{Keep identical} \\\\ \\end{matrix} \\]\n  [TODO] Other Principal Components methods SVD is PCA. Projects information on hyperplanes.\nReflect on non-linear versions: Principal Curves, Kernel Principal Components, Sparse Principal Components, Independent Component Analysis. (_Elements of Statistical Learning s. 14.5 seq.).\n Limitations and further questions Limitations  Only 1-D layers. Higher-order SVD is in principle feasible for higher order tensors. Other methods?\n We delete the eigenvectors associated to low eigenvalues and limited rotations. There are other forms of regularisations, e.g. random weight cancelling that would not care about anything eigen-.\n What is the real impact of ignoring the activation function? PCA requires centered values. Geometrically, uncentered values would mean more limited rotations since samples would be in quadrant far from 0.\n   Further questions  The final structure is a direct product of the training set. What if the training is done differently (batches sized or ordered differently)?\n What about training many variants with different subsets of the training set and using ensemble methods?\n The eigenvalues could be modified when creating the new layers. By decreasing the highest eigevalues (in absolute value), we effectively regularise the layers outputs. This decrease could bring additional non-linearity if the compression ratio depends on the eigengevalue (e.g. replacing it by it square root). And this non-linearty would not bring additional complexity to the back-propagation algorithm, or auto-differentiated functions: it only modifies the final values if the new matrices.\n    Litterature Here are a few summary litterature references related to the topic.\nThe Elements of Statistical Learning The ESL top of p 409 proposes PCA to interpret layers, i.e. to improve the interpretability of the decisions made by a network.\n Neural Network Implementations for PCA and Its Extensions http://downloads.hindawi.com/archive/2012/847305.pdf\nUses neural networks as a substitute for PCA.\n An Incremental Neural Network Construction Algorithm for Training Multilayer Perceptrons Aran, Oya, and Ethem Alpaydin. “An incremental neural network construction algorithm for training multilayer perceptrons.” Artificial Neural Networks and Neural Information Processing. Istanbul, Turkey: ICANN/ICONIP (2003).\nhttps://www.cmpe.boun.edu.tr/~ethem/files/papers/aran03incremental.pdf\n Kohonen Maps https://en.wikipedia.org/wiki/Self-organizing_map\n Self-Organising Network A Self-Organising Network That Grows When Required (2002) https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.8763\n The Cascade-Correlation Learning Architecture https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.125.6421\nGrowth with quick freeze as a way to avoid the expense of back-propagation.\n SOINN：Self-Organizing Incremental Neural Network http://www.haselab.info/soinn-e.html https://cs.nju.edu.cn/rinc/SOINN/Tutorial.pdf\nSeems focused on neuron by neuron evolution.\n    We will only consider modifying the network layer by layer, not neuron by neuron.↩︎\n This could actually be a big limitation of this discussion. In reality, even an identity matrix yields changes by piping the inputs through a new round of non-linearity, which is not necessarily identical to the preceding layer↩︎\n   ","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571788800,"objectID":"422789d1a19878673bf150e5dae84d5f","permalink":"https://emmanuel-r8.github.io/post/2019/10/23/neural-network-incremental-growth/","publishdate":"2019-10-23T00:00:00Z","relpermalink":"/post/2019/10/23/neural-network-incremental-growth/","section":"post","summary":"DRAFT 1 Background Singular matrix decomposition  Where next?  Back to SVD Regularisation  Vector coordinates Eigenvalues Threshold 2-by-2 decision matrix  [TODO] Other Principal Components methods Limitations and further questions  Limitations Further questions  Litterature   DRAFT 1 We all have laptops.","tags":["Machine Learning","Neural Network","What About?"],"title":"Neural Network - Incremental Growth","type":"post"},{"authors":null,"categories":["Data Science"],"content":"  I recently finished to penultimate final assignment for the HarvardX Data Science course. The Stanford course was clearly machine learning. This one is definitely lighter on the machine learning and much heavier on the data science: how to source, clean and visualise data are key skills. The targeted knowledge is more traditional probabilities/statistics. Long-existing fundamental techniques like inference, polling are there.\nThis time R is the centre tool of the course. It makes clear sense. When I started learning it about 15 years ago, I loathed the multiple gotchas. Since then, new libraries have simplified base R and removed its exceptions and exceptions to exceptions. In addition the Rcpp library has eased implementation of efficient algorithms and interfacing with popular libraries. Still not a speed demon, but not the snail it used to be.\nI won’t go through the project and my models. No revolutionary concepts. Just great results. I took half a day to reimplement in Julia, both to crosscheck and personal training. As expected, a lot easier to read. But the big surprise was the speed difference. Although I didn’t time it, Julia only felt about twice quicker. Credit to the R project folks (I only used matrices operations, no modeling libraries).\nOn this report, I got grades that can’t be improved upon. Happy camper.\n","date":1570233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570233600,"objectID":"7b397a47bca288261fd4b830f32f06ed","permalink":"https://emmanuel-r8.github.io/post/2019/10/05/harvardx-data-science-course-first-final-project/","publishdate":"2019-10-05T00:00:00Z","relpermalink":"/post/2019/10/05/harvardx-data-science-course-first-final-project/","section":"post","summary":"I recently finished to penultimate final assignment for the HarvardX Data Science course. The Stanford course was clearly machine learning. This one is definitely lighter on the machine learning and much heavier on the data science: how to source, clean and visualise data are key skills.","tags":["Data Science","R","Statistical Learning"],"title":"HarvardX Data Science course - First final project","type":"post"},{"authors":null,"categories":["Machine Learning"],"content":"   Review Exercises and grading Summary   Review I recently completed the Stanford online version of the Machine Learning CS229 course taught by Andrew Ng. There is no need to introduce this course which has reached stardom.\nIt often was a trip a trip down memory lane repeating what I studied in the late 90’ies. It was interesting that quite a bit has remained as relevant. Back then, and I am now talking early 90ies, neural networks were still fashionable but computationally intractable past what would hardly be considered a single layer nowadays. Backpropagation was already used, but similarly quickly tedious.\nEnough recalling old times… There was plenty I had not done back then.\nThe course was extremely pleasant. The progression made sense, pace was enjoyable. In particular, the blackboard style presentation was great. Following along with pen and paper made things easily stick.\nEvery piece of code had to be written in Matlab/Octave. The choice was surprising in those days and age where R has been a mainstay of statistics and statistical learning, and Python is now the language of choice to glue and interface so many optimised C/C++ libraries (in addition to its natural qualities). But the rationale of Matlab/Octave being very natural to implement algorithms where matrices are the mathematical object of choice, made sense. The learning curve was easy, code looked very legible and natural. For short scripts, all good. For anybody who thinks that his/her code will one day be maintained by a psychopath who know his/her address, Matlab/Octave is to be left as a Wikipedia article. Maybe Julia will become a better choice. (Numpy matrix calcs looks very far from mathematical formalism and easy to bug up.)\nThe course was light on the theory side. No surprise: long curriculum, few hours. On the flip side, the recurring emphasis on the ‘what does it mean?’, developing intuitions and, in particular, the hammering about bias/complexity or bias/variance trade-off would be of great value to anyone entering the field. There is a somewhat prevalent meme that machine learning only works because we now have train loads of sdcards of data, and that if something doesn’t quite work, just throw more data at it. Hammering that trade off will hopefully make many become at least sceptical. More data is not a magic wand.\n Exercises and grading The automated grading grading system was surprisingly efficient. There were a few gotchas on exact spelling or white spaces. But overall, no complaints. And given the lack of real-people face-to-face time, this was a nice alternative.\nThe regular coding exams were interesting and the backend infrastructure worked great. As time progressed, the difficulty significantly dropped because of the more difficult content (harder to draft an exercise that really really covers content that was superficially addressed). The course 6 exam was clearly the hardest for many of the students.\n Summary Worth it? On a personal level, definitely. And impossible to beat the value for money.\nAs a carrer-enhancing proposition, it remains to be seen, and I’ll need to see it to believe it.\n ","date":1564704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564704000,"objectID":"0099b6852ce8abb2835ea06d7dfdc50d","permalink":"https://emmanuel-r8.github.io/post/2019/08/02/stanford-online-machine-learning-c229/","publishdate":"2019-08-02T00:00:00Z","relpermalink":"/post/2019/08/02/stanford-online-machine-learning-c229/","section":"post","summary":"Review Exercises and grading Summary   Review I recently completed the Stanford online version of the Machine Learning CS229 course taught by Andrew Ng. There is no need to introduce this course which has reached stardom.","tags":["Octave","Machine Learning","Statistical Learning"],"title":"Stanford Online - Machine Learning C229 ","type":"post"},{"authors":null,"categories":["R"],"content":"   Blogdown Setup Themes   Blogdown I have been a happy user of R markdown and bookdown developed by Yixui Xie. When I decided to start this blog, giving blogdown a try was a no-brainer. To be honest, it was not my first choice. Jekyll was #1 given it’s good support by GitHub pages. Then I took a dive with Pelican. Both are impressive, but both brought equally painful theming: the base theme sort of works, and only sort of, but anyway was not what I wanted. Attempts to use anything else failed. I didn’t have time to dig into the HTML/CSS templates.\nBlogdown just worked out of the box without any sort of caveat.\n Setup Basically, I just followed up the blogdown documentation. As for all his projects, Yixui’s documentation is clear, didactic and shows how much thoughts have gone into making his software easy to use, yet powerful.\nBy defaults, blogdown uses the Hugo, but a Jekyll backend is in beta.\nGreat resources are R Blogdown Setup in GitHub, even its valuable update R Blogdown Setup in GitHub (2).\n Themes As in other solutions, theming is never straightforward. Blogdown uses Hugo themes which cannot always be imported without changes and would need a bit of massaging. Having said that, if you find a them you like, it is just a matter of running blogdown::install_theme(\"REPONAME\"), and the theme will be downloaded and installed in the themes subdirectory. blogdown will automatically change the theme: parameter in the toml configuration file and the site will be re-generated. Easy enough? Bonus points for Hugo that takes under 100ms to do that job.\n ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"bbfaa0a5e132cd5345d353cdb07c01f3","permalink":"https://emmanuel-r8.github.io/post/2019/08/01/hello-blogdown/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/2019/08/01/hello-blogdown/","section":"post","summary":"Blogdown Setup Themes   Blogdown I have been a happy user of R markdown and bookdown developed by Yixui Xie. When I decided to start this blog, giving blogdown a try was a no-brainer.","tags":["R Markdown","blogdown","RStudio"],"title":"Hello Blogdown!","type":"post"}]