<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Emmanuel Rialland">

  
  
  
    
  
  <meta name="description" content="The TransformerXL Algorithm for a single layer with matrix dimension Notation summary Useful references   This is part 2 of a discussion of the DeepMind paper about Compressive Transformers for Long-Range Sequence Modelling.">

  
  <link rel="alternate" hreflang="en-us" href="/2020/12/30/rnn-compressive-memory-part-2-the-transformerxl.html">

  


  
  
  
  <meta name="theme-color" content="rgb(0, 136, 204)">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
    
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
    
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-150743827-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-150743827-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/2020/12/30/rnn-compressive-memory-part-2-the-transformerxl.html">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Back2Numbers">
  <meta property="og:url" content="/2020/12/30/rnn-compressive-memory-part-2-the-transformerxl.html">
  <meta property="og:title" content="RNN Compressive Memory Part 2: The TransformerXL | Back2Numbers">
  <meta property="og:description" content="The TransformerXL Algorithm for a single layer with matrix dimension Notation summary Useful references   This is part 2 of a discussion of the DeepMind paper about Compressive Transformers for Long-Range Sequence Modelling."><meta property="og:image" content="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-12-30T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-12-30T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/2020/12/30/rnn-compressive-memory-part-2-the-transformerxl.html"
  },
  "headline": "RNN Compressive Memory Part 2: The TransformerXL",
  
  "datePublished": "2020-12-30T00:00:00Z",
  "dateModified": "2020-12-30T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Emmanuel Rialland"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Back2Numbers",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "The TransformerXL Algorithm for a single layer with matrix dimension Notation summary Useful references   This is part 2 of a discussion of the DeepMind paper about Compressive Transformers for Long-Range Sequence Modelling."
}
</script>

  

  


  


  





  <title>RNN Compressive Memory Part 2: The TransformerXL | Back2Numbers</title>

</head>
<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="dark">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  









<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Back2Numbers</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Back2Numbers</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks"><span>Recent talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>#> whoami </span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact me </span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>RNN Compressive Memory Part 2: The TransformerXL</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    30 December 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    4 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="/2020/12/30/rnn-compressive-memory-part-2-the-transformerxl.html#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/machine-learning.html">Machine Learning</a>, <a href="/category/neural-network.html">Neural Network</a></span>
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      

<div id="TOC">
<ul>
<li><a href="#the-transformerxl">The TransformerXL</a></li>
<li><a href="#algorithm-for-a-single-layer-with-matrix-dimension">Algorithm for a single layer with matrix dimension</a></li>
<li><a href="#notation-summary">Notation summary</a></li>
<li><a href="#useful-references">Useful references</a></li>
</ul>
</div>

<p>This is part 2 of a discussion of the DeepMind paper about <a href="https://arxiv.org/abs/1911.05507">Compressive Transformers for Long-Range Sequence Modelling</a>.</p>
<ul>
<li><p><a href="../../03/07/rnn-compressive-memory-part-1.html">Part 1</a>: A high level introduction to Compressive Memory mechanics starting from basic RNNS;</p></li>
<li><p>Part 2 (here): more details about the TransformerXL;</p></li>
<li><p>Part 3: an implementation using PyTorch (soon);</p></li>
<li><p>Part 4: finally, its application to time series (soon).</p></li>
</ul>
<p>This post describes the building blocks up to what makes a <a href="https://arxiv.org/abs/1901.02860">TransformerXL</a> model as presented in June 2019. The compression mechanics are an add-on to the TransformerXL which therefore need to be detailed. Along the way, it will become clear that, in many ways, the implementation described only applies to embeddings, i.e. one-hot encodings, and not to to truly multi-dimensional variable inputs.</p>
<div id="the-transformerxl" class="section level2">
<h2>The TransformerXL</h2>
<p><span class="math display">\[
\begin{array}{l}
\text{Notation:} &amp; \\
\tau &amp; \text{time step or index on successive segments} \\
b &amp; \text{batch size} \\
h &amp; \text{number of heads} \\
s_s &amp; \text{length of a segment} \\
s_m &amp; \text{length of the memory} \\
s = s_m + s_s &amp; \text{length of segment + memory} \\
d &amp; \text{size of an embedding} \\
d_{in} &amp; \text{size of the embedding as input to a given layer} \\
d_{out} &amp; \text{size of the embedding as output from a given layer} \\
(b, h, s, d): &amp; \text{tensor of 4 dimension reflecting those dimensions}\\
l &amp; \text{index of a layer} \\
\end{array}
\]</span></p>
<p>The number of parameters for this type of model is extremely large.</p>
</div>
<div id="algorithm-for-a-single-layer-with-matrix-dimension" class="section level2">
<h2>Algorithm for a single layer with matrix dimension</h2>
<p>The implementation will use the <a href="https://pytorch-lightning.readthedocs.io/en/latest/">Pytorch Lightning</a> library. The entire code is on <a href="https://github.com/Emmanuel-R8/Time_Series_Transformers">Github</a>.</p>
<pre class="python"><code>import sys
import inspect

from typing import *

import torch
import torch.nn as nn

from pytorch_lightning.core.lightning import LightningModule

from utils.exp_utils import logging</code></pre>
<pre class="python"><code># From https://github.com/harvardnlp/annotated-transformer
class PositionalEncoding(LightningModule):
    &quot;Implement the edited PE function, depends on sequence length rather than input dimensionnality.&quot;

    def __init__(self, runParam, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        batch

        # Compute the positional encodings once in log_2 space ceiled to sequence_length.
        b = math.ceil(math.log(max_sequence_length * 4, 2))
        a = int(2**b / 4)  # Up to a quarter of a sine wave
        x1 = np.array([[math.cos(0.5**i*x*2*math.pi) for x in range(max_sequence_length, 0, -1)] for i in range(1, b+1)])
        x2 = np.array([[math.sin(0.5**i*x*2*math.pi) for x in range(max_sequence_length, 0, -1)] for i in range(2, b+2)])
        x = np.concatenate([x1, x2], axis=0)
        print(&quot;x.shape():&quot;, x.shape)
        x = np.expand_dims(x, 0).repeat(repeats=batch_size, axis=0)
        print(&quot;x.shape():&quot;, x.shape)

        # Register it into PyTorch
        pe = torch.from_numpy(x).float()
        pe = pe.transpose(-1, -2)
        print(&quot;pe.size():&quot;, pe.size())
        self.register_buffer(&#39;pe&#39;, pe)

    def forward(self, x):
        pos = Variable(self.pe, requires_grad=False)
        # print(pos.size(), x.size())  # [batch_size, -1, sequence_length], [batch_size, sequence_length, hidden_size]
        pe = self.pe[:, -x.size(1):]  # limiting positional encoding to a poentially shorter sequence_length
        print(&quot;pe.size(), x.size():&quot;, pe.size(), x.size())
        x = torch.cat([x, pe], dim=-1)
        return self.dropout(x), pos</code></pre>
<p><span class="math display">\[
\begin{array}{l}
\text{Step for a} &amp; &amp; \text{Dimensions for multiple layers} \\
\text{single layer} &amp; &amp; \text{(with Einstein summation)} \\
\hline \\
h^{l-1}_\tau &amp; &amp; (l, h, s_s, d_{in}) \\
\hline \\
\tilde{h}^{l-1}_\tau = &amp; \left[ StopGradient(m^{l-1}_\tau) \circ  h^{l-1}_\tau \right]  &amp; (l, h, s, d_{in}) = (l, h, s_m, d_{in}) \circ (l, h, s_s, d_{in}) \\
\hline \\
q^l_\tau = &amp; h^{l-1}_\tau \cdot {W^l_Q}^\top &amp; (l, h, s_s, d_{in}) = (l, h, s_s, d_{in}) \dot {(l, h, d_{in}, d_{in})}^\top \\
&amp; &amp; lhsd_1,lhsd_1d_2 \rightarrow lhsd_2 \\
\hline \\
k^l_\tau = &amp; \tilde{h}^{l-1}_\tau \cdot {W^l_K}^\top &amp; (l, h, s, d_{in}) = (l, h, s, d_{in}) \dot {(l, h, d_{in}, d_{in})}^\top \\
&amp; &amp; lhsd_1,lhsd_1d_2 \rightarrow lhd_2 \\
\hline \\
v^l_\tau = &amp; \tilde{h}^{l-1}_\tau \cdot {W^l_V}^\top &amp; (l, h, s, d_{in}) = (l, h, s, d_{in}) \dot {(l, h, d_{in}, d_{in})}^\top \\
&amp; &amp; lhsd_1,lhsd_1d_2 \rightarrow lhd_2 \\
\hline \\
A^l_\tau = &amp; {Q^l}^\top \dot \\
\end{array}
\]</span></p>
</div>
<div id="notation-summary" class="section level2">
<h2>Notation summary</h2>
<p><strong>Size parameters</strong></p>
<p><span class="math display">\[
\begin{array}{ll}
l:        &amp;   \text{number of layers, starting from 1} \\
d:        &amp;   \text{dimension of all input (including embeddings) AND output vectors passed from a layer to the next} \\
n_{head}: &amp;   \text{number of self-attention heads in a TransformerXL layer} \\
n_s:      &amp;   \text{size of a training segment = number of inputs into and outputs from a layer} \\
s = n_s:  &amp;   s \text{ will be used as index name to represent the input dimension when using named tensors,}  \\
&amp; \text{i.e. Einstein summation.}  \\
b:        &amp;   \text{size of a training batch, i.e. number of segments in a batch. Used in named tensors as well} \\
\\
n_m:      &amp;   \text{number of hidden states kept in the primary memory. n_m is a multiple of n_s} \\
c:        &amp;   \text{memory compression ratio} \\
n_{cm}:   &amp;   \text{number of states kept in the compressed memory. n_{cm} }  \\
\end{array}
\]</span></p>
</div>
<div id="useful-references" class="section level1">
<h1>Useful references</h1>
<ul>
<li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li>
<li><a href="https://www.fromkk.com/posts/the-annotated-the-annotated-transformer/">The Annotated The Annotated Transformer</a></li>
<li><a href="https://d2l.ai/chapter%5Fattention-mechanisms/transformer.html">Dive into Deep Learning - 10.3 Transformer</a></li>
</ul>
</div>

    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/machine-learning.html">Machine Learning</a>
  
  <a class="badge badge-light" href="/tag/neural-network.html">Neural Network</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/2020/12/30/rnn-compressive-memory-part-2-the-transformerxl.html&amp;text=RNN%20Compressive%20Memory%20Part%202:%20The%20TransformerXL" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/2020/12/30/rnn-compressive-memory-part-2-the-transformerxl.html&amp;t=RNN%20Compressive%20Memory%20Part%202:%20The%20TransformerXL" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=RNN%20Compressive%20Memory%20Part%202:%20The%20TransformerXL&amp;body=/2020/12/30/rnn-compressive-memory-part-2-the-transformerxl.html" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/2020/12/30/rnn-compressive-memory-part-2-the-transformerxl.html&amp;title=RNN%20Compressive%20Memory%20Part%202:%20The%20TransformerXL" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=RNN%20Compressive%20Memory%20Part%202:%20The%20TransformerXL%20/2020/12/30/rnn-compressive-memory-part-2-the-transformerxl.html" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/2020/12/30/rnn-compressive-memory-part-2-the-transformerxl.html&amp;title=RNN%20Compressive%20Memory%20Part%202:%20The%20TransformerXL" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/emmanuel-rialland/avatar_hu52a603635ecebd45650b162dadabb4e5_12861_270x270_fill_q90_lanczos_center.jpg" alt="Emmanuel Rialland">
      

      <div class="media-body">
        <h5 class="card-title"><a href="/">Emmanuel Rialland</a></h5>
        <h6 class="card-subtitle">Consultant Finance - Machine Learning</h6>
        
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:Emmanuel.Rialland@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/emmanuelrialland" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/emmanuel-r8" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://emmanuel-r8.github.io/" target="_blank" rel="noopener">
        <i class="fas fa-blog"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://t.me/EmmanuelRialland" target="_blank" rel="noopener">
        <i class="fab fa-telegram"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  







<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "https-emmanuel-r8-github-io" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>








  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/2020/12/31/rnn-compressive-memory-part-3-the-compression-algorithm.html">RNN Compressive Memory Part 3: The Compression algorithm</a></li>
      
      <li><a href="/2020/03/07/rnn-compressive-memory-part-1.html">RNN Compressive Memory Part 1: A high level introduction.</a></li>
      
      <li><a href="/2019/10/23/neural-network-incremental-growth.html">Neural Network - Incremental Growth</a></li>
      
      <li><a href="/2019/10/25/quick-idea-universal-translator-and-same-language-translator.html">Quick Thought: Universal translator and same language translator</a></li>
      
      <li><a href="/2020/07/31/normalising-flows.html">Normalising Flows</a></li>
      
    </ul>
  </div>
  





  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.8/mermaid.min.js" integrity="sha256-lyWCDMnMeZiXRi7Zl54sZGKYmgQs4izcT7+tKc+KUBk=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/julia.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    <script id="dsq-count-scr" src="https://https-emmanuel-r8-github-io.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.66c553246b0f279a03be6e5597f72b52.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/privacy.html">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms.html">Terms</a>
    
  </p>
  

  <p class="powered-by">
    © Emmanuel Rialland 2020
  </p>

  
  






  <p class="powered-by">
    
    Published with
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic Website Builder</a>
    

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>
  

</body>


</html>
