<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Emmanuel Rialland">

  
  
  
    
  
  <meta name="description" content="The TransformerXL  Background  Compressive Memory  Background Model overall structure  Memory compression  Compression function Loss function Model timing Attention-Reconstruction Loss  Conclusion Notation summary   This is part 3 of a discussion of the DeepMind paper about Compressive Transformers for Long-Range Sequence Modelling.">

  
  <link rel="alternate" hreflang="en-us" href="/2020/12/31/rnn-compressive-memory-part-3-the-compression-algorithm.html">

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  

  
  
  
  <meta name="theme-color" content="rgb(0, 136, 204)">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/github.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/github.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    

  

  
  
  
    
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
    
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-150743827-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-150743827-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/2020/12/31/rnn-compressive-memory-part-3-the-compression-algorithm.html">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Back2Numbers">
  <meta property="og:url" content="/2020/12/31/rnn-compressive-memory-part-3-the-compression-algorithm.html">
  <meta property="og:title" content="RNN Compressive Memory Part 3: The Compression algorithm | Back2Numbers">
  <meta property="og:description" content="The TransformerXL  Background  Compressive Memory  Background Model overall structure  Memory compression  Compression function Loss function Model timing Attention-Reconstruction Loss  Conclusion Notation summary   This is part 3 of a discussion of the DeepMind paper about Compressive Transformers for Long-Range Sequence Modelling."><meta property="og:image" content="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-12-31T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-12-31T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/2020/12/31/rnn-compressive-memory-part-3-the-compression-algorithm.html"
  },
  "headline": "RNN Compressive Memory Part 3: The Compression algorithm",
  
  "datePublished": "2020-12-31T00:00:00Z",
  "dateModified": "2020-12-31T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Emmanuel Rialland"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Back2Numbers",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "The TransformerXL  Background  Compressive Memory  Background Model overall structure  Memory compression  Compression function Loss function Model timing Attention-Reconstruction Loss  Conclusion Notation summary   This is part 3 of a discussion of the DeepMind paper about Compressive Transformers for Long-Range Sequence Modelling."
}
</script>

  

  


  


  





  <title>RNN Compressive Memory Part 3: The Compression algorithm | Back2Numbers</title>

</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="dark ">

  
  
  
    <script>window.wcDarkLightEnabled = true;</script>
  
  
    <script>const isSiteThemeDark = true;</script>
  
  
  <script src="/js/load-theme.js"></script>

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Back2Numbers</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Back2Numbers</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks"><span>Recent talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>#> whoami </span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact me </span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>



  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>RNN Compressive Memory Part 3: The Compression algorithm</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    31 December 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    13 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="/2020/12/31/rnn-compressive-memory-part-3-the-compression-algorithm.html#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/machine-learning.html">Machine Learning</a>, <a href="/category/neural-network.html">Neural Network</a></span>
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#the-transformerxl">The TransformerXL</a>
<ul>
<li><a href="#background">Background</a></li>
</ul></li>
<li><a href="#compressive-memory">Compressive Memory</a>
<ul>
<li><a href="#background-1">Background</a></li>
<li><a href="#model-overall-structure">Model overall structure</a></li>
</ul></li>
<li><a href="#memory-compression">Memory compression</a>
<ul>
<li><a href="#compression-function">Compression function</a></li>
<li><a href="#loss-function">Loss function</a></li>
<li><a href="#model-timing">Model timing</a></li>
<li><a href="#attention-reconstruction-loss">Attention-Reconstruction Loss</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#notation-summary">Notation summary</a></li>
</ul>
</div>

<p>This is part 3 of a discussion of the DeepMind paper about <a href="https://arxiv.org/abs/1911.05507">Compressive Transformers for Long-Range Sequence Modelling</a>.</p>
<ul>
<li><p><a href="../07/rnn-compressive-memory-part-1.html">Part 1</a>: A high level introduction to Compressive Memory mechanics starting from basic RNNS;</p></li>
<li><p>Part 2 (here): a detailed explanation of the TransformerXL;</p></li>
<li><p>Part 3: an implementation using PyTorch;</p></li>
<li><p>Part 4: finally, its application to time series.</p></li>
</ul>
<p>This post describes the building blocks up to what makes a <a href="https://arxiv.org/abs/1901.02860">TransformerXL</a> model as presented in June 2019. The compression mechanics are an add-on to the TransformerXL which therefore need to be detailed. Along the way, it will become clear that, in many ways, the implementation described only applies to embeddings, i.e. one-hot encodings, and not to to truly multi-dimensional variable inputs.</p>
<div id="the-transformerxl" class="section level2">
<h2>The TransformerXL</h2>
<div id="background" class="section level3">
<h3>Background</h3>
<p>The previous part introduced a chronological introduction starting with RNNs. However, it would have been more accurate to present those models as Sequential Neural Network: They are concerned with dealing with sequential pieces of information (individual elements) and capturing their respective interactions. <em>Recurrent</em> networks are but one type of sequential networks.</p>
<p>The first generation Recurrent Neural Networks passes contextual information from one sequential unit to the next within a layer. A unit does not see anything past its preceding neighbour. Bidirectional networks do not feed a unit with information coming from both sides within a single layer, they are a combination of two layers going in opposite directions. RNNs do not capture information as a whole.</p>
<p>The addition of <em>attention</em> is the first mechanism that captured the entire sequence. First, within an RNN layer, each unit receives an input from the preceding layer (or a training segment). Then, an attention layer (an <em>attention head</em>) is fed with each and every output produced by the RNN layer. Each hidden state is weighted by trained parameters and the attention layer produces
Several attention heads can be trained in parallel and combine their results using trained parameters into a final output.</p>
<p>The number of parameters for this type of model is extremely large.</p>
</div>
</div>
<div id="compressive-memory" class="section level2">
<h2>Compressive Memory</h2>
<div id="background-1" class="section level3">
<h3>Background</h3>
<p>From the last post, recall that compressive transformer are an extension to the TransformerXL. TransformerXL trains using segments extracted from all the sequences available in the dataset. It also uses hidden states calculated when training prior segments. In a sense, it seeks to indirectly increase the size of the training segments. Naturally, the model stores past hidden states (past memories) in a buffer of limited size. It has to discard discarding the oldest memories from that primary memory when moving from a segment to the next.</p>
<p>The key addition of the compressive model is to avoid immediately the discarding by compressing those past states in a compressed representation (which itself will eventually be discarded). The training needs to cover the original TransformerXL and the quality of the compression mechanics. When training a typical network, the values of all the parameters are simultaneously optimised using the gradients calculated for each of those parameters. Instead, this model is trained as 2 separate models:</p>
<ul>
<li><p>The Transformer-XL layers are trained using a current segment, and the primary and compressed memories to train the cells’ parameters (and only those!). During that stage, the compressed memory parameters is given as a constant: no gradients are calculated. Conceptually, this is the normal TransformerXL where the inputs are the concatenation of the segment embeddings and all the memories (primary and compressed).</p></li>
<li><p><em>Separately</em>, another training process looks for an optimal compressed representation of past memories that would otherwise be discarded. Training the compression mechanism assumes that the transformer cells’ parameters are fixed and constant. This stage is only concerned with the efficiency of the representation. Note that certain compression mechanics, like pooling, have no parameters to optimise (although some hyperparameters might be involved).</p></li>
</ul>
</div>
<div id="model-overall-structure" class="section level3">
<h3>Model overall structure</h3>
<p>The authors describe the overall algorithm at a high level. We will retain the same notations, which are summarised at the end of of the post, but we will use a slightly different vocabulary. In particular, a <em>training set</em> (for example a full text) is split into ordered <em>sequences</em> that for a conceptual whole (a full sentence). Each sequence is fed into the model which has a limited number of inputs. If the size of a sequence is too large, it is split into <em>segments</em>. The size of a segment is the number of input of the model. The key attraction of the compressive memory is to lengthen the memory of a TransformerXL: for an identical computation budget, the paper shows that it is valuable to shorten the segments (fewer self-attention heads), in exchange for keeping longer compressed past memories (the self-attention heads become larger to give attention to compressed memories).</p>
<div class="figure">
<img src="/post/2020-03-RNN-compressive-memory/Compressive-template.png" alt="" />
<p class="caption"><strong>Compressive TransformerXL</strong></p>
</div>
<p>To complete the notation, we use the following form of indices: <span class="math inline">\(h^{(l), v}_t\)</span> which should be read as <span class="math inline">\(h^{(\text{Layer index}), \text{Segment index}}_{\text{Time step}}\)</span>. In addition, <span class="math inline">\(h^{(1)}\)</span> represents the inputs into the first layer, i.e. equals the current training segment <span class="math inline">\(x\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>The model starts with transformer layers (<span class="math inline">\(l\)</span> layers). (see step (a) on the diagram).</p></li>
<li><p>Each layer takes a number of entries (a segment of <span class="math inline">\(n_s\)</span> inputs), each being a vector of dimension <span class="math inline">\(d\)</span>. Note that <span class="math inline">\(d\)</span> is the dimension of the initial input embeddings, and of all input/output vectors of further layers. This could however be a flexible model parameter.</p></li>
<li><p>Each individual transformer layer has an identical structure with the same number of self-attention heads (<span class="math inline">\(n_{head}\)</span> heads). Recall that an attention head is a triplet of matrices <span class="math inline">\((W^{(i)}_q, W^{(i)}_k, W^{(i)}_v)\)</span> and a multi-layer perceptron. <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p></li>
<li><p>At each iteration, the self-attention heads are trained using the outputs from the previous layer, and primary and compressed memories.</p></li>
<li><p>After an input is processed by a layer, it is pushed into memory. The memory size <span class="math inline">\(n_m\)</span> has the capacity to store a number <span class="math inline">\(n_{slots}\)</span> of segments. Therefore <span class="math inline">\(n_m = n_{slots} \times n_s\)</span>.</p></li>
<li><p>The new memory pushes the oldest one out, which is compressed into a compressed memory with a <span class="math inline">\(c\)</span> compression ratio. the compressed memory can hold <span class="math inline">\(n_{old}\)</span> compressed memories. <span class="math inline">\(n_{cm}\)</span> is a multiple of <span class="math inline">\(n_{old}\)</span>.</p></li>
</ol>
</div>
</div>
<div id="memory-compression" class="section level2">
<h2>Memory compression</h2>
<p>Let’s focus on Step (f). The compression mechanics are optimised independently from the TransformerXL layers and require choosing a compression scheme (function <span class="math inline">\(f_c\)</span>) and, if there are parameters to be trained, a loss function to minimise.</p>
<div id="compression-function" class="section level3">
<h3>Compression function</h3>
<p>The authors reviewed several choices for the function <span class="math inline">\(f^{(i)}_c\)</span>.</p>
<div id="pooling" class="section level4">
<h4>Pooling</h4>
<p>The simplest is <em>Max and/or mean pooling</em>, where the kernel and stride is set to the compression rate c. This is fast, requires no training (no parameters) and was used as a simple baseline.</p>
</div>
<div id="convolution" class="section level4">
<h4>Convolution</h4>
<p><em>1D-convolution</em> also with kernel and stride set to c. This is defined by parameters which require training.</p>
</div>
<div id="dilated-convolution" class="section level4">
<h4>Dilated convolution</h4>
<p><em>Dilated convolutions</em> which were proposed in 2016 as an alternative to traditional image convolution (for image classification) when dealing with semantic networks. See <a href="https://arxiv.org/abs/1511.07122">Multi-Scale Context Aggregation by Dilated Convolutions</a>. In essence, dilated convolutions are large scale convolutions (large kernel) defined with the same number of parameters as a small kernel. They are better suited to capture distant relationships present in semantic networks.</p>
</div>
<div id="most-used-compression" class="section level4">
<h4>Most-used compression</h4>
<p><em>Most-used</em> compression where the memories are sorted by their average attention(usage) and the most-used are preserved. The most-used compression scheme is inspired from the garbage collection mechanism in the Differentiable Neural Computer (<em>DNC</em>) where low-usage memories are erased. This is described in the 2016 DeepMind paper <a href="https://www.gwern.net/docs/rl/2016-graves.pdf">Hybrid computing using a neural network with dynamic external memory</a>. A DNC is network augmented with memory, where a controller can read and write memory banks. The DNC article is very detailed on the technical aspects. We understand that most-used algorithm is presented in appendix under the <em>Dynamic memory allocation</em> subsection.</p>
</div>
<div id="auto-encoders" class="section level4">
<h4>Auto-encoders</h4>
<p>As we understand, the authors did not consider auto-encoding networks with <span class="math inline">\(c\)</span> fewer parameters. It might be that they were considered but rejected.</p>
</div>
<div id="compression-ratio" class="section level4">
<h4>Compression ratio</h4>
<p>If the compression ratio is <span class="math inline">\(c\)</span>, what are the dimensions of the compressed memory tensors?</p>
<p>The paper leaves this unspecified.</p>
<ul>
<li><p>Pooling and 1D-convolution will produce tensors where the dimension of the attention heads (along the <span class="math inline">\(d\)</span> axis) are divided by <span class="math inline">\(c\)</span>.</p></li>
<li><p>Our understanding of the most-used algorithm would be to zero all values save for a <span class="math inline">\(1 / c\)</span> proportion, but retaining identical dimensions.</p></li>
<li><p>However, we do not see <span class="math inline">\(c\)</span> memory attention heads compressed into a single vector (compression along the <span class="math inline">\(n_s\)</span> axis).</p></li>
</ul>
</div>
</div>
<div id="loss-function" class="section level3">
<h3>Loss function</h3>
<p>Several loss functions were considered.</p>
<ol style="list-style-type: decimal">
<li><p>A whole-model optimisation where all parameters (compression mechanics + TransformerXL Layers) are trained at once using the training set error. However they found the size overwhelming, especially when old compressed memories were unrolled into full-form memories. They then used <em>backpropagating-through-time</em> (BPTT), which by definition does not do full-model optimisation to avoid vanishing/exploding gradients. See <a href="https://d2l.ai/chapter_recurrent-neural-networks/bptt.html">BPTT</a> for a good overview. Note that this is not what we introduced as a two-step optimisation.</p></li>
<li><p>The compression is a form of representation learning / auto-encoding which optimises an encoder (presumably <span class="math inline">\(c\)</span> fewer parameters) by simultaneously training a decoder. The paper tried a <span class="math inline">\(\mathcal{L}_2\)</span> (square) error on the ability to reconstruct the memory tensors. It does not go into detail aside from mentioning it. The posts will not explore this.</p></li>
<li><p>The paper goes into more detail about the two-step training of an <em>attention-reconstruction loss</em>. Transformers optimise attention parameters (the attention head matrices). Optimising for the ability of retaining that information makes complete sense. This loss function is the <span class="math inline">\(\mathcal{L}_2\)</span> error between the attention matrix of a memory slot and reconstructed attention. Next section focuses on the loss function.</p></li>
</ol>
</div>
<div id="model-timing" class="section level3">
<h3>Model timing</h3>
<p>Let’s be clearer about the details of what takes place as time goes by (as segments are presented) and how the compression optimisation proceeds (at the risk of being repetitive).</p>
<ul>
<li><p>The set of the model’s state vectors (the <span class="math inline">\(h^{(i)}\)</span>) is the model’s reaction to a particular input. They are an immediate reaction to an current input. They reflect the values of the current attention heads’ parameters.</p></li>
<li><p>A memory slot is a set of a layer’s state vectors created in the past for a past input when the model’s parameters were different.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> It is the past model’s reaction to something that happened in the past (the input at that time).</p></li>
<li><p>That memory slot reflects the experience of the model at that time (the attention heads parameters). Since then, the model has been trained further. The recent updated model would have yielded a different set of state vectors for that past input.</p></li>
<li><p>Attention heads are not state vectors. They are functions to be trained.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> Those functions, for any input state vectors, will produce a different answer, a different set of state vectors.</p></li>
<li><p>Attention heads are trained and after training they will eventually be constants. However, the the state vectors will change as and when the inputs do.</p></li>
<li><p>The compression is also a function that will be frozen after training which will use (if it relies on parameters).</p></li>
</ul>
<p>Those precisions are relevant because a compression takes place now (i.e. at the time of a current segment). It is applied to a past memory which reflects a past reaction to a past input with a past model. We optimise compression parameters <em>now</em> using values and parameters that are all from the past.</p>
<p>Optimising for attention reconstruction means:</p>
<ul>
<li><p>Right now, the model gives a particular set of current state vectors for a given current input.</p></li>
<li><p>This model has a past memory that is about to be lost (as in a normal TransformerXL).</p></li>
<li><p>In this model, the old memory is about to be compressed. If compressed, the original memory could be reconstructed.</p></li>
<li><p>If the compression (and reconstruction) algorithm is perfect, it should not matter whether the compressed memory (after reconstruction) or the old memory is used.</p></li>
</ul>
<p>The optimisation is therefore to minimise the difference what the model does with the past memory (not yet discarded) and what would happen if the past memory was reconstructed from its compressed form.</p>
<p>The difference between the 2 situations is the loss function to be minimised. Transformers are about attention; the compression algorithm is judged by its ability to retain past attention (what was worthy of attention in the past).</p>
<p>As the paper indicates, this is a lossy objective. The reconstruction is not optimised to replicate the old memory. It is instead optimised to reconstruct an <em>alternative memory</em> that gives a better reconstruction of the attentions. The paper asserts that this gives superior results to the reconstruction of the memory. Keeping in mind that the ultimate purpose of the model is to optimise self-attentions, possible explanations are:</p>
<ul>
<li><p>Memories are used as input into the attention heads to determine the self-attention. So will a reconstructed memory. Therefore, primary memory reconstruction errors are then compounded with the modelling errors of the attention heads’ matrices. Skipping the intermediary avoids this.</p></li>
<li><p>Maybe more interesting, the past memory is mechanically imperfect: it is based on old model parameters. Optimising the reconstruction to reproduce an imperfect memory implicitly means incorporating attention heads’ parameters that were used at the time the memory was created. The reconstructed memory includes the errors of the past attention heads which have since been reduced (hopefully). In other words, the reconstructed memory is an <em>alternative memory</em> which might include later corrections to the model attention heads.</p></li>
</ul>
</div>
<div id="attention-reconstruction-loss" class="section level3">
<h3>Attention-Reconstruction Loss</h3>
<p>For sake of completeness, we include the <em>attention-reconstruction loss</em> algorithm detailed in the paper (<em>Algorithm 2</em> on page 4). <a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>.</p>
<p>Its overall organisation is independent from the actual loss calculation (the stop-gradient mechanics), therefore remains of value. It is replicated here with slight modifications for type consistency. Note that at Step 6 the published algorithm uses a <span class="math inline">\(\sigma\)</span>. This could suggest that a sigmoid activation is used, but it would make no sense to compare attentions calculated with different activation functions (<span class="math inline">\(softmax\)</span> in the layers vs. <span class="math inline">\(sigmoid\)</span> in the the reconstruction).</p>
<p><span class="math display">\[
\begin{array}{ll}
&amp; \textit{Reset the attention loss} \\
&amp; \hline \\
1: &amp; \mathcal{L} \leftarrow 0 \\
\\
&amp; \textit{Main Loop} \\
&amp; \hline \\
&amp; \text{Now that the loss is reset, loop through the layers, one at a time} \\
2: &amp; \textit{for layer }i = 1, 2, \cdots , l \textit{ do} \\
\\
&amp; \textit{Stop all gradients for anything but the compression function parameters} \\
&amp; \hline \\
&amp; \text{TensorFlow uses stop_gradient()} \\
&amp; \text{PyTorch uses .detach()} \\
&amp; \text{We&#39;ll use DetachGradient() to make both happy!} \\
3:  &amp;  DetachGradient(h^{(i)}) \\
4:  &amp;  DetachGradient(M^{(i)}_{t-n_{slots}}) \text{ --- oldest Primary memory slot about to be discarded}  \\
5:  &amp;  DetachGradient(Q, K, V) \text{ --- parameters of all the attention heads matrices}  \\
\\
&amp; \textit{Define the content-based attention being the same calculation as inside the layers}\\
&amp; \hline \\
6: &amp;   Attention(h, m) \leftarrow softmax((hQ).(mK))(mV)  \\
\\
&amp; \textit{Calculate the new compressed memory}\\
&amp; \hline \\
7: &amp;   CM^{(i)}_{1} \leftarrow f^{(i)}_c(M^{(i)}_{t-n_{slots}})   \\
\\
&amp; \textit{Calculate a reconstructed attention from the proposed compressed memory}\\
&amp; \hline \\
7: &amp;   R^{(i)} \leftarrow Reconstruction(CM^{(i)}_{1})  \\
\\
&amp; \textit{Update the loss for the loss at the current layer}\\
&amp; \hline \\
8: &amp;   \mathcal{L} \leftarrow \mathcal{L} + \left| Attention(h^{(i)}, M^{(i)}_{t-n_{slots}}) - Attention(h^{(i)}, R^{(i)}) \right|_2 \\
\\
&amp; \hline \\
&amp; \textit{End loop} \\
&amp; \hline \\
\end{array}
\]</span></p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>This post presents the <em>Compressive Transformers</em> in more details, including the articulation between training the attention heads and training the compression mechanics.</p>
<p>To come in part 3, the code.</p>
</div>
<div id="notation-summary" class="section level2">
<h2>Notation summary</h2>
<p><strong>Size parameters</strong></p>
<p><span class="math display">\[
\begin{array}{ll}
l:        &amp;   \text{number of layers, starting from 1} \\
d:        &amp;   \text{dimension of all input (including embeddings) AND output vectors passed from a layer to the next} \\
n_{head}: &amp;   \text{number of self-attention heads in a TransformerXL layer} \\
n_s:      &amp;   \text{size of a training segment = number of inputs into and outputs from a layer} \\
s = n_s:  &amp;   s \text{ will be used as index name to represent the input dimension when using named tensors,}  \\
&amp; \text{i.e. Einstein summation.}  \\
b:        &amp;   \text{size of a training batch, i.e. number of segments in a batch. Used in named tensors as well} \\
\\
n_m:      &amp;   \text{number of hidden states kept in the primary memory. n_m is a multiple of n_s} \\
c:        &amp;   \text{memory compression ratio} \\
n_{cm}:   &amp;   \text{number of states kept in the compressed memory. n_{cm} }  \\
\end{array}
\]</span></p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>We only consider a wide self-attention model, in part having in mind the application to time series where we will work with embeddings of smaller size. We therefore ignore the narrow self-attention model variant.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>We ignore the possible impact of training in batches.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>We ignore the multi-perceptrons.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Forgive the algorithm formatting. R markdown resisted naive efforts to use Latex algorithms.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/machine-learning.html">Machine Learning`</a>
  
  <a class="badge badge-light" href="/tag/neural-network.html">Neural Network</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/2020/12/31/rnn-compressive-memory-part-3-the-compression-algorithm.html&amp;text=RNN%20Compressive%20Memory%20Part%203:%20The%20Compression%20algorithm" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/2020/12/31/rnn-compressive-memory-part-3-the-compression-algorithm.html&amp;t=RNN%20Compressive%20Memory%20Part%203:%20The%20Compression%20algorithm" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=RNN%20Compressive%20Memory%20Part%203:%20The%20Compression%20algorithm&amp;body=/2020/12/31/rnn-compressive-memory-part-3-the-compression-algorithm.html" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/2020/12/31/rnn-compressive-memory-part-3-the-compression-algorithm.html&amp;title=RNN%20Compressive%20Memory%20Part%203:%20The%20Compression%20algorithm" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=RNN%20Compressive%20Memory%20Part%203:%20The%20Compression%20algorithm%20/2020/12/31/rnn-compressive-memory-part-3-the-compression-algorithm.html" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/2020/12/31/rnn-compressive-memory-part-3-the-compression-algorithm.html&amp;title=RNN%20Compressive%20Memory%20Part%203:%20The%20Compression%20algorithm" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/emmanuel-rialland/avatar_hu52a603635ecebd45650b162dadabb4e5_12861_270x270_fill_q90_lanczos_center.jpg" alt="Emmanuel Rialland">
      

      <div class="media-body">
        <h5 class="card-title"><a href="/">Emmanuel Rialland</a></h5>
        <h6 class="card-subtitle">Consultant Finance - Machine Learning</h6>
        
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:Emmanuel.Rialland@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/emmanuelrialland" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/emmanuel-r8" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://emmanuel-r8.github.io/" target="_blank" rel="noopener">
        <i class="fas fa-blog"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://t.me/EmmanuelRialland" target="_blank" rel="noopener">
        <i class="fab fa-telegram"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  







<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "https-emmanuel-r8-github-io" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>








  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/2020/12/30/rnn-compressive-memory-part-2-the-transformerxl.html">RNN Compressive Memory Part 2: The TransformerXL</a></li>
      
      <li><a href="/2020/03/07/rnn-compressive-memory-part-1.html">RNN Compressive Memory Part 1: A high level introduction.</a></li>
      
      <li><a href="/2019/10/23/neural-network-incremental-growth.html">Neural Network - Incremental Growth</a></li>
      
      <li><a href="/2019/10/25/quick-idea-universal-translator-and-same-language-translator.html">Quick Thought: Universal translator and same language translator</a></li>
      
      <li><a href="/2020/04/30/presentation-at-the-hong-kong-machine-learning-meetup.html">Presentation at the Kong Kong Machine Learning meetup</a></li>
      
    </ul>
  </div>
  





  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.8/mermaid.min.js" integrity="sha256-lyWCDMnMeZiXRi7Zl54sZGKYmgQs4izcT7+tKc+KUBk=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/highlight.min.js" integrity="sha512-7t8APmYpzEsZP7CYoA7RfMPV9Bb+PJHa9x2WiUnDXZx3XHveuyWUtvNOexhkierl5flZ3tr92dP1mMS+SGlD+A==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/julia.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    <script id="dsq-count-scr" src="https://https-emmanuel-r8-github-io.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.d21c308a08df2c7d0039ffaa74c9eaa9.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/privacy.html">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms.html">Terms</a>
    
  </p>
  

  <p class="powered-by">
    © Emmanuel Rialland 2020
  </p>

  
  






  <p class="powered-by">
    
    Published with
    <a href="https://wowchemy.com" target="_blank" rel="noopener">Wowchemy</a>  —
    the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">
    open source</a> website builder that empowers creators.
    

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
