<!DOCTYPE html>
<html class="no-js" lang="en-au">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>RNN Compressive Memory Part 2: The algorithm - Back2Numbers</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	
	
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="shortcut icon" href="/favicon.ico">
		
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-150743827-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">
			<a class="logo__link" href="/" title="Back2Numbers" rel="home">
				<div class="logo__title">Back2Numbers</div>
				
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/whoami/">
				
				<span class="menu__text">#&gt;whoami</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/categories">
				<i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
				<span class="menu__text">Categories</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/tags">
				<i class="sidebar-button-icon fa fa-lg fa-tags"></i>
				<span class="menu__text">Tags</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/archives">
				<i class="sidebar-button-icon fa fa-lg fa-archive"></i>
				<span class="menu__text">Archives</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/about">
				<i class="sidebar-button-icon fa fa-lg fa-question"></i>
				<span class="menu__text">About this site</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/gitbooks/movielens/introduction.html">
				<i class="sidebar-button-icon fa fa-lg fa-question"></i>
				<span class="menu__text">Gitbook Movielens</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/gitbooks/lendingclub/introduction.html">
				<i class="sidebar-button-icon fa fa-lg fa-question"></i>
				<span class="menu__text">Gitbook LendingClub</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/">
				<i class="sidebar-button-icon fa fa-lg fa-home"></i>
				<span class="menu__text">Home</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">RNN Compressive Memory Part 2: The algorithm</h1>
			
		</header><div class="content post__content clearfix">
			


<p>This is part 2 of a discussion of the DeepMind paper about <a href="https://arxiv.org/abs/1911.05507">Compressive Memory of Recurrent Neural Networks</a>.</p>
<p>This post lays out the exact algorithm which is presented at a high level in the paper. It will also draw on the details of the <a href="https://arxiv.org/abs/1901.02860">Transformer-XL</a> model as presented in June 2019.</p>
<div id="high-level-algorithm" class="section level2">
<h2>High-level algorithm</h2>
<div id="background" class="section level3">
<h3>Background</h3>
<p>From the last post, recall that compressive transformer are an extension to the Transformer-XL. Transformer-XL trains using a segment of all the sequences available and hidden states calculated when training previous segments. In a sense, it seeks to indirectly increase the size of the training segments. The key addition is to avoid discarding previous hidden states kept in a primary memory when moving from a segment to the next by compressing those past states in a compressed representation (which will eventually be discarded).</p>
<p>The training is broken down into 2 independent stages that correspond to two sets of parameters to be trained. When training a typical network, the values of all the parameters are simultaneously optimised using the gradients calculated for each of those parameters. Instead, this model should be seen as 2 separate models:</p>
<ul>
<li><p>a set Transformer-XL cells organised in layers using a current segment, and the primary and compressed memories to train the cells’ parameters (and only those!). During that stage, the compressed memory parameters is given as a constant: no gradients are calculated.</p></li>
<li><p><em>Separately</em>, another training process looks for an optimal compressed representation of the hidden states that would otherwise be discarded. Training the compression mechanism assumes that the transformer cells’ parameters are fixed and constant. This stage is only concerned with the efficiency of the representation. Note that certain representation mechanics have no parameters to optimise (e.g. pooling, although some hyperparameters might be involved).</p></li>
</ul>
</div>
<div id="the-authors-algorithm" class="section level3">
<h3>The authors’ algorithm</h3>
<p>The authors describe the algorithm at a high level as follows (slightly reformatted with additional commentary) <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<p><strong>Algorithm 1: Compressive Transformer</strong></p>
<pre><code>
Descriptions of the parameters:
-------------------------------

l:       number of layers, starting from 1
d:       size of the embedding AND of the hidden state vector passed from a layer to the next
ns:      size of a training segment = number of cells in a layer
nm:      number of hidden states kept in the primary memory. nm is a multiple of ns
c:       memory compression ratio
ncm:     number of hidden states kept in the compressed memory. 

Descriptions of the variables:
------------------------------

In the following, i indexes layers from 1 and l. Unless stated otherwise, the variables are also implicitly 
indexed with t being the training step (or the current segment number). Indexing by time does not 
necessarily imply that previous timesteps are retained in memory.

m(i):    matrix containing the primary memory of nm of nm vector states
cm(i):   matrix containing the compressed memory of nm vector states
h(i):    matrix of shape (ns, d) representing the input of layer i = ns vectors (input of each cell 
         in a layer i); the size of the matrix reflects the fact that the size of the input embedding and subsequent 
         hidden states is identical
mha(i):  contains the results of the multi-head attention.


Other:
------

fc(i):   compression function of layer i. Each layer has its own compression function fc(i). The 
         compressed state is not trained over all layers: this is not a multi-layer compressed state 
         generated from a multilayer old primary memory state. 



At time zero:
=============

// Initialize primary memory to zeros 
// ----------------------------------
// Zeros prevents the attention mechanism to train on spurious values.
// size: l × nm × d
1: m(0) &lt;- 0

// Initialize compressed memory to zeros 
// -------------------------------------
// size: l × ncm × d
2: cm(0) &lt;- 0


At time t:
==========

// Embed input sequence
// --------------------
// The input of the first layer is the current segment. The first layer here receives a list of word embeddings 
// defined by Wemb. This embedding would be pretrained, and, unlike the original Transformer, does not include 
// a positional embedding.
// size: ns × d
3: h(1) &lt;- x Wemb


// Main loop
// ---------
// Now that the first layer has an input, loop through the layers, one at a time
4: for layer i = 1, 2,..., l do

      // The content of the memories are concatenated
      // --------------------------------------------
      // size: (ncm + nm) × d
5:    mem(i) &lt;- concat( cm(i), m(i) )

      // Multi-Head Attention over to concatenated mem types 
      // ---------------------------------------------------
      // This is the starting point of Transformer training. It hides a lot of implementation 
      // complexity alluded to in the &quot;Transformers from Scratch&quot; blog post and shown in gory 
      // details in the Transformer-XL paper (appendix B) (see previous part 1). This line does not show the multiple 
      // triplets of matrices involved in the multihead attention details. 
      // See references in the first blog post of this series
      // size: ns × d
6:    mha(i) &lt;- multihead_attention(i)( h(i), mem(i) )

      // Regular skip + layernorm 
      // ------------------------
      // Recall that the transformers adopt the ResNet method of training for marginal 
      // difference compared to the previous layer. This gives quicker and more accurate training.
      // The layer normalisation brings the results to a range similar to the input vector (~unit vectors)
      // size: ncm × d
7:    a(i) &lt;- layer_norm(mha(i) + h(i))

      // Oldest memories to be forgotten 
      // -------------------------------
      //The memory to be discarded/compressed is the last ns vectors in the primary memory.
      // size: ns × d
8:    old_mem(i) &lt;- m(i)[ nm - ns : ns]

      // Compress oldest memories by factor c 
      // ------------------------------------
      // The primary memory of each layer is compressed into a compressed memory of that layer.
      // size: ns / c × d
9:    new_cm(i) &lt;- fc(i)(old mem(i))

      // Update memory 
      // -------------
      // The primary memory is updated to get ready for the next time step. It is now made of the new 
      // set of hidden vectors and the primary memory after dropping what was just compressed.
      // size: nm x d
10:   m(t+1)(i) &lt;- concat(h(i), m(i)[1 : nm - ns])

      // Update compressed memory 
      // ------------------------
      // Discards the oldest ns vectors and appends the newly created compressed memories.
      // Recall the compression schemes described in the previous post. Each primary memory hidden
      // state creates a compressed memory hidden state. This is why the cmt(i) is shifted by ns.
      // size: ncm × d
11:   cmt(i) &lt;- concat(new_cm, cmt(i)[1 : nm - ns])

      // Mixing Multi-Layer Perceptron 
      // -----------------------------
      // size: ns × d
12:   h(i+1) &lt;- layer norm(MLP(i) (a(i)) + a(i))       

</code></pre>
</div>
</div>
<div id="proposed-extensions-of-the-model" class="section level2">
<h2>Proposed extensions of the model</h2>
<p>Having gone through the details, a few suggestions can be suggested to extend this model (having in mind the application to time series)</p>
<ul>
<li><p>It makes sense to remove the constraint that the input vector from the dataset be of the same size as the hidden state vector. Market regimes are typically trained with few regimes, at least in comparison with the number of data feeds. Another way to look at this: embeddings are created to ensure discrimination between entries; compared to this, market data feeds are highly correlated. A hidden states would highly compress the market information.</p></li>
<li><p>In first approximation, markets show some level of (buzzword alert) ‘fractal’ behaviour. At different time scales, price curves / return distributions exhibit some similar properties. This suggests that using compressed memories reflecting those multiple time scales could be of value: multiple generations of compressed information.</p></li>
<li><p>Currently, the compressions functions are per-layer. It would be interesting to explore a multilayer compression mechanism. There is no particular intuition as to why this could be useful, Just curiosity…</p></li>
</ul>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Forgive the text format. R markdown resisted naive efforts to input Latex algorithms. But if it is good enough for Internet draft RFCs…<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/machine-learning/" rel="tag">Machine Learning</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/neural-network/" rel="tag">Neural Network</a></li>
	</ul>
</div>
		</footer>
	</article>
</main>



<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "https-emmanuel-r8-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2020 Back2Numbers.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>