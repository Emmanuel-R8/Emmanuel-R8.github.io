<!DOCTYPE html>
<html class="no-js" lang="en-au">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>RNN Compressive Memory Part 1: A high level introduction. - Back2Numbers</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	
	
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="shortcut icon" href="/favicon.ico">
		
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-150743827-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">
			<a class="logo__link" href="/" title="Back2Numbers" rel="home">
				<div class="logo__title">Back2Numbers</div>
				
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/whoami/">
				
				<span class="menu__text">#&gt;whoami</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/categories">
				<i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
				<span class="menu__text">Categories</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/tags">
				<i class="sidebar-button-icon fa fa-lg fa-tags"></i>
				<span class="menu__text">Tags</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/archives">
				<i class="sidebar-button-icon fa fa-lg fa-archive"></i>
				<span class="menu__text">Archives</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/about">
				<i class="sidebar-button-icon fa fa-lg fa-question"></i>
				<span class="menu__text">About this site</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/gitbooks/movielens/introduction.html">
				<i class="sidebar-button-icon fa fa-lg fa-question"></i>
				<span class="menu__text">Gitbook Movielens</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/gitbooks/lendingclub/introduction.html">
				<i class="sidebar-button-icon fa fa-lg fa-question"></i>
				<span class="menu__text">Gitbook LendingClub</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/">
				<i class="sidebar-button-icon fa fa-lg fa-home"></i>
				<span class="menu__text">Home</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">RNN Compressive Memory Part 1: A high level introduction.</h1>
			
		</header><div class="content post__content clearfix">
			


<p>This is the first post of series dedicated to Compressive Memory of Recurrent Neural Networks. This is inspired by a recent DeepMind paper published in November 2019 on <a href="https://arxiv.org/abs/1911.05507">Arxiv</a>.</p>
<p>Currently, the ambition of the series is to follow this plan:</p>
<ul>
<li><p>Part 1 (here): A high level introduction to Compressive Memory mechanics starting from basic RNNS;</p></li>
<li><p><a href="../10/rnn-compressive-memory-part-2.html">Part 2</a>: a detailed explanation of the algorithm and its interesting details;</p></li>
<li><p>Part 3: an implementation using PyTorch;</p></li>
<li><p>Part 4: finally, its application to time series.</p></li>
</ul>
<p>Most likely, this will be fine-tuned over time.</p>
<p><strong>Big thanks to <a href="https://gmarti.gitlab.io/">Gautier Marti</a> and <a href="http://zoonek.free.fr/blosxom/">Vincent Zoonekynd</a> for their suggestions and proof-reading!</strong></p>
<p><strong>Update:</strong> Additional diagrams (14 March 2020)</p>
<div id="recurrent-neural-networks-rnn" class="section level2">
<h2>Recurrent Neural Networks (<em>RNN</em>)</h2>
<div id="from-simple-rnns-to-lstms" class="section level3">
<h3>From simple RNNs to LSTMs</h3>
<p>Traditional neural networks were developed to train/run on information provided in a single step in a consistent format (e.g. images with identical resolution). Conceptually, a neural network could similarly be taught on sequential information (e.g. a video as a series of images) looking at it as a single sample, but that would require (1) being trained on the full sequence (e.g. an entire video), (2) being able to cope with information of variable length (i.e. short vs. long video). (1) is computationally intractable, and (2) means that units analysing later parts of the video would not be receiving as much training as earlier units when ideally they should be all share the same amount of training .</p>
<div class="figure">
<img src="/post/2020-03-RNN-compressive-memory/Recurrent_neural_network_unfold.svg" alt="" />
<p class="caption"><strong>Basic RNN</strong> (source: <em>Wikipedia</em>)</p>
</div>
<p>The original RNN address those issues:</p>
<ul>
<li><p>Sequences are chopped in small consistent sub-sequences (say, a <em>segment</em> of 10 images, or a group of 20 words).</p></li>
<li><p>An RNN layer is a group of blocks (or <em>cells</em>), each receiving a single element of the segment as input. Note that here <em>layer</em> does not have the traditional meaning of a layer of neural units fully connected to a previous layer of units. It is a layer of RNN cells. Within each cell, quite a few things happen, including using layers of neural units. From here on, a <em>layer</em> will refer to an <em>RNN layer</em> and not a layer of neural units..</p></li>
<li><p>Within a layer, cells are identical: they have the same parameters.</p></li>
</ul>
<p>Although each element of a sequence might be of interest on its own, it only becomes really meaningful in the context of the other elements. Each cell contains a state vector (called <em>hidden state</em>). Each cell is trained using an individual element from a segment and the hidden state from the preceding cell. Training the network means training the creation of those states. Passing of the hidden state transfers some context or memory from prior elements of the segment. The cells receiving a segment form a single layer. Each cell would typically (but not necessarily) also include an additional sub-cell to create an output as a function of the hidden step. In that case, the output of a layer can then be used as input of new RNN layer.</p>
<p>A layer is trained passing hidden states from prior cells to later cells. The hidden state from prior elements is used to contextualise a current element. To use context from later elements (e.g. in English, a noun giving context to a preceding adjective), a separate layer is trained where context instead passes from later to prior elements. Those forward and backward layers jointly create a <em>bidirectional RNN</em> .</p>
<p>Historically, RNNs applied to NLP deal with elements which are either one-hot encoded (either letters, or, more efficient, tokens), or word embeddings often normalised as unit vectors (for example see <a href="https://nlp.stanford.edu/projects/glove/">Word2Vec</a> and <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a>). RNN cells therefore deal with values between 0 and 1. Typically, non-linearity is brought by <span class="math inline">\(tanh\)</span> or <span class="math inline">\(sigmoid\)</span> activations which guarantee unit values within that range. Those activation functions quickly have very flat gradients. Segments often have 10s or 100s of elements. Because of vanishing gradients, a hidden state receives little information from distant cells (training gradients are hardly influenced by gradients of distant cells).</p>
</div>
<div id="longshort-term-memory-rnns" class="section level3">
<h3>Long/Short Term Memory RNNs</h3>
<div class="figure">
<img src="/post/2020-03-RNN-compressive-memory/Long_Short-Term_Memory.svg" alt="" />
<p class="caption"><strong>Basic LSTM RNN</strong> (source: <em>Wikipedia</em>)</p>
</div>
<p>Long/Short Term Memory RNNs (<em>LSTM</em>) address this by passing two states:</p>
<ul>
<li><p>a hidden state <span class="math inline">\(h\)</span> as described above trained with non-linearity: this is the <em>short-term memory</em>; and,</p></li>
<li><p>another hidden state <span class="math inline">\(c\)</span> (called <em>context</em>) weighting previous contexts with a simple exponential moving average (in <em>Gated Recurrent Units</em>) or a slightly more complicated version thereof in the original LSTM model structure. Determining the optimal exponential decay is part of the training process. This minimally processed state is the <em>long-term memory</em>.</p></li>
</ul>
<p>LTSM can also be made bidirectional.</p>
<p>Without going into further details, note that each <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\tanh\)</span> orange block represents matrix of parameters to be learned.</p>
</div>
<div id="attention" class="section level3">
<h3>Attention</h3>
<div class="figure">
<img src="/post/2020-03-RNN-compressive-memory/Attention_RNN.svg" alt="" />
<p class="caption"><strong>Attention RNN</strong></p>
</div>
<p>RNN were further extended with an <em>attention mechanism</em>. Blog posts on attention by <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Jay Alammar</a> and <a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">Lilian Weng</a> are good introductions.</p>
<p>A multi-layer RNN takes the output a layer and uses it as input for the next. With the attention mechanism, the outputs go through an attention unit.</p>
</div>
<div id="beyond-lstm-transformers" class="section level3">
<h3>Beyond LSTM: Transformers</h3>
<p>, and were then simplified (insert large air quotes) with <em>Transformers</em> (using what is called <em>self-attention</em>) that significantly reduce the number of model parameters and can be efficiently parallelised with minimum model performance impact. For an extremely clear introduction to those significant improvements, you cannot do better than reading , and by <a href="http://www.peterbloem.nl/blog/transformers">Peter Bloem</a> on transformers. The following assumes that you are broadly familiar with those ideas.</p>
<p>The basic transformer structure uses self-attention where, for a given element (the <em>query</em>), the transformer looks at the other elements of the segment (the <em>keys</em>) to determine how much ‘attention’ other elements of the segment influence the role of the query in changing the hidden state.</p>
<p>Broadly:</p>
<ul>
<li><p>The query is projected in some linear space (a matrix <span class="math inline">\(W_q\)</span>). That’s basically an embedding which is part of the model training.</p></li>
<li><p>All the other elements, the keys, are projected in another linear space (a matrix <span class="math inline">\(W_k\)</span>); another embedding which is part of the model training.</p></li>
<li><p>The similarity (perharps <em>affinity</em> would be a better word) between the projected query and each projected key is calculated with a dot product / cosine distance. This is exactly the approach of basic recommender systems with the difference that the recommendation is between sets of completely different nature (for example affinity between users and movies). Note that although query and keys are elements of identical type, they are embedded into different spaces with different projections matrices.</p></li>
<li><p>We now have a vector of the same size as the segment length (one cosine distance per input element). It goes through another layer (a matrix <span class="math inline">\(W_v\)</span>) to give a <em>value</em>.</p></li>
</ul>
<p>The triplet of <span class="math inline">\(\left( W_q, W_k, W_v \right)\)</span> is called an <em>attention head</em>. Actual models would include multiple heads (of the order of 10), and the output of a transformer layer could then feed into a new transformer layer.</p>
<p>This model is great until you notice that the dot product / cosine similarity is commutative and does not reflect whether a key element is located before or after the query element: order is fundamental to sequential information (“quick fly” vs. “fly quick”). To address this, the input elements are always enriched with a positional embedding: the input elements are concatenated with positional information showing where they stand within a segment.</p>
<p>Note that a transformer layer is trained on a segment using only the information from that segment. This is fine to train on sentences, but it cannot really account for more distant relationships between words within a lengthy paragraph, let alone a full text.</p>
</div>
<div id="transformer-xl" class="section level3">
<h3>Transformer-XL</h3>
<p>Transformers have been further improved with <a href="https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html">Tranformer-XL</a> (XL = extra long) which are trained using hidden states from previous segments, therefore using information from several segments, to improve a model’s memory span.</p>
<p>Conceptually, this is an obvious extension of the basic transformer to increase its memory span. But there is a fundamental problem. Going back to the basic transformer, each element includes its absolute position within the segment. The position of the first word of the segment is 1, that of the last one is, say, 250 . Such a scheme breaks down as soon as the state of the previous segment is taken into account. Word 1 of the current segment obviously comes before word 250, but has to come after word 250 of the previous segment. The absolute position encoding does not reflect the relative position of elements located in different segments.</p>
<p>The key contribution of the Transformer-XL is to develop a relative positional encoding that allows hidden state information to cross segment boundaries. In their implementation, the authors evaluate that the attention length, being basically how many hidden states are used, is 450% longer that the basic transformer. That’s going from sentence length to full paragraph, but still far from a complete book.</p>
<p>A side, but impressive, benefit is that the evaluation speed of the model, or it use once trained, is significantly increased thanks to the relative addressing (the paper states up to a 1,800-fold increase depending on the attention length).</p>
</div>
</div>
<div id="compressive-transformers" class="section level2">
<h2>Compressive Transformers</h2>
<p>Full text understanding cannot be achieved by simply lengthening segment sizes from 100s to the <a href="https://blog.reedsy.com/how-many-words-in-a-novel/">word count</a> of a typical novel (about 100,000). When training a model routinely takes 10s of hours on GPU clusters, an increase by 3 orders of magnitude is not realistic.</p>
<p>In a recent <a href="https://arxiv.org/abs/1911.05507">paper</a>, DeepMind proposes a new RNN model called <em>Compressive Transformers</em>.</p>
<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>Transformer-XL uses the hidden state of a prior segment (<span class="math inline">\(h_{T-1}\)</span>) to improve the training of the current segment (<span class="math inline">\(h_{T}\)</span>). When moving to the next segment, training (<span class="math inline">\(h_{T+1}\)</span>) now only uses <span class="math inline">\(h_{T}\)</span> and <span class="math inline">\(h_{T-1}\)</span> is discarded. To increase the memory span, one could train using more past segments at the expense of increase in memory usage and computation time (quadratic). The actual Transformer-XL uses the hidden states of several previous segments, but the discarding mechanism will remain.</p>
<p>The key contribution of the Compressive Transformers is the ability to retain salient information from those otherwise discarded past states. Instead of being discarded, they are stored in compressed form.</p>
<p>Each Transformer-XL layer is now trained with prior hidden states (<em>primary memory</em>) and the <em>compressed memory</em> of older hidden states.</p>
<p>As an aside, although not explicitly mentioned, we should note that the ‘-XL’ aspect of the Transformer-XL and the memory compression mechanics are conceptually independent from the actual types of RNN cell. Simple RNNs, GRUs or LSTMs could be trained using the hidden states of past segments (not dissimilar to state/context peeking into past cells in certain RNN variants). But the performance benefit of Transformer-XL is such that the paper only focuses on transformer-XL.</p>
</div>
<div id="compression-scheme" class="section level3">
<h3>Compression scheme</h3>
<p>As compared to Transformer-XL, the key difference is the compression scheme. The rest of the model seems identical.</p>
<div id="size-parameters" class="section level4">
<h4>Size parameters</h4>
<p>The size of the model is described with a few size parameters:</p>
<ul>
<li><p><span class="math inline">\(n_s\)</span>: size of a segment = the number of cells in a layer.</p></li>
<li><p><span class="math inline">\(n_m\)</span>: number of hidden states in the primary uncompressed memory (like the Transformer-XL). <span class="math inline">\(n_m\)</span> is a multiple of <span class="math inline">\(n_s\)</span>. The primary memory is a FIFO buffer: the first (oldest) memories will be the first to be later compressed.</p></li>
<li><p><span class="math inline">\(n_{cm}\)</span>: number of compressed hidden states in the compressed memory. States in the compressed memory will compress an old segment of size <span class="math inline">\(n_s\)</span> dropping out of the primary memory. <span class="math inline">\(c\)</span> is an information compression ratio from <span class="math inline">\(n_s\)</span> primary memory entries into compressed memory entries. There can be two ways of applying this compression ratio, which both reduce the number of hidden states by the same ratio:</p>
<ul>
<li><p><span class="math inline">\(c\)</span> uncompressed layers could create a single compressed hidden state of identical size. This merges the information of a group of elements (e.g. <span class="math inline">\(c\)</span> words) into a single hidden state. In this case, <span class="math inline">\(n_s\)</span> is proportional to <span class="math inline">\(c\)</span> and <span class="math inline">\(n_{cm}\)</span> is proportional to <span class="math inline">\(n_s / c\)</span>. The authors do not use this approach. It would enforce a sub-segmentation of an uncompressed segment at arbitrary intervals (why group 3 words instead of 5 or 7…)</p></li>
<li><p>Instead, the authors use dimension reduction: a single uncompressed hidden state is compressed into a new hidden state with <span class="math inline">\(c\)</span> times fewer hidden states. If the size of the hidden state of a Transformer-XL cell is <span class="math inline">\(n_h\)</span>, hidden states in the primary memory will have the same size, and the compressed memory hidden states will have a size of <span class="math inline">\(n_h / c\)</span>.</p></li>
</ul></li>
</ul>
<p>By way of example, a segment could have 100 cells (<span class="math inline">\(n_s = 100\)</span>). This segment could be trained with the hidden states of the past 3 segments’ training (<span class="math inline">\(n_m = 3 * n_s = 300\)</span>). When training the next segment, an old segment of size 100 becomes available for compression which will create 100 new hidden states.</p>
<p>This example is for a single layer. The same scheme would be replicated for each layer of the model</p>
<p>Note that the paper only contemplates a single set of compressed memories. There could also be multiple generations of compressed memories, primary memory compresses in generation 1, then compressing into generation 2…</p>
</div>
<div id="compression-functions" class="section level4">
<h4>Compression functions</h4>
<p>A compressed hidden state is created from <span class="math inline">\(c\)</span> primary memory hidden states. When training on texts with word embeddings,the authors used a value of <span class="math inline">\(c=3\)</span> or <span class="math inline">\(c=4\)</span>.</p>
<p>Several compression schemes are explored in the paper:</p>
<ul>
<li><p>max or mean pooling with a stride of <span class="math inline">\(c\)</span>. This is typical of image convolution networks - no explanation required.</p></li>
<li><p>1-dimensional convolution with a stride of <span class="math inline">\(c\)</span>. This is also typical of image convolution network apart from being one-dimensional. This requires parameter training.</p></li>
<li><p><a href="https://arxiv.org/pdf/1511.07122.pdf">dilated convolution</a>. In practice image convolutions have shown to be inadequate for sequential information where dependencies can be at both short and long ranges: working at different scales makes sense. Dilated convolutions use convolution filters that are contracted and dilated versions of a template to be trained.</p></li>
<li><p>a <em>most-used</em> mechanism that identifies and retains part of the hidden states according to their importance in the cells training gauged by the attention they received.</p></li>
</ul>
</div>
</div>
<div id="compression-training" class="section level3">
<h3>Compression training</h3>
<p>Training the compression parameters is done separately from the optimisation of the Transformer-XL cells.</p>
<p>The purpose of the compressed memory is to provide a compressed and lossy representation of the primary memory (hidden states) or the attention heads parameters: the quality of the compression mechanics is assessed by how well the original information can be re-generated from it. In essence, the compressed hidden states are a compressed representations to a learned representation vector in an auto-encoder. This is the training mechanics used by the authors.</p>
<p>As in an auto-encoder, the representation is learned by comparing the original information to its reconstruction. This training is kept completely independent from the training of the transformers: the auto-encoding loss and gradients do not impact the attention heads’ parameters.</p>
<p>Conversely, the loss and gradients of the attention heads’ training do not flow into the training of the compression scheme.</p>
</div>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>This was a high level introduction of RNNs all the way up to Compressive Memory mechanics. Next, the algorithm’s nitty-gritty.</p>
</div>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/machine-learning/" rel="tag">Machine Learning</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/neural-network/" rel="tag">Neural Network</a></li>
	</ul>
</div>
		</footer>
	</article>
</main>



<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "https-emmanuel-r8-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2020 Back2Numbers.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>