<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Back2Numbers</title>
    <link>https://emmanuel-r8.github.io/tag/r/</link>
      <atom:link href="https://emmanuel-r8.github.io/tag/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Emmanuel Rialland 2020</copyright><lastBuildDate>Thu, 12 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://emmanuel-r8.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>R</title>
      <link>https://emmanuel-r8.github.io/tag/r/</link>
    </image>
    
    <item>
      <title>HarvardX Gitbooks available</title>
      <link>https://emmanuel-r8.github.io/post/2019/12/12/harvardx-gitbooks-available/</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://emmanuel-r8.github.io/post/2019/12/12/harvardx-gitbooks-available/</guid>
      <description>
&lt;script src=&#34;../../rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Both capstones for the HarvardX certificates are now available. Just click on the &lt;code&gt;Projects&lt;/code&gt; link!&lt;/p&gt;
&lt;p&gt;If Gitbooks are not your thing, at the top of their main page, there is a download link to a pdf version.&lt;/p&gt;
&lt;p&gt;They make for a good knock-me-asleep reading…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HarvardX Final Report - LendingClub dataset</title>
      <link>https://emmanuel-r8.github.io/post/2019/12/11/harvardx-final-report-lendingclub-dataset/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://emmanuel-r8.github.io/post/2019/12/11/harvardx-final-report-lendingclub-dataset/</guid>
      <description>
&lt;script src=&#34;../../rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;After 3 months of work, the final report for the HarvardX Data Science course was submitted.&lt;/p&gt;
&lt;p&gt;It is based on the LendingClub dataset. LendingClub is a peer-2-peer lender. This is a matching of private borrowers and investors. Small amounts, fairly high risk (if they could, borrowers would probably have had a bank involved). Surprisingly, after tapping a market of individual lenders, the biggest lenders are now the banks. To inform the investors, LendingClub make historical information publicly available.&lt;/p&gt;
&lt;p&gt;This work went through many blind alleys. I won’t list them, they are in the report (post-mortem section). But it was an overall enriching experience. I learned a lot, often about limitations of what I tried (the dataset is big with a few millions samples (big for an old laptop), with many (ca. 150) mispecified mixed categorical and numeric variables). The experience will be filed in the &lt;em&gt;‘it-builds-character’&lt;/em&gt; category…&lt;/p&gt;
&lt;p&gt;One point that is still tingling my mind is learning about Conditional Inference Trees used to bin variables. That is then used for logistic regression to predict probabilities of loan default.&lt;/p&gt;
&lt;p&gt;Why are those trees interesting? They are sourced in information theory and measure the information content of a prediction variable to predict a binary response. The prediction variable is then partitioned in a few intervals (bins). What is great?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The measurement does NOT rely on the value of the prediction variable. This means that variable NAs go from being a nuisance to being stashed in a bin of their own treated as any other bin.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The logistic regression context predicates binary variables which were perfect for the purpose of this report. But those trees do not require binary outcomes. They rely on what are called &lt;em&gt;Weight of Evidence&lt;/em&gt; (calculated for each bin) and &lt;em&gt;Information Value&lt;/em&gt; (calculated for each variable).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The calculations are very quick (about 1/10th second to bin 1 million samples) with a small memory footprint.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, whatever comes in, we do not have to worry about scaling/z-scoring/filling NAs; it is quickly reformatted into a handful (literally of that order of magnitude depending on parameters used) based on the relevance to predicting what needs to come out.&lt;/p&gt;
&lt;p&gt;If I didn’t know better, this should be called &lt;em&gt;model impedance matching&lt;/em&gt; (electrical engineers can explain)!&lt;/p&gt;
&lt;p&gt;Apart from that, the number of avenues to explore with this dataset (especially using data from other sources) could fill many more months. I listed a list of possible techniques in the report’s conclusion. This is what does and will keep banks’ credit risk departments busy and well-staffed…&lt;/p&gt;
&lt;p&gt;I am working on making the MovieLens and LendingClub reports available as gitbooks. To be announced.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HarvardX Data Science course - First final project</title>
      <link>https://emmanuel-r8.github.io/post/2019/10/05/harvardx-data-science-course-first-final-project/</link>
      <pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://emmanuel-r8.github.io/post/2019/10/05/harvardx-data-science-course-first-final-project/</guid>
      <description>
&lt;script src=&#34;../../rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I recently finished to penultimate &lt;a href=&#34;https://github.com/Emmanuel-R8/capstone-movielens/blob/master/MovieLens.pdf&#34;&gt;final assignment&lt;/a&gt; for the &lt;a href=&#34;https://www.edx.org/professional-certificate/harvardx-data-science&#34;&gt;HarvardX Data Science course&lt;/a&gt;. The Stanford course was clearly machine learning. This one is definitely lighter on the machine learning and much heavier on the data science: how to source, clean and visualise data are key skills. The targeted knowledge is more traditional probabilities/statistics. Long-existing fundamental techniques like inference, polling are there.&lt;/p&gt;
&lt;p&gt;This time R is the centre tool of the course. It makes clear sense. When I started learning it about 15 years ago, I loathed the multiple gotchas. Since then, new libraries have simplified base R and removed its exceptions and exceptions to exceptions. In addition the &lt;code&gt;Rcpp&lt;/code&gt; library has eased implementation of efficient algorithms and interfacing with popular libraries. Still not a speed demon, but not the snail it used to be.&lt;/p&gt;
&lt;p&gt;I won’t go through the project and my models. No revolutionary concepts. Just great results. I took half a day to reimplement in Julia, both to crosscheck and personal training. As expected, a lot easier to read. But the big surprise was the speed difference. Although I didn’t time it, Julia only felt about twice quicker. Credit to the R project folks (I only used matrices operations, no modeling libraries).&lt;/p&gt;
&lt;p&gt;On this report, I got grades that can’t be improved upon. Happy camper.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
