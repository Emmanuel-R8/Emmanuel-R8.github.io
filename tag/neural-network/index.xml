<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neural Network | Back2Numbers</title>
    <link>/tag/neural-network.html</link>
      <atom:link href="/tag/neural-network/index.xml" rel="self" type="application/rss+xml" />
    <description>Neural Network</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Emmanuel Rialland 2020</copyright><lastBuildDate>Thu, 31 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Neural Network</title>
      <link>/tag/neural-network.html</link>
    </image>
    
    <item>
      <title>RNN Compressive Memory Part 3: The Compression algorithm</title>
      <link>/2020/12/31/rnn-compressive-memory-part-3-the-compression-algorithm.html</link>
      <pubDate>Thu, 31 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/2020/12/31/rnn-compressive-memory-part-3-the-compression-algorithm.html</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-transformerxl&#34;&gt;The TransformerXL&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compressive-memory&#34;&gt;Compressive Memory&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#background-1&#34;&gt;Background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-overall-structure&#34;&gt;Model overall structure&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#memory-compression&#34;&gt;Memory compression&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#compression-function&#34;&gt;Compression function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loss-function&#34;&gt;Loss function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-timing&#34;&gt;Model timing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#attention-reconstruction-loss&#34;&gt;Attention-Reconstruction Loss&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#notation-summary&#34;&gt;Notation summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;This is part 3 of a discussion of the DeepMind paper about &lt;a href=&#34;https://arxiv.org/abs/1911.05507&#34;&gt;Compressive Transformers for Long-Range Sequence Modelling&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;../07/rnn-compressive-memory-part-1.html&#34;&gt;Part 1&lt;/a&gt;: A high level introduction to Compressive Memory mechanics starting from basic RNNS;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Part 2 (here): a detailed explanation of the TransformerXL;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Part 3: an implementation using PyTorch;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Part 4: finally, its application to time series.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This post describes the building blocks up to what makes a &lt;a href=&#34;https://arxiv.org/abs/1901.02860&#34;&gt;TransformerXL&lt;/a&gt; model as presented in June 2019. The compression mechanics are an add-on to the TransformerXL which therefore need to be detailed. Along the way, it will become clear that, in many ways, the implementation described only applies to embeddings, i.e. one-hot encodings, and not to to truly multi-dimensional variable inputs.&lt;/p&gt;
&lt;div id=&#34;the-transformerxl&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The TransformerXL&lt;/h2&gt;
&lt;div id=&#34;background&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;The previous part introduced a chronological introduction starting with RNNs. However, it would have been more accurate to present those models as Sequential Neural Network: They are concerned with dealing with sequential pieces of information (individual elements) and capturing their respective interactions. &lt;em&gt;Recurrent&lt;/em&gt; networks are but one type of sequential networks.&lt;/p&gt;
&lt;p&gt;The first generation Recurrent Neural Networks passes contextual information from one sequential unit to the next within a layer. A unit does not see anything past its preceding neighbour. Bidirectional networks do not feed a unit with information coming from both sides within a single layer, they are a combination of two layers going in opposite directions. RNNs do not capture information as a whole.&lt;/p&gt;
&lt;p&gt;The addition of &lt;em&gt;attention&lt;/em&gt; is the first mechanism that captured the entire sequence. First, within an RNN layer, each unit receives an input from the preceding layer (or a training segment). Then, an attention layer (an &lt;em&gt;attention head&lt;/em&gt;) is fed with each and every output produced by the RNN layer. Each hidden state is weighted by trained parameters and the attention layer produces
Several attention heads can be trained in parallel and combine their results using trained parameters into a final output.&lt;/p&gt;
&lt;p&gt;The number of parameters for this type of model is extremely large.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;compressive-memory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compressive Memory&lt;/h2&gt;
&lt;div id=&#34;background-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;From the last post, recall that compressive transformer are an extension to the TransformerXL. TransformerXL trains using segments extracted from all the sequences available in the dataset. It also uses hidden states calculated when training prior segments. In a sense, it seeks to indirectly increase the size of the training segments. Naturally, the model stores past hidden states (past memories) in a buffer of limited size. It has to discard discarding the oldest memories from that primary memory when moving from a segment to the next.&lt;/p&gt;
&lt;p&gt;The key addition of the compressive model is to avoid immediately the discarding by compressing those past states in a compressed representation (which itself will eventually be discarded). The training needs to cover the original TransformerXL and the quality of the compression mechanics. When training a typical network, the values of all the parameters are simultaneously optimised using the gradients calculated for each of those parameters. Instead, this model is trained as 2 separate models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The Transformer-XL layers are trained using a current segment, and the primary and compressed memories to train the cells’ parameters (and only those!). During that stage, the compressed memory parameters is given as a constant: no gradients are calculated. Conceptually, this is the normal TransformerXL where the inputs are the concatenation of the segment embeddings and all the memories (primary and compressed).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Separately&lt;/em&gt;, another training process looks for an optimal compressed representation of past memories that would otherwise be discarded. Training the compression mechanism assumes that the transformer cells’ parameters are fixed and constant. This stage is only concerned with the efficiency of the representation. Note that certain compression mechanics, like pooling, have no parameters to optimise (although some hyperparameters might be involved).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-overall-structure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model overall structure&lt;/h3&gt;
&lt;p&gt;The authors describe the overall algorithm at a high level. We will retain the same notations, which are summarised at the end of of the post, but we will use a slightly different vocabulary. In particular, a &lt;em&gt;training set&lt;/em&gt; (for example a full text) is split into ordered &lt;em&gt;sequences&lt;/em&gt; that for a conceptual whole (a full sentence). Each sequence is fed into the model which has a limited number of inputs. If the size of a sequence is too large, it is split into &lt;em&gt;segments&lt;/em&gt;. The size of a segment is the number of input of the model. The key attraction of the compressive memory is to lengthen the memory of a TransformerXL: for an identical computation budget, the paper shows that it is valuable to shorten the segments (fewer self-attention heads), in exchange for keeping longer compressed past memories (the self-attention heads become larger to give attention to compressed memories).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2020-03-RNN-compressive-memory/Compressive-template.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;strong&gt;Compressive TransformerXL&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To complete the notation, we use the following form of indices: &lt;span class=&#34;math inline&#34;&gt;\(h^{(l), v}_t\)&lt;/span&gt; which should be read as &lt;span class=&#34;math inline&#34;&gt;\(h^{(\text{Layer index}), \text{Segment index}}_{\text{Time step}}\)&lt;/span&gt;. In addition, &lt;span class=&#34;math inline&#34;&gt;\(h^{(1)}\)&lt;/span&gt; represents the inputs into the first layer, i.e. equals the current training segment &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;The model starts with transformer layers (&lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; layers). (see step (a) on the diagram).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each layer takes a number of entries (a segment of &lt;span class=&#34;math inline&#34;&gt;\(n_s\)&lt;/span&gt; inputs), each being a vector of dimension &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the dimension of the initial input embeddings, and of all input/output vectors of further layers. This could however be a flexible model parameter.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each individual transformer layer has an identical structure with the same number of self-attention heads (&lt;span class=&#34;math inline&#34;&gt;\(n_{head}\)&lt;/span&gt; heads). Recall that an attention head is a triplet of matrices &lt;span class=&#34;math inline&#34;&gt;\((W^{(i)}_q, W^{(i)}_k, W^{(i)}_v)\)&lt;/span&gt; and a multi-layer perceptron. &lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;At each iteration, the self-attention heads are trained using the outputs from the previous layer, and primary and compressed memories.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;After an input is processed by a layer, it is pushed into memory. The memory size &lt;span class=&#34;math inline&#34;&gt;\(n_m\)&lt;/span&gt; has the capacity to store a number &lt;span class=&#34;math inline&#34;&gt;\(n_{slots}\)&lt;/span&gt; of segments. Therefore &lt;span class=&#34;math inline&#34;&gt;\(n_m = n_{slots} \times n_s\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The new memory pushes the oldest one out, which is compressed into a compressed memory with a &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; compression ratio. the compressed memory can hold &lt;span class=&#34;math inline&#34;&gt;\(n_{old}\)&lt;/span&gt; compressed memories. &lt;span class=&#34;math inline&#34;&gt;\(n_{cm}\)&lt;/span&gt; is a multiple of &lt;span class=&#34;math inline&#34;&gt;\(n_{old}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;memory-compression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Memory compression&lt;/h2&gt;
&lt;p&gt;Let’s focus on Step (f). The compression mechanics are optimised independently from the TransformerXL layers and require choosing a compression scheme (function &lt;span class=&#34;math inline&#34;&gt;\(f_c\)&lt;/span&gt;) and, if there are parameters to be trained, a loss function to minimise.&lt;/p&gt;
&lt;div id=&#34;compression-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compression function&lt;/h3&gt;
&lt;p&gt;The authors reviewed several choices for the function &lt;span class=&#34;math inline&#34;&gt;\(f^{(i)}_c\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;pooling&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Pooling&lt;/h4&gt;
&lt;p&gt;The simplest is &lt;em&gt;Max and/or mean pooling&lt;/em&gt;, where the kernel and stride is set to the compression rate c. This is fast, requires no training (no parameters) and was used as a simple baseline.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;convolution&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Convolution&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;1D-convolution&lt;/em&gt; also with kernel and stride set to c. This is defined by parameters which require training.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dilated-convolution&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Dilated convolution&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Dilated convolutions&lt;/em&gt; which were proposed in 2016 as an alternative to traditional image convolution (for image classification) when dealing with semantic networks. See &lt;a href=&#34;https://arxiv.org/abs/1511.07122&#34;&gt;Multi-Scale Context Aggregation by Dilated Convolutions&lt;/a&gt;. In essence, dilated convolutions are large scale convolutions (large kernel) defined with the same number of parameters as a small kernel. They are better suited to capture distant relationships present in semantic networks.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-used-compression&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Most-used compression&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Most-used&lt;/em&gt; compression where the memories are sorted by their average attention(usage) and the most-used are preserved. The most-used compression scheme is inspired from the garbage collection mechanism in the Differentiable Neural Computer (&lt;em&gt;DNC&lt;/em&gt;) where low-usage memories are erased. This is described in the 2016 DeepMind paper &lt;a href=&#34;https://www.gwern.net/docs/rl/2016-graves.pdf&#34;&gt;Hybrid computing using a neural network with dynamic external memory&lt;/a&gt;. A DNC is network augmented with memory, where a controller can read and write memory banks. The DNC article is very detailed on the technical aspects. We understand that most-used algorithm is presented in appendix under the &lt;em&gt;Dynamic memory allocation&lt;/em&gt; subsection.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;auto-encoders&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Auto-encoders&lt;/h4&gt;
&lt;p&gt;As we understand, the authors did not consider auto-encoding networks with &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; fewer parameters. It might be that they were considered but rejected.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compression-ratio&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Compression ratio&lt;/h4&gt;
&lt;p&gt;If the compression ratio is &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, what are the dimensions of the compressed memory tensors?&lt;/p&gt;
&lt;p&gt;The paper leaves this unspecified.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pooling and 1D-convolution will produce tensors where the dimension of the attention heads (along the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; axis) are divided by &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Our understanding of the most-used algorithm would be to zero all values save for a &lt;span class=&#34;math inline&#34;&gt;\(1 / c\)&lt;/span&gt; proportion, but retaining identical dimensions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, we do not see &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; memory attention heads compressed into a single vector (compression along the &lt;span class=&#34;math inline&#34;&gt;\(n_s\)&lt;/span&gt; axis).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;loss-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loss function&lt;/h3&gt;
&lt;p&gt;Several loss functions were considered.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;A whole-model optimisation where all parameters (compression mechanics + TransformerXL Layers) are trained at once using the training set error. However they found the size overwhelming, especially when old compressed memories were unrolled into full-form memories. They then used &lt;em&gt;backpropagating-through-time&lt;/em&gt; (BPTT), which by definition does not do full-model optimisation to avoid vanishing/exploding gradients. See &lt;a href=&#34;https://d2l.ai/chapter_recurrent-neural-networks/bptt.html&#34;&gt;BPTT&lt;/a&gt; for a good overview. Note that this is not what we introduced as a two-step optimisation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The compression is a form of representation learning / auto-encoding which optimises an encoder (presumably &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; fewer parameters) by simultaneously training a decoder. The paper tried a &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}_2\)&lt;/span&gt; (square) error on the ability to reconstruct the memory tensors. It does not go into detail aside from mentioning it. The posts will not explore this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The paper goes into more detail about the two-step training of an &lt;em&gt;attention-reconstruction loss&lt;/em&gt;. Transformers optimise attention parameters (the attention head matrices). Optimising for the ability of retaining that information makes complete sense. This loss function is the &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}_2\)&lt;/span&gt; error between the attention matrix of a memory slot and reconstructed attention. Next section focuses on the loss function.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;model-timing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model timing&lt;/h3&gt;
&lt;p&gt;Let’s be clearer about the details of what takes place as time goes by (as segments are presented) and how the compression optimisation proceeds (at the risk of being repetitive).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The set of the model’s state vectors (the &lt;span class=&#34;math inline&#34;&gt;\(h^{(i)}\)&lt;/span&gt;) is the model’s reaction to a particular input. They are an immediate reaction to an current input. They reflect the values of the current attention heads’ parameters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A memory slot is a set of a layer’s state vectors created in the past for a past input when the model’s parameters were different.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; It is the past model’s reaction to something that happened in the past (the input at that time).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;That memory slot reflects the experience of the model at that time (the attention heads parameters). Since then, the model has been trained further. The recent updated model would have yielded a different set of state vectors for that past input.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Attention heads are not state vectors. They are functions to be trained.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; Those functions, for any input state vectors, will produce a different answer, a different set of state vectors.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Attention heads are trained and after training they will eventually be constants. However, the the state vectors will change as and when the inputs do.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The compression is also a function that will be frozen after training which will use (if it relies on parameters).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Those precisions are relevant because a compression takes place now (i.e. at the time of a current segment). It is applied to a past memory which reflects a past reaction to a past input with a past model. We optimise compression parameters &lt;em&gt;now&lt;/em&gt; using values and parameters that are all from the past.&lt;/p&gt;
&lt;p&gt;Optimising for attention reconstruction means:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Right now, the model gives a particular set of current state vectors for a given current input.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This model has a past memory that is about to be lost (as in a normal TransformerXL).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In this model, the old memory is about to be compressed. If compressed, the original memory could be reconstructed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the compression (and reconstruction) algorithm is perfect, it should not matter whether the compressed memory (after reconstruction) or the old memory is used.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The optimisation is therefore to minimise the difference what the model does with the past memory (not yet discarded) and what would happen if the past memory was reconstructed from its compressed form.&lt;/p&gt;
&lt;p&gt;The difference between the 2 situations is the loss function to be minimised. Transformers are about attention; the compression algorithm is judged by its ability to retain past attention (what was worthy of attention in the past).&lt;/p&gt;
&lt;p&gt;As the paper indicates, this is a lossy objective. The reconstruction is not optimised to replicate the old memory. It is instead optimised to reconstruct an &lt;em&gt;alternative memory&lt;/em&gt; that gives a better reconstruction of the attentions. The paper asserts that this gives superior results to the reconstruction of the memory. Keeping in mind that the ultimate purpose of the model is to optimise self-attentions, possible explanations are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Memories are used as input into the attention heads to determine the self-attention. So will a reconstructed memory. Therefore, primary memory reconstruction errors are then compounded with the modelling errors of the attention heads’ matrices. Skipping the intermediary avoids this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Maybe more interesting, the past memory is mechanically imperfect: it is based on old model parameters. Optimising the reconstruction to reproduce an imperfect memory implicitly means incorporating attention heads’ parameters that were used at the time the memory was created. The reconstructed memory includes the errors of the past attention heads which have since been reduced (hopefully). In other words, the reconstructed memory is an &lt;em&gt;alternative memory&lt;/em&gt; which might include later corrections to the model attention heads.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;attention-reconstruction-loss&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Attention-Reconstruction Loss&lt;/h3&gt;
&lt;p&gt;For sake of completeness, we include the &lt;em&gt;attention-reconstruction loss&lt;/em&gt; algorithm detailed in the paper (&lt;em&gt;Algorithm 2&lt;/em&gt; on page 4). &lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Its overall organisation is independent from the actual loss calculation (the stop-gradient mechanics), therefore remains of value. It is replicated here with slight modifications for type consistency. Note that at Step 6 the published algorithm uses a &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. This could suggest that a sigmoid activation is used, but it would make no sense to compare attentions calculated with different activation functions (&lt;span class=&#34;math inline&#34;&gt;\(softmax\)&lt;/span&gt; in the layers vs. &lt;span class=&#34;math inline&#34;&gt;\(sigmoid\)&lt;/span&gt; in the the reconstruction).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{ll}
&amp;amp; \textit{Reset the attention loss} \\
&amp;amp; \hline \\
1: &amp;amp; \mathcal{L} \leftarrow 0 \\
\\
&amp;amp; \textit{Main Loop} \\
&amp;amp; \hline \\
&amp;amp; \text{Now that the loss is reset, loop through the layers, one at a time} \\
2: &amp;amp; \textit{for layer }i = 1, 2, \cdots , l \textit{ do} \\
\\
&amp;amp; \textit{Stop all gradients for anything but the compression function parameters} \\
&amp;amp; \hline \\
&amp;amp; \text{TensorFlow uses stop_gradient()} \\
&amp;amp; \text{PyTorch uses .detach()} \\
&amp;amp; \text{We&amp;#39;ll use DetachGradient() to make both happy!} \\
3:  &amp;amp;  DetachGradient(h^{(i)}) \\
4:  &amp;amp;  DetachGradient(M^{(i)}_{t-n_{slots}}) \text{ --- oldest Primary memory slot about to be discarded}  \\
5:  &amp;amp;  DetachGradient(Q, K, V) \text{ --- parameters of all the attention heads matrices}  \\
\\
&amp;amp; \textit{Define the content-based attention being the same calculation as inside the layers}\\
&amp;amp; \hline \\
6: &amp;amp;   Attention(h, m) \leftarrow softmax((hQ).(mK))(mV)  \\
\\
&amp;amp; \textit{Calculate the new compressed memory}\\
&amp;amp; \hline \\
7: &amp;amp;   CM^{(i)}_{1} \leftarrow f^{(i)}_c(M^{(i)}_{t-n_{slots}})   \\
\\
&amp;amp; \textit{Calculate a reconstructed attention from the proposed compressed memory}\\
&amp;amp; \hline \\
7: &amp;amp;   R^{(i)} \leftarrow Reconstruction(CM^{(i)}_{1})  \\
\\
&amp;amp; \textit{Update the loss for the loss at the current layer}\\
&amp;amp; \hline \\
8: &amp;amp;   \mathcal{L} \leftarrow \mathcal{L} + \left| Attention(h^{(i)}, M^{(i)}_{t-n_{slots}}) - Attention(h^{(i)}, R^{(i)}) \right|_2 \\
\\
&amp;amp; \hline \\
&amp;amp; \textit{End loop} \\
&amp;amp; \hline \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This post presents the &lt;em&gt;Compressive Transformers&lt;/em&gt; in more details, including the articulation between training the attention heads and training the compression mechanics.&lt;/p&gt;
&lt;p&gt;To come in part 3, the code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notation-summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notation summary&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Size parameters&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{ll}
l:        &amp;amp;   \text{number of layers, starting from 1} \\
d:        &amp;amp;   \text{dimension of all input (including embeddings) AND output vectors passed from a layer to the next} \\
n_{head}: &amp;amp;   \text{number of self-attention heads in a TransformerXL layer} \\
n_s:      &amp;amp;   \text{size of a training segment = number of inputs into and outputs from a layer} \\
s = n_s:  &amp;amp;   s \text{ will be used as index name to represent the input dimension when using named tensors,}  \\
&amp;amp; \text{i.e. Einstein summation.}  \\
b:        &amp;amp;   \text{size of a training batch, i.e. number of segments in a batch. Used in named tensors as well} \\
\\
n_m:      &amp;amp;   \text{number of hidden states kept in the primary memory. n_m is a multiple of n_s} \\
c:        &amp;amp;   \text{memory compression ratio} \\
n_{cm}:   &amp;amp;   \text{number of states kept in the compressed memory. n_{cm} }  \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;We only consider a wide self-attention model, in part having in mind the application to time series where we will work with embeddings of smaller size. We therefore ignore the narrow self-attention model variant.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;We ignore the possible impact of training in batches.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;We ignore the multi-perceptrons.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Forgive the algorithm formatting. R markdown resisted naive efforts to use Latex algorithms.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>RNN Compressive Memory Part 2: The TransformerXL</title>
      <link>/2020/12/30/rnn-compressive-memory-part-2-the-transformerxl.html</link>
      <pubDate>Wed, 30 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/2020/12/30/rnn-compressive-memory-part-2-the-transformerxl.html</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-transformerxl&#34;&gt;The TransformerXL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#algorithm-for-a-single-layer-with-matrix-dimension&#34;&gt;Algorithm for a single layer with matrix dimension&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#notation-summary&#34;&gt;Notation summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#useful-references&#34;&gt;Useful references&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;This is part 2 of a discussion of the DeepMind paper about &lt;a href=&#34;https://arxiv.org/abs/1911.05507&#34;&gt;Compressive Transformers for Long-Range Sequence Modelling&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;../../03/07/rnn-compressive-memory-part-1.html&#34;&gt;Part 1&lt;/a&gt;: A high level introduction to Compressive Memory mechanics starting from basic RNNS;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Part 2 (here): more details about the TransformerXL;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Part 3: an implementation using PyTorch (soon);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Part 4: finally, its application to time series (soon).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This post describes the building blocks up to what makes a &lt;a href=&#34;https://arxiv.org/abs/1901.02860&#34;&gt;TransformerXL&lt;/a&gt; model as presented in June 2019. The compression mechanics are an add-on to the TransformerXL which therefore need to be detailed. Along the way, it will become clear that, in many ways, the implementation described only applies to embeddings, i.e. one-hot encodings, and not to to truly multi-dimensional variable inputs.&lt;/p&gt;
&lt;div id=&#34;the-transformerxl&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The TransformerXL&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
\text{Notation:} &amp;amp; \\
\tau &amp;amp; \text{time step or index on successive segments} \\
b &amp;amp; \text{batch size} \\
h &amp;amp; \text{number of heads} \\
s_s &amp;amp; \text{length of a segment} \\
s_m &amp;amp; \text{length of the memory} \\
s = s_m + s_s &amp;amp; \text{length of segment + memory} \\
d &amp;amp; \text{size of an embedding} \\
d_{in} &amp;amp; \text{size of the embedding as input to a given layer} \\
d_{out} &amp;amp; \text{size of the embedding as output from a given layer} \\
(b, h, s, d): &amp;amp; \text{tensor of 4 dimension reflecting those dimensions}\\
l &amp;amp; \text{index of a layer} \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The number of parameters for this type of model is extremely large.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;algorithm-for-a-single-layer-with-matrix-dimension&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Algorithm for a single layer with matrix dimension&lt;/h2&gt;
&lt;p&gt;The implementation will use the &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/latest/&#34;&gt;Pytorch Lightning&lt;/a&gt; library. The entire code is on &lt;a href=&#34;https://github.com/Emmanuel-R8/Time_Series_Transformers&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import sys
import inspect

from typing import *

import torch
import torch.nn as nn

from pytorch_lightning.core.lightning import LightningModule

from utils.exp_utils import logging&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# From https://github.com/harvardnlp/annotated-transformer
class PositionalEncoding(LightningModule):
    &amp;quot;Implement the edited PE function, depends on sequence length rather than input dimensionnality.&amp;quot;

    def __init__(self, runParam, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        batch

        # Compute the positional encodings once in log_2 space ceiled to sequence_length.
        b = math.ceil(math.log(max_sequence_length * 4, 2))
        a = int(2**b / 4)  # Up to a quarter of a sine wave
        x1 = np.array([[math.cos(0.5**i*x*2*math.pi) for x in range(max_sequence_length, 0, -1)] for i in range(1, b+1)])
        x2 = np.array([[math.sin(0.5**i*x*2*math.pi) for x in range(max_sequence_length, 0, -1)] for i in range(2, b+2)])
        x = np.concatenate([x1, x2], axis=0)
        print(&amp;quot;x.shape():&amp;quot;, x.shape)
        x = np.expand_dims(x, 0).repeat(repeats=batch_size, axis=0)
        print(&amp;quot;x.shape():&amp;quot;, x.shape)

        # Register it into PyTorch
        pe = torch.from_numpy(x).float()
        pe = pe.transpose(-1, -2)
        print(&amp;quot;pe.size():&amp;quot;, pe.size())
        self.register_buffer(&amp;#39;pe&amp;#39;, pe)

    def forward(self, x):
        pos = Variable(self.pe, requires_grad=False)
        # print(pos.size(), x.size())  # [batch_size, -1, sequence_length], [batch_size, sequence_length, hidden_size]
        pe = self.pe[:, -x.size(1):]  # limiting positional encoding to a poentially shorter sequence_length
        print(&amp;quot;pe.size(), x.size():&amp;quot;, pe.size(), x.size())
        x = torch.cat([x, pe], dim=-1)
        return self.dropout(x), pos&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
\text{Step for a} &amp;amp; &amp;amp; \text{Dimensions for multiple layers} \\
\text{single layer} &amp;amp; &amp;amp; \text{(with Einstein summation)} \\
\hline \\
h^{l-1}_\tau &amp;amp; &amp;amp; (l, h, s_s, d_{in}) \\
\hline \\
\tilde{h}^{l-1}_\tau = &amp;amp; \left[ StopGradient(m^{l-1}_\tau) \circ  h^{l-1}_\tau \right]  &amp;amp; (l, h, s, d_{in}) = (l, h, s_m, d_{in}) \circ (l, h, s_s, d_{in}) \\
\hline \\
q^l_\tau = &amp;amp; h^{l-1}_\tau \cdot {W^l_Q}^\top &amp;amp; (l, h, s_s, d_{in}) = (l, h, s_s, d_{in}) \dot {(l, h, d_{in}, d_{in})}^\top \\
&amp;amp; &amp;amp; lhsd_1,lhsd_1d_2 \rightarrow lhsd_2 \\
\hline \\
k^l_\tau = &amp;amp; \tilde{h}^{l-1}_\tau \cdot {W^l_K}^\top &amp;amp; (l, h, s, d_{in}) = (l, h, s, d_{in}) \dot {(l, h, d_{in}, d_{in})}^\top \\
&amp;amp; &amp;amp; lhsd_1,lhsd_1d_2 \rightarrow lhd_2 \\
\hline \\
v^l_\tau = &amp;amp; \tilde{h}^{l-1}_\tau \cdot {W^l_V}^\top &amp;amp; (l, h, s, d_{in}) = (l, h, s, d_{in}) \dot {(l, h, d_{in}, d_{in})}^\top \\
&amp;amp; &amp;amp; lhsd_1,lhsd_1d_2 \rightarrow lhd_2 \\
\hline \\
A^l_\tau = &amp;amp; {Q^l}^\top \dot \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notation-summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notation summary&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Size parameters&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{ll}
l:        &amp;amp;   \text{number of layers, starting from 1} \\
d:        &amp;amp;   \text{dimension of all input (including embeddings) AND output vectors passed from a layer to the next} \\
n_{head}: &amp;amp;   \text{number of self-attention heads in a TransformerXL layer} \\
n_s:      &amp;amp;   \text{size of a training segment = number of inputs into and outputs from a layer} \\
s = n_s:  &amp;amp;   s \text{ will be used as index name to represent the input dimension when using named tensors,}  \\
&amp;amp; \text{i.e. Einstein summation.}  \\
b:        &amp;amp;   \text{size of a training batch, i.e. number of segments in a batch. Used in named tensors as well} \\
\\
n_m:      &amp;amp;   \text{number of hidden states kept in the primary memory. n_m is a multiple of n_s} \\
c:        &amp;amp;   \text{memory compression ratio} \\
n_{cm}:   &amp;amp;   \text{number of states kept in the compressed memory. n_{cm} }  \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;useful-references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Useful references&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://nlp.seas.harvard.edu/2018/04/03/attention.html&#34;&gt;The Annotated Transformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.fromkk.com/posts/the-annotated-the-annotated-transformer/&#34;&gt;The Annotated The Annotated Transformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://d2l.ai/chapter%5Fattention-mechanisms/transformer.html&#34;&gt;Dive into Deep Learning - 10.3 Transformer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>RNN Compressive Memory Part 1: A high level introduction.</title>
      <link>/2020/03/07/rnn-compressive-memory-part-1.html</link>
      <pubDate>Sat, 07 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/2020/03/07/rnn-compressive-memory-part-1.html</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#recurrent-neural-networks-rnn&#34;&gt;Recurrent Neural Networks (&lt;em&gt;RNN&lt;/em&gt;)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#from-simple-rnns-to-lstms&#34;&gt;From simple RNNs to LSTMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#longshort-term-memory-rnns&#34;&gt;Long/Short Term Memory RNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#attention&#34;&gt;Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#beyond-lstm-transformers&#34;&gt;Beyond LSTM: Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transformer-xl&#34;&gt;Transformer-XL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compressive-transformers&#34;&gt;Compressive Transformers&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compression-scheme&#34;&gt;Compression scheme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compression-training&#34;&gt;Compression training&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;This is the first post of series dedicated to Compressive Memory of Recurrent Neural Networks. This is inspired by a recent DeepMind paper published in November 2019 on &lt;a href=&#34;https://arxiv.org/abs/1911.05507&#34;&gt;Arxiv&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Currently, the ambition of the series is to follow this plan:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Part 1 (here): A high level introduction to Compressive Memory mechanics starting from basic RNNS;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;../../05/20/rnn-compressive-memory-part-2.html&#34;&gt;Part 2&lt;/a&gt;: a detailed explanation of the TransformerXL;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Part 3: an implementation using PyTorch (soon);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Part 4: finally, its application to time series (soon).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most likely, this will be fine-tuned over time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Big thanks to &lt;a href=&#34;https://gmarti.gitlab.io/&#34;&gt;Gautier Marti&lt;/a&gt; and &lt;a href=&#34;http://zoonek.free.fr/blosxom/&#34;&gt;Vincent Zoonekynd&lt;/a&gt; for their suggestions and proof-reading!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Additional diagrams (14 March 2020)&lt;/p&gt;
&lt;div id=&#34;recurrent-neural-networks-rnn&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recurrent Neural Networks (&lt;em&gt;RNN&lt;/em&gt;)&lt;/h2&gt;
&lt;div id=&#34;from-simple-rnns-to-lstms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;From simple RNNs to LSTMs&lt;/h3&gt;
&lt;p&gt;Traditional neural networks were developed to train/run on information provided in a single step in a consistent format (e.g. images with identical resolution). Conceptually, a neural network could similarly be taught on sequential information (e.g. a video as a series of images) looking at it as a single sample, but that would require (1) being trained on the full sequence (e.g. an entire video), (2) being able to cope with information of variable length (i.e. short vs. long video). (1) is computationally intractable, and (2) means that units analysing later parts of the video would not be receiving as much training as earlier units when ideally they should be all share the same amount of training .&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/media/post/2020-rnn-compressive-memory/Recurrent_neural_network_unfold.svg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;strong&gt;Basic RNN&lt;/strong&gt; (source: &lt;em&gt;Wikipedia&lt;/em&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The original RNN address those issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Sequences are chopped in small consistent sub-sequences (say, a &lt;em&gt;segment&lt;/em&gt; of 10 images, or a group of 20 words).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An RNN layer is a group of blocks (or &lt;em&gt;cells&lt;/em&gt;), each receiving a single element of the segment as input. Note that here &lt;em&gt;layer&lt;/em&gt; does not have the traditional meaning of a layer of neural units fully connected to a previous layer of units. It is a layer of RNN cells. Within each cell, quite a few things happen, including using layers of neural units. From here on, a &lt;em&gt;layer&lt;/em&gt; will refer to an &lt;em&gt;RNN layer&lt;/em&gt; and not a layer of neural units..&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Within a layer, cells are identical: they have the same parameters.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although each element of a sequence might be of interest on its own, it only becomes really meaningful in the context of the other elements. Each cell contains a state vector (called &lt;em&gt;hidden state&lt;/em&gt;). Each cell is trained using an individual element from a segment and the hidden state from the preceding cell. Training the network means training the creation of those states. Passing of the hidden state transfers some context or memory from prior elements of the segment. The cells receiving a segment form a single layer. Each cell would typically (but not necessarily) also include an additional sub-cell to create an output as a function of the hidden step. In that case, the output of a layer can then be used as input of new RNN layer.&lt;/p&gt;
&lt;p&gt;A layer is trained passing hidden states from prior cells to later cells. The hidden state from prior elements is used to contextualise a current element. To use context from later elements (e.g. in English, a noun giving context to a preceding adjective), a separate layer is trained where context instead passes from later to prior elements. Those forward and backward layers jointly create a &lt;em&gt;bidirectional RNN&lt;/em&gt; .&lt;/p&gt;
&lt;p&gt;Historically, RNNs applied to NLP deal with elements which are either one-hot encoded (either letters, or, more efficient, tokens), or word embeddings often normalised as unit vectors (for example see &lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34;&gt;Word2Vec&lt;/a&gt; and &lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34;&gt;GloVe&lt;/a&gt;). RNN cells therefore deal with values between 0 and 1. Typically, non-linearity is brought by &lt;span class=&#34;math inline&#34;&gt;\(tanh\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(sigmoid\)&lt;/span&gt; activations which guarantee unit values within that range. Those activation functions quickly have very flat gradients. Segments often have 10s or 100s of elements. Because of vanishing gradients, a hidden state receives little information from distant cells (training gradients are hardly influenced by gradients of distant cells).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;longshort-term-memory-rnns&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Long/Short Term Memory RNNs&lt;/h3&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/media/post/2020-rnn-compressive-memory/Long_Short-Term_Memory.svg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;strong&gt;Basic LSTM RNN&lt;/strong&gt; (source: &lt;em&gt;Wikipedia&lt;/em&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Long/Short Term Memory RNNs (&lt;em&gt;LSTM&lt;/em&gt;) address this by passing two states:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;a hidden state &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; as described above trained with non-linearity: this is the &lt;em&gt;short-term memory&lt;/em&gt;; and,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;another hidden state &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; (called &lt;em&gt;context&lt;/em&gt;) weighting previous contexts with a simple exponential moving average (in &lt;em&gt;Gated Recurrent Units&lt;/em&gt;) or a slightly more complicated version thereof in the original LSTM model structure. Determining the optimal exponential decay is part of the training process. This minimally processed state is the &lt;em&gt;long-term memory&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LTSM can also be made bidirectional.&lt;/p&gt;
&lt;p&gt;Without going into further details, note that each &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tanh\)&lt;/span&gt; orange block represents matrix of parameters to be learned.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;attention&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Attention&lt;/h3&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/media/post/2020-rnn-compressive-memory/Attention_RNN.svg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;strong&gt;Attention RNN&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;RNN were further extended with an &lt;em&gt;attention mechanism&lt;/em&gt;. Blog posts on attention by &lt;a href=&#34;https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&#34;&gt;Jay Alammar&lt;/a&gt; and &lt;a href=&#34;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html&#34;&gt;Lilian Weng&lt;/a&gt; are good introductions.&lt;/p&gt;
&lt;p&gt;A multi-layer RNN takes the output a layer and uses it as input for the next. With the attention mechanism, the outputs go through an attention unit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;beyond-lstm-transformers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Beyond LSTM: Transformers&lt;/h3&gt;
&lt;p&gt;RNNs were then simplified (insert large air quotes) with &lt;em&gt;Transformers&lt;/em&gt; (using what is called &lt;em&gt;self-attention&lt;/em&gt;) that significantly reduce the number of model parameters and can be efficiently parallelised with minimum model performance impact. For an extremely clear introduction to those significant improvements, you cannot do better than reading , and by &lt;a href=&#34;http://www.peterbloem.nl/blog/transformers&#34;&gt;Peter Bloem&lt;/a&gt; on transformers. The following assumes that you are broadly familiar with those ideas.&lt;/p&gt;
&lt;p&gt;The basic transformer structure uses self-attention where, for a given element (the &lt;em&gt;query&lt;/em&gt;), the transformer looks at the other elements of the segment (the &lt;em&gt;keys&lt;/em&gt;) to determine how much ‘attention’ other elements of the segment influence the role of the query in changing the hidden state.&lt;/p&gt;
&lt;p&gt;Broadly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The query is projected in some linear space (a matrix &lt;span class=&#34;math inline&#34;&gt;\(W_q\)&lt;/span&gt;). That’s basically an embedding which is part of the model training.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All the other elements, the keys, are projected in another linear space (a matrix &lt;span class=&#34;math inline&#34;&gt;\(W_k\)&lt;/span&gt;); another embedding which is part of the model training.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The similarity (perharps &lt;em&gt;affinity&lt;/em&gt; would be a better word) between the projected query and each projected key is calculated with a dot product / cosine distance. This is exactly the approach of basic recommender systems with the difference that the recommendation is between sets of completely different nature (for example affinity between users and movies). Note that although query and keys are elements of identical type, they are embedded into different spaces with different projections matrices.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We now have a vector of the same size as the segment length (one cosine distance per input element). It goes through another layer (a matrix &lt;span class=&#34;math inline&#34;&gt;\(W_v\)&lt;/span&gt;) to give a &lt;em&gt;value&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The triplet of &lt;span class=&#34;math inline&#34;&gt;\(\left( W_q, W_k, W_v \right)\)&lt;/span&gt; is called an &lt;em&gt;attention head&lt;/em&gt;. Actual models would include multiple heads (of the order of 10), and the output of a transformer layer could then feed into a new transformer layer.&lt;/p&gt;
&lt;p&gt;This model is great until you notice that the dot product / cosine similarity is commutative and does not reflect whether a key element is located before or after the query element: order is fundamental to sequential information (“quick fly” vs. “fly quick”). To address this, the input elements are always enriched with a positional embedding: the input elements are concatenated with positional information showing where they stand within a segment.&lt;/p&gt;
&lt;p&gt;Note that a transformer layer is trained on a segment using only the information from that segment. This is fine to train on sentences, but it cannot really account for more distant relationships between words within a lengthy paragraph, let alone a full text.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transformer-xl&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Transformer-XL&lt;/h3&gt;
&lt;p&gt;Transformers have been further improved with &lt;a href=&#34;https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html&#34;&gt;Tranformer-XL&lt;/a&gt; (XL = extra long) which are trained using hidden states from previous segments, therefore using information from several segments, to improve a model’s memory span.&lt;/p&gt;
&lt;p&gt;Conceptually, this is an obvious extension of the basic transformer to increase its memory span. But there is a fundamental problem. Going back to the basic transformer, each element includes its absolute position within the segment. The position of the first word of the segment is 1, that of the last one is, say, 250 . Such a scheme breaks down as soon as the state of the previous segment is taken into account. Word 1 of the current segment obviously comes before word 250, but has to come after word 250 of the previous segment. The absolute position encoding does not reflect the relative position of elements located in different segments.&lt;/p&gt;
&lt;p&gt;The key contribution of the Transformer-XL is to develop a relative positional encoding that allows hidden state information to cross segment boundaries. In their implementation, the authors evaluate that the attention length, being basically how many hidden states are used, is 450% longer that the basic transformer. That’s going from sentence length to full paragraph, but still far from a complete book.&lt;/p&gt;
&lt;p&gt;A side, but impressive, benefit is that the evaluation speed of the model, or it use once trained, is significantly increased thanks to the relative addressing (the paper states up to a 1,800-fold increase depending on the attention length).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;compressive-transformers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compressive Transformers&lt;/h2&gt;
&lt;p&gt;Full text understanding cannot be achieved by simply lengthening segment sizes from 100s to the &lt;a href=&#34;https://blog.reedsy.com/how-many-words-in-a-novel/&#34;&gt;word count&lt;/a&gt; of a typical novel (about 100,000). When training a model routinely takes 10s of hours on GPU clusters, an increase by 3 orders of magnitude is not realistic.&lt;/p&gt;
&lt;p&gt;In a recent &lt;a href=&#34;https://arxiv.org/abs/1911.05507&#34;&gt;paper&lt;/a&gt;, DeepMind proposes a new RNN model called &lt;em&gt;Compressive Transformers&lt;/em&gt;.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Transformer-XL uses the hidden state of a prior segment (&lt;span class=&#34;math inline&#34;&gt;\(h_{T-1}\)&lt;/span&gt;) to improve the training of the current segment (&lt;span class=&#34;math inline&#34;&gt;\(h_{T}\)&lt;/span&gt;). When moving to the next segment, training (&lt;span class=&#34;math inline&#34;&gt;\(h_{T+1}\)&lt;/span&gt;) now only uses &lt;span class=&#34;math inline&#34;&gt;\(h_{T}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(h_{T-1}\)&lt;/span&gt; is discarded. To increase the memory span, one could train using more past segments at the expense of increase in memory usage and computation time (quadratic). The actual Transformer-XL uses the hidden states of several previous segments, but the discarding mechanism will remain.&lt;/p&gt;
&lt;p&gt;The key contribution of the Compressive Transformers is the ability to retain salient information from those otherwise discarded past states. Instead of being discarded, they are stored in compressed form.&lt;/p&gt;
&lt;p&gt;Each Transformer-XL layer is now trained with prior hidden states (&lt;em&gt;primary memory&lt;/em&gt;) and the &lt;em&gt;compressed memory&lt;/em&gt; of older hidden states.&lt;/p&gt;
&lt;p&gt;As an aside, although not explicitly mentioned, we should note that the ‘-XL’ aspect of the Transformer-XL and the memory compression mechanics are conceptually independent from the actual types of RNN cell. Simple RNNs, GRUs or LSTMs could be trained using the hidden states of past segments (not dissimilar to state/context peeking into past cells in certain RNN variants). But the performance benefit of Transformer-XL is such that the paper only focuses on transformer-XL.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compression-scheme&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compression scheme&lt;/h3&gt;
&lt;p&gt;As compared to Transformer-XL, the key difference is the compression scheme. The rest of the model seems identical.&lt;/p&gt;
&lt;div id=&#34;size-parameters&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Size parameters&lt;/h4&gt;
&lt;p&gt;The size of the model is described with a few size parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n_s\)&lt;/span&gt;: size of a segment = the number of cells in a layer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n_m\)&lt;/span&gt;: number of hidden states in the primary uncompressed memory (like the Transformer-XL). &lt;span class=&#34;math inline&#34;&gt;\(n_m\)&lt;/span&gt; is a multiple of &lt;span class=&#34;math inline&#34;&gt;\(n_s\)&lt;/span&gt;. The primary memory is a FIFO buffer: the first (oldest) memories will be the first to be later compressed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(n_{cm}\)&lt;/span&gt;: number of compressed hidden states in the compressed memory. States in the compressed memory will compress an old segment of size &lt;span class=&#34;math inline&#34;&gt;\(n_s\)&lt;/span&gt; dropping out of the primary memory. &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is an information compression ratio from &lt;span class=&#34;math inline&#34;&gt;\(n_s\)&lt;/span&gt; primary memory entries into compressed memory entries. There can be two ways of applying this compression ratio, which both reduce the number of hidden states by the same ratio:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; uncompressed layers could create a single compressed hidden state of identical size. This merges the information of a group of elements (e.g. &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; words) into a single hidden state. In this case, &lt;span class=&#34;math inline&#34;&gt;\(n_s\)&lt;/span&gt; is proportional to &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_{cm}\)&lt;/span&gt; is proportional to &lt;span class=&#34;math inline&#34;&gt;\(n_s / c\)&lt;/span&gt;. The authors do not use this approach. It would enforce a sub-segmentation of an uncompressed segment at arbitrary intervals (why group 3 words instead of 5 or 7…)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Instead, the authors use dimension reduction: a single uncompressed hidden state is compressed into a new hidden state with &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; times fewer hidden states. If the size of the hidden state of a Transformer-XL cell is &lt;span class=&#34;math inline&#34;&gt;\(n_h\)&lt;/span&gt;, hidden states in the primary memory will have the same size, and the compressed memory hidden states will have a size of &lt;span class=&#34;math inline&#34;&gt;\(n_h / c\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By way of example, a segment could have 100 cells (&lt;span class=&#34;math inline&#34;&gt;\(n_s = 100\)&lt;/span&gt;). This segment could be trained with the hidden states of the past 3 segments’ training (&lt;span class=&#34;math inline&#34;&gt;\(n_m = 3 * n_s = 300\)&lt;/span&gt;). When training the next segment, an old segment of size 100 becomes available for compression which will create 100 new hidden states.&lt;/p&gt;
&lt;p&gt;This example is for a single layer. The same scheme would be replicated for each layer of the model&lt;/p&gt;
&lt;p&gt;Note that the paper only contemplates a single set of compressed memories. There could also be multiple generations of compressed memories, primary memory compresses in generation 1, then compressing into generation 2…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compression-functions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Compression functions&lt;/h4&gt;
&lt;p&gt;A compressed hidden state is created from &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; primary memory hidden states. When training on texts with word embeddings,the authors used a value of &lt;span class=&#34;math inline&#34;&gt;\(c=3\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(c=4\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Several compression schemes are explored in the paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;max or mean pooling with a stride of &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;. This is typical of image convolution networks - no explanation required.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;1-dimensional convolution with a stride of &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;. This is also typical of image convolution network apart from being one-dimensional. This requires parameter training.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1511.07122.pdf&#34;&gt;dilated convolution&lt;/a&gt;. In practice image convolutions have shown to be inadequate for sequential information where dependencies can be at both short and long ranges: working at different scales makes sense. Dilated convolutions use convolution filters that are contracted and dilated versions of a template to be trained.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;a &lt;em&gt;most-used&lt;/em&gt; mechanism that identifies and retains part of the hidden states according to their importance in the cells training gauged by the attention they received.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;compression-training&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compression training&lt;/h3&gt;
&lt;p&gt;Training the compression parameters is done separately from the optimisation of the Transformer-XL cells.&lt;/p&gt;
&lt;p&gt;The purpose of the compressed memory is to provide a compressed and lossy representation of the primary memory (hidden states) or the attention heads parameters: the quality of the compression mechanics is assessed by how well the original information can be re-generated from it. In essence, the compressed hidden states are a compressed representations to a learned representation vector in an auto-encoder. This is the training mechanics used by the authors.&lt;/p&gt;
&lt;p&gt;As in an auto-encoder, the representation is learned by comparing the original information to its reconstruction. This training is kept completely independent from the training of the transformers: the auto-encoding loss and gradients do not impact the attention heads’ parameters.&lt;/p&gt;
&lt;p&gt;Conversely, the loss and gradients of the attention heads’ training do not flow into the training of the compression scheme.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;This was a high level introduction of RNNs all the way up to Compressive Memory mechanics. Next, the algorithm’s nitty-gritty.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Quick Thought: Universal translator and same language translator</title>
      <link>/2019/10/25/quick-idea-universal-translator-and-same-language-translator.html</link>
      <pubDate>Fri, 25 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/2019/10/25/quick-idea-universal-translator-and-same-language-translator.html</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Quick Thoughts are random thoughts looking for comments&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let’s imagine a universal translator able to translate any language to any language. Sourcing a corpus of pair translation is a major hurdle. However there is an almost infinite corpus of pair translations: a language with itself; translating English to English is easy, even for a computer.&lt;/p&gt;
&lt;p&gt;Let’s give the blackbox universal translator three inputs: a source text, the language of the source text, the language of the desired translation. What would be the consequences for the learning system inside the blackbox of being constrained that if the languages are the same, the output has to be identical to the input?&lt;/p&gt;
&lt;p&gt;Obviously, the blackbox could quickly learn that bypassing the translation does the trick. However, that would probably require the internal circuitry to allow for the bypass, and that could be constrained out. So:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Could we expect any interesting result?&lt;/li&gt;
&lt;li&gt;Could the input to be eventually forced down to a language-independent universal representation?&lt;/li&gt;
&lt;li&gt;Let’s say there is a language-independent universal representation kernel. If the input comes in without information of which is the output language, and the output has no information of what the input language was, does it force the network to create a universal representation, or would it just withered away?&lt;/li&gt;
&lt;li&gt;Is it possible to &lt;em&gt;invert&lt;/em&gt; a network? Probably not in a truly bijective way, but to model the fact that text representation &lt;span class=&#34;math inline&#34;&gt;\(\rightarrow\)&lt;/span&gt; universal representation is the &lt;em&gt;inverse&lt;/em&gt; (for some definition of the word) of universal representation &lt;span class=&#34;math inline&#34;&gt;\(\rightarrow\)&lt;/span&gt; text representation of the same language?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Comments welcome&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Network - Incremental Growth</title>
      <link>/2019/10/23/neural-network-incremental-growth.html</link>
      <pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/2019/10/23/neural-network-incremental-growth.html</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#draft-1&#34;&gt;&lt;strong&gt;&lt;em&gt;DRAFT 1&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#singular-matrix-decomposition&#34;&gt;Singular matrix decomposition&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#where-next&#34;&gt;Where next?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#back-to-svd&#34;&gt;Back to SVD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regularisation&#34;&gt;Regularisation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#vector-coordinates&#34;&gt;Vector coordinates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eigenvalues&#34;&gt;Eigenvalues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#threshold&#34;&gt;Threshold&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#by-2-decision-matrix&#34;&gt;2-by-2 decision matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#todo-other-principal-components-methods&#34;&gt;[TODO] Other Principal Components methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#todo-proof-of-the-pudding&#34;&gt;[TODO] Proof of the pudding…&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#limitations-and-further-questions&#34;&gt;Limitations and further questions&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#limitations&#34;&gt;Limitations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#further-questions&#34;&gt;Further questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#litterature&#34;&gt;Litterature&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;div id=&#34;draft-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;strong&gt;&lt;em&gt;DRAFT 1&lt;/em&gt;&lt;/strong&gt;&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;We all have laptops. But le’ts face it, even in times of 32GB of RAM and NVMe2 drives, forget about running any interesting TensorFlow model. You need to get an external GPU, build your own rig, or very quickly pay a small fortune for cloud instances.&lt;/p&gt;
&lt;p&gt;Back in 1993, I read a paper about growing neural networks neuron-by-neuron. I have no other precise recollection about this paper apart from the models considered being of the order of 10s of neurons and the weight optimisation being made on a global basis, i.e. not layer-by-layer like backpropagation. Nowadays, it is still too often the case that finding a network structure that solves a particular problem is a random walk: how many layers, with how many neurons, with which activation functions? Regularisation methods? Drop-out rate? Training batch size? The list goes on.&lt;/p&gt;
&lt;p&gt;This got me thinking about how a training heuristic could incrementally modify a network structure given a particular training set and, apart maybe from a few hyperparameters, do that with no external intervention. At regular training intervals, a layer&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; will be modified depending on what it &lt;em&gt;seems&lt;/em&gt; able or not to achieve. As we will see, we will use unsupervised learning methods to do this: a layer modification will be independent of the actual learning problem and automatic.&lt;/p&gt;
&lt;p&gt;Many others have looked into that. But what I found regarding self-organising networks is pre-2000, and nothing in the context of deep learning. So it seems that the topic has gone out of fashion because of the current amounts of computing power, or has been set aside for reasons unknown. (See references at the end). In any event, it is interesting enough a question to research it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Let us look at a simple 1-D layer and decompose what it exactly does. Basically a layer does:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{ouput} = f(M \times \text{input})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the input &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; has size &lt;span class=&#34;math inline&#34;&gt;\(n_I\)&lt;/span&gt;, the output &lt;span class=&#34;math inline&#34;&gt;\(O\)&lt;/span&gt; has size &lt;span class=&#34;math inline&#34;&gt;\(n_I\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; being the activation function, we have (where &lt;span class=&#34;math inline&#34;&gt;\(\odot\)&lt;/span&gt; represents the matrix element-wise application of a function):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
O = f \odot (M \times I)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, looking at &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;, what does it really do? At one extreme, if &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; was the identity matrix, it would essentially be useless (bar the activation function&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;). This would be a layer candidate for deletion. The question is then:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Looking at the matrix representing a layer, can we identify which parts are (1) useless, (2) useful and complex enough, or (3) useful but too simplistic?&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here, &lt;em&gt;complex enough&lt;/em&gt; or &lt;em&gt;simplistic&lt;/em&gt; is basically a synonym of “&lt;em&gt;one layer is enough&lt;/em&gt;”, or “&lt;em&gt;more layers are necessary&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;The idea to look for important/complex information which where the network needs to grow more complex; and identify trivial information which can be discarded, or can be viewed as minor adjustments to improve error rates (basically overfitting…)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Caveat&lt;/em&gt;: Note that we ignore the activation function. They are key to introduce non-linearity. Without it, a network is only a linear function, i.e. no interest. They have a clear impact on the performance of a network.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;singular-matrix-decomposition&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Singular matrix decomposition&lt;/h1&gt;
&lt;p&gt;There exists many ways to decompose a matrix. Singular matrix decomposition (&lt;em&gt;SVD&lt;/em&gt;) &lt;span class=&#34;math inline&#34;&gt;\(M = O \Sigma I^\intercal\)&lt;/span&gt; is an easy and efficient way to interpret what a given matrix does. SVD builds on the eigenvectors (expressed in an orthonormal basis), and eigenvalues. (Note that &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; is real-valued, so we use the transpose notation &lt;span class=&#34;math inline&#34;&gt;\(M^\intercal\)&lt;/span&gt; instead of the conjugate transpose &lt;span class=&#34;math inline&#34;&gt;\(M^*\)&lt;/span&gt;.)&lt;/p&gt;
&lt;p&gt;In a statistical world, SVD (with eigenvalues ordered by decreasing value) is how to do principal component analysis(&lt;em&gt;PCA&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;In a geometrical context, SVD:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;takes a vector (expressed in the orthonormal basis);&lt;/li&gt;
&lt;li&gt;re-expresses onto a new basis made of the eigenvectors (that would only exceptionally be orthonormal);&lt;/li&gt;
&lt;li&gt;dilates/compresses those components by the relevant eigenvalues;&lt;/li&gt;
&lt;li&gt;and returns this resulting vector expressed back onto the orthonormal basis.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As presented here, this explanation requires a bit more intellectual gymnastic when the matrix is not square (i.e. when the input and output layers have different dimensions), but the principle remains identical.&lt;/p&gt;
&lt;div id=&#34;where-next&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Where next?&lt;/h3&gt;
&lt;p&gt;Taking the statistical and geometrical points of view together, the layer (matrix &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;) shuffles the input vector in its original space space where some specific directions are more important than others. Those directions are linear combinations of the input neurons, each combinations is along the eigenvectors. Those combinations are given more or less importance as expressed by the eigenvalues. (Note that the squares of the eigenvalues expressed how much information each combination brings to the table.)&lt;/p&gt;
&lt;p&gt;Intuitively, the simplest and most useless &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; would be the identity matrix (the input units are repeated), or zero matrix (the input units are dropped because useless). Let us repeat the caveat that the activation function is ignored.&lt;/p&gt;
&lt;p&gt;If compared to the identity matrix, the SVD shows that &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; includes (at least) two types of important information identified:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are interesting combinations of the input units? This is expressed by how much the input vector is rotated in space.&lt;/li&gt;
&lt;li&gt;Independently from whether a combination is complicated or not (i.e. multiple units, or unit passthrough), how an input is amplified (as expressed by the eigenvalues).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The idea is then produce a 2x2 decision matrix with high/low rotation mess and high/low eigenvalues.&lt;/p&gt;
&lt;p&gt;A picture is gives the intuition of what we are after:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/media/post/3-random-thoughts/2019-10-23-neural-network-incremental-growth/Network-Incremental-Growth-Matrix-Split.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Transformation of the Layer Matrix&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Looking from top to bottom at what the “after” matrices would be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Part of the original layer, immediately followed by a new one (we will see below what that would look like). The intuition is that this layer is really messing things up down the line, or seems very sensitive.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Part of the original layer where the number of units would be increased (here doubled as an example).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Part of the original layer kept &lt;em&gt;functionally&lt;/em&gt; essentially as is.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Delete the rest which is either not sensitive to input or outputs nothings. This would be within a certain precision. That is basically a form of regularisation preventing the overall model to be too sensitive. I am aware that there are other types of regularisations, but that will go in the limitations category.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The next layer would take as input all the transformed outputs.&lt;/p&gt;
&lt;p&gt;In practice, the picture presents the matrices separated. This is for ease of understanding. In reality the same effect would be achieved if the three dark blue sub-layers are merged in a single layer.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;back-to-svd&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Back to SVD&lt;/h1&gt;
&lt;p&gt;Let us assume that there are &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; input units and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; output units. &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; then is of dimensions &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt;. The matrices of the SVD have dimensions:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{matrix}
M          &amp;amp; = &amp;amp; O          &amp;amp; \Sigma     &amp;amp; I^\intercal \\
m \times n &amp;amp;   &amp;amp; m \times m &amp;amp; m \times n &amp;amp; n \times n \\
\end{matrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that instead of using &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; to name the sub-matrices of the SVD, we use &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(O\)&lt;/span&gt; to represent &lt;em&gt;input&lt;/em&gt; and &lt;em&gt;output&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(O\)&lt;/span&gt; can be written as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
I =
\begin{pmatrix} |   &amp;amp;        &amp;amp; |    \\ i_1 &amp;amp; \cdots &amp;amp; i_m  \\ |   &amp;amp;        &amp;amp; |    \\ \end{pmatrix} 
\qquad \text{and} \qquad
O =
\begin{pmatrix} |   &amp;amp;       &amp;amp; | \\ o_1 &amp;amp; \cdots &amp;amp; o_n \\ |   &amp;amp;       &amp;amp; | \\ \end{pmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
M =
O \Sigma I^\intercal =
\begin{pmatrix} | &amp;amp;&amp;amp; | \\ o_1 &amp;amp; \dots &amp;amp; o_m \\ | &amp;amp; &amp;amp; | \\ \end{pmatrix}
\begin{pmatrix} \sigma_1 \\ &amp;amp; \sigma_2 \\ &amp;amp;&amp;amp; \ddots \\ &amp;amp;&amp;amp;&amp;amp; \sigma_r \\ &amp;amp;&amp;amp;&amp;amp;&amp;amp; 0 \\ &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp; \ddots \\ &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp; 0 \\ \end{pmatrix}
\begin{pmatrix} - &amp;amp; i_1 &amp;amp; - \\ &amp;amp; \vdots &amp;amp; \\ -  &amp;amp; i_n &amp;amp; - \\ \end{pmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; non-zero eigenvalues.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regularisation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Regularisation&lt;/h1&gt;
&lt;p&gt;At this stage, we can regularise all components.&lt;/p&gt;
&lt;div id=&#34;vector-coordinates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Vector coordinates&lt;/h2&gt;
&lt;p&gt;For each vector &lt;span class=&#34;math inline&#34;&gt;\(i_k\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(o_k\)&lt;/span&gt;, we could zero its coordinates when below a certain threshold (in absolute value). All the coordinates will &lt;span class=&#34;math inline&#34;&gt;\(-\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; since each vector has norm 1 (&lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(O\)&lt;/span&gt; are orthonormal), therefore all of them will be regularised in similar ways.&lt;/p&gt;
&lt;p&gt;After regularisation, the matrices will not be orthonormal anymore. They can easily be made normal by scaling up by &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{\sum_{k}i_k^2}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{\sum_{k}o_k^2}\)&lt;/span&gt;. There is no generic way to revert to an orthogonal basis and keep the zeros.&lt;/p&gt;
&lt;p&gt;We need a way to measure the &lt;code&gt;rotation messiness&lt;/code&gt; of each vector. As a shortcut, we can use the proportion of non-zero vector coordinates (after &lt;em&gt;de minimis&lt;/em&gt; regularisation).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;eigenvalues&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Eigenvalues&lt;/h2&gt;
&lt;p&gt;The same can be done for the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;s. As an avenue of experimentation, those values can not only be zero-ed in places, but also rescale the large values in some non-linear way (e.g. logarithmic or square root rescaling).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;threshold&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Threshold&lt;/h2&gt;
&lt;p&gt;Where to set the threshold is to be experimented with. Mean? Median since more robust? Some quartile?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;by-2-decision-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2-by-2 decision matrix&lt;/h2&gt;
&lt;p&gt;Based on those regularisation, we would propose the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
\begin{matrix}
                    &amp;amp; \text{low rotation messiness} &amp;amp; \text{high rotation messiness} \\
\text{high } \sigma &amp;amp; \text{Double height}          &amp;amp; \text{Double depth}            \\
\text{low } \sigma  &amp;amp; \text{Delete}                 &amp;amp; \text{Keep identical}          \\
\end{matrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;todo-other-principal-components-methods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;[TODO] Other Principal Components methods&lt;/h1&gt;
&lt;p&gt;SVD is PCA. Projects information on hyperplanes.&lt;/p&gt;
&lt;p&gt;Reflect on non-linear versions: Principal Curves, Kernel Principal Components, Sparse Principal Components, Independent Component Analysis. (_Elements of Statistical Learning s. 14.5 seq.).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;todo-proof-of-the-pudding&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;[TODO] Proof of the pudding…&lt;/h1&gt;
&lt;p&gt;Implementations!!!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;limitations-and-further-questions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Limitations and further questions&lt;/h1&gt;
&lt;div id=&#34;limitations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Limitations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Only 1-D layers. Higher-order SVD is in principle feasible for higher order tensors. Other methods?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We delete the eigenvectors associated to low eigenvalues and limited rotations. There are other forms of regularisations, e.g. random weight cancelling that would not care about anything eigen-.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the real impact of ignoring the activation function? PCA requires centered values. Geometrically, uncentered values would mean more limited rotations since samples would be in quadrant far from 0.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;further-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Further questions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The final structure is a direct product of the training set. What if the training is done differently (batches sized or ordered differently)?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What about training many variants with different subsets of the training set and using ensemble methods?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The eigenvalues could be modified when creating the new layers. By decreasing the highest eigevalues (in absolute value), we effectively regularise the layers outputs. This decrease could bring additional non-linearity if the compression ratio depends on the eigengevalue (e.g. replacing it by it square root). And this non-linearty would not bring additional complexity to the back-propagation algorithm, or auto-differentiated functions: it only modifies the final values if the new matrices.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;litterature&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Litterature&lt;/h1&gt;
&lt;p&gt;Here are a few summary litterature references related to the topic.&lt;/p&gt;
&lt;div id=&#34;the-elements-of-statistical-learning&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The Elements of Statistical Learning&lt;/h4&gt;
&lt;p&gt;The ESL top of p 409 proposes PCA to interpret layers, i.e. to improve the interpretability of the decisions made by a network.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;neural-network-implementations-for-pca-and-its-extensions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Neural Network Implementations for PCA and Its Extensions&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://downloads.hindawi.com/archive/2012/847305.pdf&#34; class=&#34;uri&#34;&gt;http://downloads.hindawi.com/archive/2012/847305.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Uses neural networks as a substitute for PCA.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-incremental-neural-network-construction-algorithm-for-training-multilayer-perceptrons&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;An Incremental Neural Network Construction Algorithm for Training Multilayer Perceptrons&lt;/h4&gt;
&lt;p&gt;Aran, Oya, and Ethem Alpaydin. “An incremental neural network construction algorithm for training multilayer perceptrons.” Artificial Neural Networks and Neural Information Processing. Istanbul, Turkey: ICANN/ICONIP (2003).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cmpe.boun.edu.tr/~ethem/files/papers/aran03incremental.pdf&#34; class=&#34;uri&#34;&gt;https://www.cmpe.boun.edu.tr/~ethem/files/papers/aran03incremental.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kohonen-maps&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Kohonen Maps&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Self-organizing_map&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Self-organizing_map&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;self-organising-network&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Self-Organising Network&lt;/h4&gt;
&lt;div id=&#34;a-self-organising-network-that-grows-when-required-2002&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;A Self-Organising Network That Grows When Required (2002)&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.8763&#34; class=&#34;uri&#34;&gt;https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.8763&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-cascade-correlation-learning-architecture&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;The Cascade-Correlation Learning Architecture&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.125.6421&#34; class=&#34;uri&#34;&gt;https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.125.6421&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Growth with quick freeze as a way to avoid the expense of back-propagation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;soinnself-organizing-incremental-neural-network&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;SOINN：Self-Organizing Incremental Neural Network&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;http://www.haselab.info/soinn-e.html&#34; class=&#34;uri&#34;&gt;http://www.haselab.info/soinn-e.html&lt;/a&gt;
&lt;a href=&#34;https://cs.nju.edu.cn/rinc/SOINN/Tutorial.pdf&#34; class=&#34;uri&#34;&gt;https://cs.nju.edu.cn/rinc/SOINN/Tutorial.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Seems focused on neuron by neuron evolution.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;We will only consider modifying the network layer by layer, not neuron by neuron.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;This could actually be a big limitation of this discussion. In reality, even an identity matrix yields changes by piping the inputs through a new round of non-linearity, which is not necessarily identical to the preceding layer&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
