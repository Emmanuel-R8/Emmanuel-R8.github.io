<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neural Network on Back2Numbers</title>
    <link>/tags/neural-network.html</link>
    <description>Recent content in Neural Network on Back2Numbers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-au</language>
    <lastBuildDate>Fri, 06 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/neural-network/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>RNN Compressive Memory Part 2: The algorithm</title>
      <link>/2020/03/06/rnn-compressive-memory-part-2-the-algorithm.html</link>
      <pubDate>Fri, 06 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/06/rnn-compressive-memory-part-2-the-algorithm.html</guid>
      <description>This is part 2 of a discussion of the DeepMind paper about Compressive Memory of Recurrent Neural Networks.
This post lays out the exact algorithm which is presented at a high level in the paper. It will also draw on the details of the Transformer-XL model as presented in June 2019.
High-level algorithm Background From the last post, recall that compressive transformer are an extension to the Transformer-XL. Recall that Transformer-XL trains using a segment of all the sequences available and hidden states calculated when training previous segments.</description>
    </item>
    
    <item>
      <title>RNN Compressive Memory Part 1: A high level introduction.</title>
      <link>/2020/03/05/rnn-progressive-memory.html</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/05/rnn-progressive-memory.html</guid>
      <description>This is the first post of series dedicated to Compressive Memory of Recurrent Neural Networks. This is inspired by a recent DeepMind paper published in November 2019 on Arxiv.
Currently, the ambition of the series is to follow this plan:
 A high level introduction to Compressive Memory mechanics starting from basic RNNS;
 a detailed explanation of the algorithm and its tricky details;
 a walk-through of an implementation made in Sonnet (Deepmind’s high level framework on top of Tensorflow);</description>
    </item>
    
    <item>
      <title>Quick Thought: Universal translator and same language translator</title>
      <link>/2019/10/25/quick-idea-universal-translator-and-same-language-translator.html</link>
      <pubDate>Fri, 25 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/25/quick-idea-universal-translator-and-same-language-translator.html</guid>
      <description>Quick Thoughts are random thoughts looking for comments
 Let’s imagine a universal translator able to translate any language to any language. Sourcing a corpus of pair translation is a major hurdle. However there is an almost infinite corpus of pair translations: a language with itself; translating English to English is easy, even for a computer.
Let’s give the blackbox universal translator three inputs: a source text, the language of the source text, the language of the desired translation.</description>
    </item>
    
    <item>
      <title>Neural Network - Incremental Growth </title>
      <link>/2019/10/23/neural-network-incremental-growth.html</link>
      <pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/23/neural-network-incremental-growth.html</guid>
      <description>|  | DRAFT 1  | This post is first and foremost asking for assistance. Is anything blatantly incorrect? If not, can anybody point to existing articles/posts/litterature?
We all have laptops. But le’ts face it, even in times of 32GB of RAM and NVMe2 drives, forget about running any interesting TensorFlow model. You need to get an external GPU, build your own rig, or very quickly pay a small fortune for cloud instances.</description>
    </item>
    
  </channel>
</rss>