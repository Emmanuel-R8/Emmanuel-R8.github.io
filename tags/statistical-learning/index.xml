<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistical Learning on Back2Numbers</title>
    <link>/tags/statistical-learning.html</link>
    <description>Recent content in Statistical Learning on Back2Numbers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-au</language>
    <lastBuildDate>Sat, 05 Oct 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/statistical-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>HarvardX Data Science course - First final project</title>
      <link>/post/harvardx-data-science-course-first-final-project.html</link>
      <pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/harvardx-data-science-course-first-final-project.html</guid>
      <description>I recently finished to penultimate final assignment for the HarvardX Data Science course. The Stanford course was clearly machine learning. This one is definitely lighter on the machine learning and much heavier on the data science: how to source, clean and visualise data are key skills. The targeted knowledge is more traditional probabilities/statistics. Long-existing fundamental techniques like inference, polling are there.
This time R is the centre tool of the course.</description>
    </item>
    
    <item>
      <title>Stanford Online - Machine Learning C229 </title>
      <link>/post/stanford-online-machine-learning-c229.html</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/stanford-online-machine-learning-c229.html</guid>
      <description>Review I recently completed the Stanford online version of the Machine Learning CS229 course taught by Andrew Ng. There is no need to introduce this course which has reached stardom.
It often was a trip a trip down memory lane repeating what I studied in the late 90â€™ies. It was interesting that quite a bit has remained as relevant. Back then, and I am now talking early 90ies, neural networks were still fashionable but computationally intractable past what would hardly be considered a single layer nowadays.</description>
    </item>
    
  </channel>
</rss>